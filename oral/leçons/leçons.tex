\pdfobjcompresslevel 0
\documentclass[12pt, a4paper, parskip=full]{report}

% --- PREAMBULE ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage[top=1cm,bottom=2cm,left=1cm,right=1cm]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{titlesec}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\LARGE}
\titlespacing*{\chapter}{0pt}{0pt}{\baselineskip}

% \documentclass[a4paper, 11pt, twocolumn, oneside, openright]{report}

% %====================== PACKAGES ======================
% \usepackage{amsfonts}
% \usepackage{amsthm}
% \usepackage{float}
% \usepackage[top=2cm,bottom=2cm,left=2cm,right=2cm,includehead]{geometry}
% \usepackage{tikz-cd}
% \usetikzlibrary{calc}
% \usepackage{circuitikz}
% \usepackage{makecell}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage[french]{babel}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage[]{mdframed}
% \usepackage{enumitem}
% \usepackage{graphicx}
% \usepackage[colorlinks=true, allcolors=magenta]{hyperref}
% \usepackage[skip=10pt plus1pt, indent=20pt]{parskip}

% Marges
% \geometry{a4paper, left=2cm, right=2cm, top=2.5cm, bottom=2.5cm}

% --- ENVIRONNEMENTS PERSONNALISES ---
\newtheoremstyle{agregstyle}
  {\topsep}   % Espace avant
  {\topsep}   % Espace après
  {\itshape}  % Style de la police du corps
  {}          % Indentation
  {\bfseries} % Style de la police du titre
  {.}         % Ponctuation après le titre
  {.5em}      % Espace après le titre
  {}          % Spécification de la tête du théorème
\theoremstyle{agregstyle}

\newtheorem{definition}{Définition}[section]
\newtheorem{theorem}[definition]{Théorème}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemme}[definition]{Lemme}
\newtheorem{corollaire}[definition]{Corollaire}
\newtheorem{example}[definition]{Exemple}
\newtheorem{remark}[definition]{Remarque}
\newtheorem{application}[definition]{Application}

\newenvironment{objectif}
  {\par\medskip\noindent\begin{oframed}\noindent\textbf{\textcolor{teal}{Objectif :}}\itshape}
  {\end{oframed}\par\medskip}

\newenvironment{philosophie}
  {\par\medskip\noindent\begin{oframed}\noindent\textbf{\textcolor{blue}{Philosophie de la Leçon :}}\itshape}
  {\end{oframed}\par\medskip}

\newenvironment{developpements}
  {\par\medskip\noindent\begin{oframed}\noindent\textbf{\textcolor{teal}{Développements Possibles et Stratégie :}}}
  {\end{oframed}\par\medskip}

\newenvironment{erreurs}
  {\par\medskip\noindent\begin{oframed}\noindent\textbf{\textcolor{red}{Erreurs à Éviter / Points de Vigilance :}}}
  {\end{oframed}\par\medskip}


% Cadre pour les sections méta
\colorlet{framecolor}{gray!30}
% \newmdenv[
%   frametitleaboveskip=1mm,
%   frametitlebelowskip=2mm,
%   linewidth=1pt,
%   linecolor=framecolor,
%   backgroundcolor=framecolor!10,
%   innertopmargin=5pt,
%   innerbottommargin=5pt,
%   skipabove=\topsep,
%   skipbelow=\topsep
% ]{oframed}

% Titre
\title{Leçons d'oral de l'Agrégation}
\author{Gautier Laisné}
\date{}


\begin{document}

\maketitle
% \tableofcontents

\chapter{Leçon 101 : Groupe opérant sur un ensemble. Exemples et applications.}

\begin{philosophie}
    C'est une des leçons les plus fondamentales et transversales. L'objectif n'est pas de réciter un cours, mais de convaincre le jury que l'on a compris la philosophie de la théorie des groupes : c'est la science de la symétrie. Une symétrie n'existe pas dans l'abstrait, c'est toujours la symétrie *d'un objet*. La notion d'action est le formalisme mathématique qui décrit cette relation entre le groupe (les transformations abstraites) et l'objet (l'ensemble sur lequel il agit). Le plan doit être une démonstration de la puissance de cet outil, montrant comment une idée simple permet de résoudre des problèmes variés et profonds en algèbre, géométrie et combinatoire. Le mot "Exemples" dans le titre est la clé : ils doivent être le moteur de la leçon.
\end{philosophie}

\section{Introduction et Vocabulaire Fondamental}

\subsection{Motivation : La Symétrie en Action}
\begin{itemize}
    \item \textbf{Exemple introductif :} Considérons le groupe diédral $D_4$, groupe des isométries du carré. Abstraitement, c'est un groupe d'ordre 8. Concrètement, il "agit" sur :
        \begin{itemize}
            \item L'ensemble $V$ des 4 sommets du carré.
            \item L'ensemble $E$ des 4 arêtes du carré.
            \item L'ensemble $D$ des 2 diagonales du carré.
        \end{itemize}
    Ces différentes actions révèlent des facettes différentes de la même structure algébrique. L'étude de ces interactions est l'objet de cette leçon.
\end{itemize}

\subsection{Cadre Formel de l'Action}
\begin{itemize}
    \item \textbf{Définition (Action de groupe) :} Une action d'un groupe $G$ sur un ensemble $X$ est un morphisme de groupes $\phi: G \to \mathfrak{S}(X)$, où $\mathfrak{S}(X)$ est le groupe des bijections de $X$. On note souvent $g \cdot x$ pour $\phi(g)(x)$.
    \item \textbf{Définition équivalente :} Une application $G \times X \to X$, $(g,x) \mapsto g \cdot x$, vérifiant $e \cdot x = x$ et $g \cdot (h \cdot x) = (gh) \cdot x$.
    \item \textbf{Exemples Fondamentaux :}
        \begin{itemize}
            \item Action triviale : $g \cdot x = x$ pour tout $g,x$.
            \item Action de $G$ sur lui-même par translation à gauche : $g \cdot x = gx$.
            \item Action de $G$ sur lui-même par conjugaison : $g \cdot x = gxg^{-1}$.
            \item Action de $GL_n(K)$ sur $K^n$.
        \end{itemize}
\end{itemize}

\subsection{Vocabulaire de l'Action et Partition en Orbites}
\begin{itemize}
    \item \textbf{Définition (Orbite et Stabilisateur) :}
        \begin{itemize}
            \item L'\textbf{orbite} de $x \in X$ est $\mathcal{O}(x) = \{g \cdot x \mid g \in G\}$. Les orbites forment une partition de $X$.
            \item Le \textbf{stabilisateur} de $x$ est $\mathrm{Stab}(x) = \{g \in G \mid g \cdot x = x\}$. C'est un sous-groupe de $G$.
        \end{itemize}
    \item \textbf{Définition (Propriétés de l'action) :}
        \begin{itemize}
            \item L'action est \textbf{transitive} s'il n'y a qu'une seule orbite.
            \item L'action est \textbf{fidèle} si son noyau (les éléments de $G$ qui fixent tout $X$) est réduit à $\{e\}$.
            \item L'action est \textbf{libre} si tous les stabilisateurs sont triviaux.
        \end{itemize}
    \item \textbf{Annonce du plan :} Nous établirons d'abord les outils fondamentaux de comptage issus de cette notion. Nous verrons ensuite comment ces outils permettent de sonder la structure interne des groupes eux-mêmes, en les faisant agir sur des ensembles algébriques. Enfin, nous explorerons leurs applications classiques en géométrie et en combinatoire énumérative.
\end{itemize}

\section{Le Théorème Fondamental des Actions et ses Conséquences Directes}
\subsection{Relation Orbite-Stabilisateur}
\begin{itemize}
    \item \textbf{Théorème :} Pour toute action d'un groupe fini $G$, on a $|G| = |\mathcal{O}(x)| \times |\mathrm{Stab}(x)|$.
    \item \textbf{Idée de la preuve :} C'est une application du "principe des bergers". On construit une bijection entre l'orbite $\mathcal{O}(x)$ et l'ensemble des classes à gauche $G/\mathrm{Stab}(x)$.
\end{itemize}

\subsection{Équation aux classes : une formule de comptage globale}
\begin{itemize}
    \item \textbf{Formule :} Comme les orbites partitionnent $X$, on a $|X| = \sum_{i=1}^k |\mathcal{O}(x_i)|$, où les $x_i$ sont des représentants des orbites distinctes.
    \item \textbf{Application I : Centre d'un p-groupe.}
        \begin{itemize}
            \item \textbf{Théorème :} Tout groupe d'ordre $p^n$ ($p$ premier) a un centre non trivial.
            \item \textbf{Preuve :} On fait agir $G$ sur lui-même par conjugaison. Les orbites de taille 1 sont les éléments du centre $Z(G)$. L'équation aux classes devient $|G| = |Z(G)| + \sum [G:C(x_i)]$. Comme $|G|=p^n$, tous les termes $[G:C(x_i)]$ sont des multiples de $p$. Donc $|Z(G)|$ doit aussi être un multiple de $p$, et le centre n'est pas trivial.
        \end{itemize}
    \item \textbf{Application II : Lemme de Cauchy.}
        \begin{itemize}
            \item \textbf{Théorème :} Si $p$ est un premier divisant $|G|$, alors $G$ admet un élément d'ordre $p$.
            \item \textbf{Preuve par action de groupe :} On fait agir le groupe cyclique $\mathbb{Z}/p\mathbb{Z}$ par permutation circulaire sur l'ensemble $X = \{ (g_1, \dots, g_p) \in G^p \mid g_1 \dots g_p = e \}$. Les orbites sont de taille 1 (points fixes) ou $p$. L'équation aux classes $|X| \equiv |X^{\mathbb{Z}/p\mathbb{Z}}| \pmod p$ permet de conclure, car $(e,..,e)$ est un point fixe.
        \end{itemize}
\end{itemize}

\section{L'Action comme Outil d'Investigation de la Structure des Groupes}
\subsection{Les Théorèmes de Sylow : le Cœur de la Théorie}
\begin{itemize}
    \item \textbf{Théorème (Existence) :} Se prouve en faisant agir $G$ par translation sur l'ensemble $\mathcal{P}_{p^k}(G)$ de ses parties de cardinal $p^k$. La relation orbite-stabilisateur montre qu'il existe une orbite dont le cardinal n'est pas divisible par $p$, et le stabilisateur d'un point de cette orbite est un $p$-Sylow.
    \item \textbf{Théorème (Conjugaison et Nombre) :} Se prouve en faisant agir un $p$-Sylow $S$ (ou $G$ tout entier) sur l'ensemble de tous les $p$-Sylow par conjugaison. La transitivité de l'action de $G$ montre qu'ils sont tous conjugués. L'orbite-stabilisateur pour l'action de $S$ donne la congruence $n_p \equiv 1 \pmod p$.
\end{itemize}

\subsection{Critères de non-simplicité}
\begin{itemize}
    \item \textbf{Le Morphisme vers $\mathfrak{S}_{G/H}$ :} Si $H$ est un sous-groupe d'indice $n$, l'action de $G$ par translation sur l'ensemble des classes $G/H$ induit un morphisme $\phi: G \to \mathfrak{S}_n$. Le noyau de cette action est $\ker(\phi) = \cap_{g \in G} gHg^{-1}$, qui est le plus grand sous-groupe distingué de $G$ contenu dans $H$.
    \item \textbf{Application (Argument de l'indice) :} Si $G$ est simple, ce morphisme est injectif. Donc $|G|$ doit diviser $|\mathfrak{S}_n| = n!$.
    \item \textbf{Exemple :} Un groupe simple d'ordre 36 n'existe pas. S'il existait, son 3-Sylow serait d'ordre 9, donc d'indice 4. Il se plongerait dans $\mathfrak{S}_4$, mais $|\mathfrak{S}_4|=24$. Contradiction.
\end{itemize}

\section{Exemples d'Actions en Géométrie et Combinatoire}
\subsection{Géométrie des Polyèdres}
\begin{itemize}
    \item \textbf{Action :} Le groupe des isométries d'un cube agit sur ses sommets (8), ses arêtes (12), ses faces (6).
    \item \textbf{Calcul du cardinal de $Isom(Cube)$ :} L'action sur les faces est transitive. Le stabilisateur d'une face est le groupe de ses isométries, i.e. $D_4$ d'ordre 8. Donc $|Isom(Cube)| = |\mathcal{O}(\text{face})| \times |\mathrm{Stab}(\text{face})| = 6 \times 8 = 48$.
\end{itemize}

\subsection{Combinatoire Énumérative}
\begin{itemize}
    \item \textbf{Formule de Burnside :} Le nombre d'orbites est la moyenne du nombre de points fixes : $N = \frac{1}{|G|} \sum_{g \in G} |\text{Fix}(g)|$.
    \item \textbf{Application : Dénombrement des Colliers :} Combien de colliers différents peut-on faire avec 4 perles de 2 couleurs (N/B) ? L'objet est l'ensemble des $2^4=16$ coloriages. Le groupe est le groupe diédral $D_4$ d'ordre 8. On compte les points fixes pour chaque classe de conjugaison d'isométries.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Preuve des théorèmes de Sylow (Place : II.1) :} Un développement long mais fondamental, qui est une véritable démonstration de virtuosité dans la manipulation des actions de groupes. C'est un choix de haut niveau.
        \item \textbf{Formule de Burnside et dénombrement des coloriages distincts des faces d'un cube (Place : IV.2) :} Un développement très apprécié qui connecte l'algèbre à un problème combinatoire visuel. Il montre la puissance de l'outil sur un exemple concret et non trivial.
        \item \textbf{Simplicité de $\mathcal{A}_n$ pour $n \ge 5$ (peut être lié à II.2) :} Preuve basée sur la génération par les 3-cycles et l'étude des classes de conjugaison, qui sont des orbites de l'action par conjugaison.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un plan catalogue :} Le plan doit raconter une histoire. Partir des outils, puis ce qu'ils révèlent sur les groupes, puis ce qu'ils révèlent sur le monde extérieur (géométrie, etc.).
        \item \textbf{Oublier l'équation aux classes :} C'est l'outil de calcul principal, il doit apparaître explicitement et être utilisé.
        \item \textbf{Ne pas maîtriser les applications "de base" :} La preuve que le centre d'un $p$-groupe est non trivial est un attendu qui teste la compréhension des outils.
        \item \textbf{Confondre les actions :} Il faut être très clair sur quelle action on utilise (translation, conjugaison) et quel est son noyau pour en tirer les bonnes conclusions.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 102 : Groupe des nombres complexes de module 1. Sous-groupes. Applications.}

\begin{philosophie}
    Cette leçon est un bijou à l'intersection de l'algèbre, de la géométrie et de l'analyse. L'objet d'étude, le groupe unitaire $\mathbb{U}$, est d'une richesse phénoménale. Le plan doit mettre en valeur cette triple nature. On commencera par sa structure de groupe topologique, on étudiera en détail ses sous-groupes (qui sont au cœur de la leçon), puis on montrera comment cet objet simple est fondamental en analyse de Fourier, en théorie des nombres et en géométrie.
\end{philosophie}

\section{Le Groupe Unitaire $\mathbb{U}$ : un Objet Triple}

\subsection{Structure de Groupe Abélien}
\begin{itemize}
    \item \textbf{Définition :} $\mathbb{U} = \{ z \in \mathbb{C} \mid |z|=1 \}$. C'est un sous-groupe du groupe multiplicatif $(\mathbb{C}^*, \times)$.
    \item \textbf{Paramétrisation :} L'application $\theta \mapsto e^{i\theta}$ est une surjection de $(\mathbb{R}, +)$ sur $(\mathbb{U}, \times)$.
    \item \textbf{Théorème d'isomorphisme :} C'est un morphisme de groupes de noyau $2\pi\mathbb{Z}$. On a l'isomorphisme fondamental $\mathbb{R}/2\pi\mathbb{Z} \cong \mathbb{U}$.
\end{itemize}

\subsection{Structure Topologique et Géométrique}
\begin{itemize}
    \item \textbf{Topologie :} En tant que sous-ensemble de $\mathbb{C} \cong \mathbb{R}^2$, $\mathbb{U}$ est un compact. C'est le cercle unité.
    \item \textbf{Groupe Topologique :} Les opérations de groupe (multiplication, inversion) sont continues. $\mathbb{U}$ est un groupe topologique compact.
    \item \textbf{Géométrie :} L'action de $\mathbb{U}$ sur $\mathbb{C}$ par multiplication correspond à l'action du groupe des rotations $SO(2)$ sur le plan $\mathbb{R}^2$. On a un isomorphisme de groupes (topologiques) $\mathbb{U} \cong SO(2)$.
\end{itemize}

\section{Étude des Sous-Groupes de $\mathbb{U}$}

\subsection{Les Sous-Groupes Finis : les Racines de l'Unité}
\begin{itemize}
    \item \textbf{Théorème :} Les sous-groupes finis de $(\mathbb{C}^*, \times)$ (et donc de $\mathbb{U}$) sont exactement les groupes de racines $n$-ièmes de l'unité, notés $\mu_n$.
    \item \textbf{Structure :} Le groupe $\mu_n$ est un groupe cyclique d'ordre $n$.
    \item \textbf{Générateurs :} Les générateurs de $\mu_n$ sont les racines primitives $n$-ièmes de l'unité. Il y en a $\varphi(n)$.
\end{itemize}

\subsection{Les Sous-Groupes Monogènes Infinis}
\begin{itemize}
    \item \textbf{Proposition :} Soit $\alpha = e^{2i\pi\theta}$. Le sous-groupe engendré par $\alpha$, $\langle \alpha \rangle$, est fini si et seulement si $\theta \in \mathbb{Q}$.
    \item \textbf{Théorème :} Si $\theta \notin \mathbb{Q}$, le sous-groupe $\langle \alpha \rangle$ est infini et dense dans $\mathbb{U}$.
\end{itemize}

\subsection{Classification des Sous-Groupes Fermés}
\begin{itemize}
    \item \textbf{Théorème :} Les sous-groupes fermés de $\mathbb{U}$ sont $\mathbb{U}$ lui-même et les groupes de racines $n$-ièmes de l'unité $\mu_n$.
    \item \textbf{Idée de la preuve :} On utilise la structure de $\mathbb{R}/2\pi\mathbb{Z}$ et la classification des sous-groupes fermés de $(\mathbb{R},+)$.
\end{itemize}

\section{Applications}

\subsection{Analyse de Fourier}
\begin{itemize}
    \item \textbf{Caractères d'un groupe fini :} Les caractères du groupe cyclique $\mathbb{Z}/n\mathbb{Z}$ sont les morphismes $\mathbb{Z}/n\mathbb{Z} \to \mathbb{U}$. Ils forment un groupe isomorphe à $\mathbb{Z}/n\mathbb{Z}$. C'est la base de la transformée de Fourier discrète.
    \item \textbf{Séries de Fourier :} L'ensemble des fonctions $\{e_n : t \mapsto e^{int}\}_{n \in \mathbb{Z}}$ est l'ensemble des caractères continus de $\mathbb{U}$. L'analyse de Fourier sur le cercle consiste à décomposer une fonction sur cette base.
\end{itemize}

\subsection{Théorie des Nombres et Algèbre}
\begin{itemize}
    \item \textbf{Polynômes Cyclotomiques :} Le $n$-ième polynôme cyclotomique $\Phi_n(X)$ est le polynôme minimal sur $\mathbb{Q}$ des racines primitives $n$-ièmes de l'unité. Il est à coefficients entiers et irréductible.
    \item \textbf{Théorème de Wedderburn :} Tout corps fini est commutatif. La preuve utilise des arguments sur les polynômes cyclotomiques.
\end{itemize}

\subsection{Géométrie}
\begin{itemize}
    \item \textbf{Isomorphismes $SO(2) \cong \mathbb{U}$ et $O(2)$ :} Permet d'étudier les isométries du plan via l'arithmétique des nombres complexes. Le groupe $O(2)$ peut être décrit comme un produit semi-direct en utilisant $\mathbb{U}$ et la conjugaison complexe.
    \item \textbf{Polygones réguliers constructibles :} Le théorème de Gauss-Wantzel affirme qu'un polygone régulier à $n$ côtés est constructible à la règle et au compas si et seulement si $\varphi(n)$ est une puissance de 2. La preuve repose sur la théorie de Galois des extensions cyclotomiques $\mathbb{Q}(e^{2i\pi/n})/\mathbb{Q}$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Les sous-groupes finis de $\mathbb{C}^*$ sont les $\mu_n$ :} Un développement très classique et élégant, qui peut être casé dans plusieurs leçons de groupe.
        \item \textbf{Densité des sous-groupes monogènes engendrés par $e^{2i\pi\theta}$ avec $\theta \notin \mathbb{Q}$ :} Un beau développement qui mêle algèbre et topologie.
        \item \textbf{Irréductibilité des polynômes cyclotomiques sur $\mathbb{Q}$ :} Un développement d'arithmétique et d'algèbre très consistant, qui montre une grande maîtrise.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item Oublier la nature topologique de $\mathbb{U}$ (compacité).
        \item Mal maîtriser la classification des sous-groupes. C'est le cœur de la leçon.
        \item Ne pas faire le lien avec $\mathbb{R}/2\pi\mathbb{Z}$, qui est la clé de nombreuses preuves.
        \item Confondre les applications (Fourier, cyclotomie) avec le cœur de la leçon. Elles doivent illustrer la richesse de l'objet, pas le noyer.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 103 : Exemples de sous-groupes distingués et de groupes quotients. Applications.}

\begin{philosophie}
    Le titre est "Exemples... Applications.". La leçon doit être une démonstration de la puissance du concept de quotient, et non un cours théorique. Le plan doit être piloté par des exemples variés et significatifs, montrant que le quotient est l'outil fondamental pour simplifier un groupe et en révéler la structure. On doit montrer qu'on a compris que le noyau d'un morphisme est le prototype du sous-groupe distingué.
\end{philosophie}

\section{Le Noyau de Morphisme : La Source Canonique de Quotients}

\subsection{Le Quotient comme Image d'un Morphisme}
\begin{itemize}
    \item \textbf{Proposition Fondamentale :} Un sous-groupe $H$ de $G$ est distingué si et seulement s'il est le noyau d'un morphisme de groupes.
    \item \textbf{Théorème (Premier d'isomorphisme) :} $G/\ker(f) \cong \mathrm{Im}(f)$. Ce théorème est la machine à fabriquer des quotients : pour comprendre un quotient, on cherche un morphisme dont le noyau est le sous-groupe par lequel on quotiente.
\end{itemize}

\subsection{Exemples Algébriques et Géométriques Fondamentaux}
\begin{itemize}
    \item \textbf{Exemple 1 (Le groupe alterné $\mathcal{A}_n$) :} Le morphisme signature $\varepsilon: \mathfrak{S}_n \to \{-1, 1\}$ a pour noyau $\mathcal{A}_n$. Le quotient $\mathfrak{S}_n / \mathcal{A}_n \cong \mathbb{Z}/2\mathbb{Z}$ capture l'idée qu'il n'y a que deux "types" de permutations.
    \item \textbf{Exemple 2 (Le groupe spécial orthogonal $SO(n)$) :} Le morphisme déterminant $\det: O(n) \to \{-1, 1\}$ a pour noyau $SO(n)$. Le quotient $O(n)/SO(n) \cong \mathbb{Z}/2\mathbb{Z}$ capture l'idée qu'il n'y a que deux "types" d'isométries (directes et indirectes).
    \item \textbf{Exemple 3 (Le groupe projectif linéaire $PGL(E)$) :} Le centre $Z(GL(E))$ (les homothéties) est distingué. Le quotient $PGL(E) = GL(E)/Z(GL(E))$ est le groupe qui agit sur l'espace projectif.
\end{itemize}

\section{Exemples de Quotients dans les Groupes Classiques}

\subsection{Le Quotient Additif Fondamental : $\mathbb{Z}/n\mathbb{Z}$}
\begin{itemize}
    \item \textbf{Construction :} C'est le quotient du groupe $(\mathbb{Z}, +)$ par son sous-groupe distingué (car $\mathbb{Z}$ est abélien) $n\mathbb{Z}$.
\end{itemize}

\subsection{Quotients dans les Groupes de Matrices}
\begin{itemize}
    \item \textbf{Le groupe spécial linéaire $SL(E)$ :} Est-il toujours le noyau de $\det: GL(E) \to K^*$ ? Oui.
    \item \textbf{Le centre d'un groupe :} Est toujours distingué. Le quotient $G/Z(G)$ est isomorphe au groupe des automorphismes intérieurs.
    \item \textbf{Exemple :} Le groupe de Heisenberg. Son centre est distingué, et le quotient est un groupe abélien.
\end{itemize}

\subsection{Les Théorèmes d'Isomorphisme en Action}
\begin{itemize}
    \item \textbf{Théorème (Troisième d'isomorphisme) :} $(G/N)/(H/N) \cong G/H$ si $N \subset H$ sont distingués.
    \item \textbf{Application :} $(\mathbb{Z}/12\mathbb{Z}) / (6\mathbb{Z}/12\mathbb{Z}) \cong \mathbb{Z}/6\mathbb{Z}$.
\end{itemize}

\section{Applications à la Classification et à la Structure des Groupes}

\subsection{Les Atomes de la Théorie : les Groupes Simples}
\begin{itemize}
    \item \textbf{Définition :} Un groupe est \textbf{simple} s'il n'a pas de sous-groupe distingué non trivial. On ne peut pas le "simplifier" par un quotient.
    \item \textbf{Exemple :} Les groupes alternés $\mathcal{A}_n$ sont simples pour $n \geq 5$.
\end{itemize}

\subsection{La Déconstruction par Quotients : les Groupes Résolubles}
\begin{itemize}
    \item \textbf{Définition :} Un groupe est \textbf{résoluble} s'il peut être "déconstruit" en une suite de quotients abéliens (via la suite des groupes dérivés $D^{k+1}(G)=D(D^k(G))$).
    \item \textbf{Exemple :} $\mathfrak{S}_4$ est résoluble. La suite $\mathfrak{S}_4 \triangleright \mathcal{A}_4 \triangleright V_4 \triangleright \{\text{Id}\}$ a des quotients abéliens.
    \item \textbf{Contre-exemple :} $\mathfrak{S}_5$ n'est pas résoluble car son sous-groupe dérivé est $\mathcal{A}_5$, qui est simple et non-abélien.
    \item \textbf{Application (Théorie de Galois) :} Un polynôme est résoluble par radicaux si et seulement si son groupe de Galois est résoluble. L'existence de $\mathfrak{S}_5$ comme groupe de Galois explique l'impossibilité de la résolution de la quintique.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Simplicité de $\mathcal{A}_n$ pour $n \ge 5$ :} Un développement très classique et formateur.
        \item \textbf{Étude du groupe $O(p,q)$ :} Montrer que le centre est distingué et étudier le quotient (groupe projectif orthogonal).
        \item \textbf{Montrer qu'un groupe d'ordre $p^2q$ n'est pas simple :} Un bel exercice d'application des théorèmes de Sylow et de la notion de quotient.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Plan catalogue :} Le plan doit être piloté par les exemples et les applications.
        \item \textbf{Oublier le lien Noyau $\iff$ Distingué :} C'est la justification philosophique de tout le chapitre.
        \item \textbf{Ne donner que des exemples triviaux :} Montrer des quotients dans des groupes non abéliens est essentiel.
        \item \textbf{Confondre sous-groupe et sous-groupe distingué :} Un grand classique à éviter.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 104 : Groupes finis. Exemples et applications.}

\begin{philosophie}
    C'est une leçon de synthèse qui doit montrer une maîtrise du "zoo" des groupes finis. L'objectif est de dépasser la simple énumération pour proposer une véritable \textbf{stratégie de classification}. Le plan doit s'organiser autour des outils qui permettent de "démonter" un groupe (Sylow, produits) et de classifier les "briques élémentaires" (groupes simples, cycliques). Les exemples doivent être nombreux et variés pour illustrer la richesse de ce monde.
\end{philosophie}

\section{Outils Fondamentaux d'Analyse}
\subsection{Contraintes sur l'Ordre}
\begin{itemize}
    \item \textbf{Théorème de Lagrange :} L'ordre d'un sous-groupe divise l'ordre du groupe.
    \item \textbf{Théorèmes de Sylow :} Une réciproque partielle qui garantit l'existence de sous-groupes d'ordre $p^k$. C'est l'outil le plus puissant pour sonder la structure d'un groupe.
\end{itemize}
\subsection{Outils de Décomposition}
\begin{itemize}
    \item \textbf{Produit direct :} Pour "recoller" des groupes qui ne s'influencent pas. Condition : $G=HK$, $H,K$ distingués, $H \cap K = \{e\}$.
    \item \textbf{Produit semi-direct :} Pour construire des groupes non-abéliens à partir de briques plus simples. Condition : $G=NK$, $N$ distingué, $N \cap K = \{e\}$.
\end{itemize}

\section{Vers une Classification : les Briques Élémentaires}
\subsection{Les Groupes les plus Simples : les Groupes Abéliens}
\begin{itemize}
    \item \textbf{Théorème de Structure des Groupes Abéliens Finis :} Tout groupe abélien fini est un produit de groupes cycliques.
    \item \textbf{Deux formes :} Décomposition en facteurs invariants ($\mathbb{Z}/n_1\mathbb{Z} \times \dots$) ou en composantes primaires ($\mathbb{Z}/p_i^{\alpha_i}\mathbb{Z} \times \dots$).
\end{itemize}
\subsection{Les "Atomes" : les Groupes Simples}
\begin{itemize}
    \item \textbf{Définition :} Un groupe est simple s'il n'a pas de sous-groupe distingué propre non trivial.
    \item \textbf{Théorème de Jordan-Hölder :} Tout groupe fini a une unique suite de composition dont les facteurs sont des groupes simples.
    \item \textbf{Les Familles Connues :}
        \begin{itemize}
            \item Les groupes cycliques d'ordre premier $\mathbb{Z}/p\mathbb{Z}$.
            \item Les groupes alternés $\mathcal{A}_n$ pour $n \ge 5$.
            \item Les groupes de type Lie (e.g., $PSL_n(\mathbb{F}_q)$).
            \item Les 26 groupes sporadiques (culture).
        \end{itemize}
\end{itemize}
\subsection{Les Groupes d'Ordre $p^n$ : les $p$-groupes}
\begin{itemize}
    \item \textbf{Propriétés :} Centre non trivial, existence d'une suite de sous-groupes distingués. Ils sont donc toujours résolubles.
    \item \textbf{Classification pour les petits ordres :} Classification des groupes d'ordre $p^2$ (il y en a deux : $\mathbb{Z}/p^2\mathbb{Z}$ et $(\mathbb{Z}/p\mathbb{Z})^2$) et d'ordre 8.
\end{itemize}

\section{Exemples et Applications Concrètes}
\subsection{Analyse de Groupes de Petit Ordre}
\begin{itemize}
    \item \textbf{Exemple (Groupes d'ordre 15) :} En utilisant Sylow, on montre que $n_3=1$ et $n_5=1$. Les deux Sylow sont distingués. Le groupe est donc produit direct de ses Sylow, $\mathbb{Z}/3\mathbb{Z} \times \mathbb{Z}/5\mathbb{Z} \cong \mathbb{Z}/15\mathbb{Z}$. Tout groupe d'ordre 15 est cyclique.
    \item \textbf{Exemple (Groupes d'ordre 8) :} Il y en a 5 : 3 abéliens ($\mathbb{Z}/8\mathbb{Z}$, $\mathbb{Z}/4\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$, $(\mathbb{Z}/2\mathbb{Z})^3$) et 2 non-abéliens (le groupe diédral $D_4$ et le groupe des quaternions $\mathbb{H}_8$).
\end{itemize}
\subsection{Applications en Géométrie}
\begin{itemize}
    \item \textbf{Groupes d'isométries des solides de Platon :} Ce sont des groupes finis. On les identifie à des groupes de permutations connus : $Isom^+(Tétraèdre) \cong \mathcal{A}_4$, $Isom^+(Cube) \cong \mathfrak{S}_4$.
\end{itemize}
\subsection{Applications en Théorie des Nombres}
\begin{itemize}
    \item \textbf{Le groupe de Galois} d'une extension finie est un groupe fini qui encode les symétries des racines. La théorie des groupes finis est la clé pour comprendre la résolubilité des équations.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Classification des groupes d'ordre 8 :} Un très bon développement qui montre une maîtrise de la construction par produit semi-direct et des $p$-groupes.
        \item \textbf{Montrer qu'un groupe d'ordre $p^2q$ n'est pas simple :} Un excellent exercice d'application des théorèmes de Sylow.
        \item \textbf{Le groupe des isométries directes du tétraèdre est isomorphe à $\mathcal{A}_4$ :} Un beau développement qui connecte géométrie et algèbre.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Manquer de structure :} Le plan doit montrer une stratégie de classification, pas une liste d'exemples.
        \item \textbf{Sous-estimer les Théorèmes de Sylow :} C'est l'outil principal, il doit être au cœur de la leçon.
        \item \textbf{Ne pas connaître les groupes de petit ordre :} La classification jusqu'à l'ordre 15 est un attendu.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 105 : Groupe des permutations d'un ensemble fini. Applications.}

\begin{philosophie}
    Le groupe symétrique $\mathfrak{S}_n$ est le groupe fini le plus important. Il est à la fois concret (permutations de $n$ objets) et universel (théorème de Cayley). La leçon doit mettre en évidence cette dualité. Il faut maîtriser sa structure interne (générateurs, classes de conjugaison, signature) et montrer sa puissance à travers des applications fondamentales en algèbre et en géométrie.
\end{philosophie}

\section{Structure du Groupe Symétrique $\mathfrak{S}_n$}
\subsection{Décomposition et Générateurs}
\begin{itemize}
    \item \textbf{Décomposition en cycles à supports disjoints :} Toute permutation se décompose de manière unique. La structure cyclique est un invariant fondamental.
    \item \textbf{Parties génératrices :}
        \begin{itemize}
            \item $\mathfrak{S}_n$ est engendré par les transpositions.
            \item $\mathfrak{S}_n$ est engendré par les transpositions de la forme $(1, i)$.
            \item $\mathfrak{S}_n$ est engendré par les transpositions adjacentes $(i, i+1)$.
            \item $\mathfrak{S}_n$ est engendré par deux éléments : une transposition et un $n$-cycle.
        \end{itemize}
\end{itemize}
\subsection{Classes de Conjugaison}
\begin{itemize}
    \item \textbf{Théorème :} Deux permutations sont conjuguées dans $\mathfrak{S}_n$ si et seulement si elles ont la même structure cyclique.
    \item \textbf{Conséquence :} Les classes de conjugaison de $\mathfrak{S}_n$ sont en bijection avec les partitions de l'entier $n$.
\end{itemize}
\subsection{Le Morphisme Signature et le Groupe Alterné}
\begin{itemize}
    \item \textbf{Définition de la signature :} $\varepsilon(\sigma) = (-1)^{n - \text{nb d'orbites}}$. C'est l'unique morphisme non trivial de $\mathfrak{S}_n$ dans $(\mathbb{C}^*, \times)$.
    \item \textbf{Le groupe alterné $\mathcal{A}_n = \ker(\varepsilon)$ :} C'est un sous-groupe distingué d'indice 2.
    \item \textbf{Théorème :} Le groupe $\mathcal{A}_n$ est simple pour $n \ge 5$.
\end{itemize}

\section{Applications en Algèbre}
\subsection{Définition du Déterminant}
\begin{itemize}
    \item \textbf{Formule de Leibniz :} Le déterminant d'une matrice $A=(a_{ij})$ peut être défini par :
    $$ \det(A) = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i=1}^n a_{i, \sigma(i)} $$
    Les propriétés du déterminant (multilinéarité, caractère alterné) découlent de cette définition.
\end{itemize}
\subsection{Théorie de Galois}
\begin{itemize}
    \item \textbf{Groupe de Galois d'un polynôme :} Le groupe de Galois d'un polynôme $P$ de degré $n$ est un sous-groupe de $\mathfrak{S}_n$ qui permute les racines de $P$.
    \item \textbf{Le cas "générique" :} Le groupe de Galois du polynôme général de degré $n$ est $\mathfrak{S}_n$ tout entier.
    \item \textbf{Conséquence :} Comme $\mathfrak{S}_n$ n'est pas résoluble pour $n \ge 5$, l'équation générale de degré $n$ n'est pas résoluble par radicaux.
\end{itemize}
\subsection{Théorie des Représentations}
\begin{itemize}
    \item \textbf{Représentations de $\mathfrak{S}_n$ :} La classification des représentations irréductibles de $\mathfrak{S}_n$ est en bijection avec les partitions de $n$, via les diagrammes de Young.
\end{itemize}

\section{Applications en Géométrie}
\subsection{Isométries des Solides de Platon}
\begin{itemize}
    \item \textbf{Théorème :} Le groupe des isométries directes du tétraèdre est isomorphe à $\mathcal{A}_4$.
    \item \textbf{Théorème :} Le groupe des isométries directes du cube (et de l'octaèdre) est isomorphe à $\mathfrak{S}_4$.
    \item \textbf{Théorème :} Le groupe des isométries directes du dodécaèdre (et de l'icosaèdre) est isomorphe à $\mathcal{A}_5$.
\end{itemize}
\subsection{Géométrie des Symétries}
\begin{itemize}
    \item \textbf{Matrices de permutation :} Le morphisme $\mathfrak{S}_n \to GL_n(K)$ qui à $\sigma$ associe la matrice qui permute les vecteurs de la base canonique est une représentation.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Simplicité de $\mathcal{A}_n$ pour $n \ge 5$ :} Un grand classique, très formateur.
        \item \textbf{Générateurs de $\mathfrak{S}_n$ :} Montrer que $\mathfrak{S}_n$ est engendré par une transposition et un $n$-cycle.
        \item \textbf{Le groupe des isométries directes du cube est isomorphe à $\mathfrak{S}_4$ :} Un très beau développement qui lie algèbre et géométrie, en faisant agir le groupe sur les grandes diagonales du cube.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Se perdre dans les détails :} La structure de $\mathfrak{S}_n$ est très riche. Il faut faire des choix et construire une narration.
        \item \textbf{Mal définir la signature :} Il existe plusieurs définitions (nombre d'inversions, décomposition en transpositions...). Il faut en maîtriser au moins une parfaitement.
        \item \textbf{Ne pas faire le lien entre classes de conjugaison et partitions :} C'est un résultat central qui doit être su.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 106 : Groupe linéaire d'un espace vectoriel de dimension finie E, sous-groupes de GL(E).}

\begin{philosophie}
    Le groupe linéaire $GL(E)$ est le groupe des symétries de la structure la plus importante en algèbre : l'espace vectoriel. Cette leçon doit montrer la richesse de ce groupe, qui est à la fois un objet d'algèbre (on cherche ses générateurs, son centre, ses sous-groupes distingués) et un objet de géométrie (ses éléments sont les transformations qui préservent les droites, les plans, etc.).
\end{philosophie}

\section{Structure du Groupe Linéaire $GL(E)$}
\subsection{Définitions et Propriétés Fondamentales}
\begin{itemize}
    \item \textbf{Définition :} $GL(E) = \{ u \in \mathcal{L}(E) \mid u \text{ est bijectif} \}$. C'est un groupe pour la composition. C'est aussi un ouvert de l'espace normé $\mathcal{L}(E)$.
    \item \textbf{Le Centre $Z(GL(E))$ :} Il est constitué des homothéties non nulles.
    \item \textbf{Le Groupe Spécial Linéaire $SL(E)$ :} C'est le noyau du morphisme déterminant, $\det: GL(E) \to K^*$. C'est un sous-groupe distingué.
\end{itemize}

\subsection{Générateurs de $GL(E)$ et $SL(E)$}
\begin{itemize}
    \item \textbf{Théorème (Transvections et Dilatations) :} Le groupe $GL(E)$ est engendré par les transvections et les dilatations (sauf cas exceptionnel $n=2, K=\mathbb{F}_2$).
    \item \textbf{Théorème :} Le groupe $SL(E)$ est engendré par les transvections (sauf cas exceptionnels).
    \item \textbf{Conséquence :} Le groupe dérivé de $GL(E)$ est $SL(E)$ (sauf cas exceptionnels). L'abélianisé de $GL(E)$ est isomorphe à $K^*$.
\end{itemize}
\begin{remark}[La Simplicité de $PSL(E)$]
    Le quotient $PSL(E) = SL(E)/Z(SL(E))$ est un groupe simple (sauf cas exceptionnels). C'est une des familles infinies de groupes simples finis (les groupes de type Lie).
\end{remark}

\section{Sous-Groupes Classiques de $GL(E)$}
\subsection{Sous-Groupes liés à la Structure Algébrique}
\begin{itemize}
    \item \textbf{Le groupe des matrices diagonales inversibles :} Isomorphe à $(K^*)^n$.
    \item \textbf{Le groupe des matrices triangulaires supérieures inversibles (Groupe de Borel).}
    \item \textbf{Le groupe des matrices unipotentes (triangulaires avec des 1 sur la diagonale).}
\end{itemize}

\subsection{Sous-Groupes liés à une Structure Géométrique}
\begin{itemize}
    \item \textbf{Le Groupe Orthogonal $O(q)$ :} Si $E$ est muni d'une forme quadratique non dégénérée $q$, le groupe orthogonal $O(q) = \{ u \in GL(E) \mid q \circ u = q \}$ est le groupe des isométries pour cette géométrie.
        \begin{itemize}
            \item Si $K=\mathbb{R}$ et $q$ est la norme euclidienne, on obtient le groupe orthogonal usuel $O(n)$. C'est un sous-groupe compact.
        \end{itemize}
    \item \textbf{Le Groupe Symplectique $Sp(\phi)$ :} Si $E$ est muni d'une forme bilinéaire antisymétrique non dégénérée $\phi$.
\end{itemize}

\section{Actions du Groupe Linéaire}
\subsection{Actions Naturelles}
\begin{itemize}
    \item \textbf{Action sur $E$ :} C'est l'action définissante. Elle est fidèle. Elle n'est transitive que sur $E \setminus \{0\}$.
    \item \textbf{Action sur les sous-espaces vectoriels :} Action sur l'ensemble des droites (espace projectif), des hyperplans...
    \item \textbf{Action sur les bases :} L'action est simplement transitive.
\end{itemize}

\subsection{Action par Congruence et par Similitude}
\begin{itemize}
    \item \textbf{Action par Similitude :} $GL(E)$ agit sur $\mathcal{L}(E)$ par conjugaison ($u \mapsto pup^{-1}$). Les orbites sont les classes de similitude. La réduction des endomorphismes (Jordan) est la classification de ces orbites.
    \item \textbf{Action par Congruence :} $GL(E)$ agit sur l'espace des formes bilinéaires par $(p, \phi) \mapsto \phi \circ (p^{-1} \times p^{-1})$. Les orbites sont les classes d'équivalence de formes bilinéaires. La loi d'inertie de Sylvester classifie ces orbites sur $\mathbb{R}$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Générateurs de $SL_n(K)$ (transvections) :} Un développement d'algèbre linéaire un peu calculatoire mais très classique.
        \item \textbf{Le groupe $O(n)$ est un compact de $\mathcal{M}_n(\mathbb{R})$ :} Un beau développement qui connecte $GL(E)$ à la topologie.
        \item \textbf{Simplicité de $PSL_2(\mathbb{F}_q)$ pour $q \ge 4$ :} Un développement de haut niveau.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier les cas exceptionnels :} Pour les générateurs ou la simplicité de $PSL$, il y a toujours des cas de petite dimension ou sur des petits corps à exclure.
        \item \textbf{Ne pas distinguer $GL(E)$ et $GL_n(K)$ :} La leçon porte sur l'objet intrinsèque $GL(E)$. Les matrices sont une représentation.
        \item \textbf{Sous-estimer les actions :} Les actions par similitude et congruence sont des applications fondamentales qui relient cette leçon à d'autres chapitres majeurs (réduction, formes quadratiques).
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 107 : Représentations et caractères d'un groupe fini sur un C-espace vectoriel.}

\begin{philosophie}
    La théorie des groupes est très abstraite. L'idée fondamentale de la théorie des représentations est de rendre les groupes \textbf{concrets} en les faisant agir \textbf{linéairement} sur des espaces vectoriels. On remplace les permutations abstraites par des objets que l'on maîtrise parfaitement : les matrices. Le miracle est que l'étude de ces "ombres linéaires" permet de révéler des propriétés profondes sur le groupe lui-même. L'outil de calcul, le caractère, est d'une efficacité redoutable.
\end{philosophie}

\section{Représentations : Linéariser les Groupes}
\subsection{Définitions et Premiers Exemples}
\begin{itemize}
    \item \textbf{Définition (Représentation) :} Un morphisme de groupes $\rho: G \to GL(V)$.
    \item \textbf{Exemples :} Représentation triviale, représentation régulière, représentation par permutation.
\end{itemize}
\subsection{Irréductibilité et Décomposition}
\begin{itemize}
    \item \textbf{Définition :} Sous-représentation, représentation irréductible (irrep).
    \item \textbf{Théorème de Maschke :} Toute représentation d'un groupe fini sur $\mathbb{C}$ est somme directe de représentations irréductibles (décomposabilité totale).
\end{itemize}

\section{Le Lemme de Schur et ses Conséquences}
\subsection{Le Lemme Fondamental}
\begin{itemize}
    \item \textbf{Lemme de Schur :} Un morphisme entre deux irreps non isomorphes est nul. Un morphisme d'une irrep dans elle-même est une homothétie.
\end{itemize}
\subsection{Conséquences Immédiates}
\begin{itemize}
    \item \textbf{Représentations irréductibles des groupes abéliens :} Elles sont toutes de dimension 1.
    \item \textbf{Structure de l'algèbre du groupe $\mathbb{C}[G]$ :} Théorème de Wedderburn.
\end{itemize}

\section{Théorie des Caractères : L'ADN des Représentations}
\subsection{Le Caractère, un Invariant Complet}
\begin{itemize}
    \item \textbf{Définition :} $\chi_\rho(g) = \mathrm{Tr}(\rho(g))$. C'est une fonction centrale.
    \item \textbf{Propriétés :} $\chi(e)=\dim V$, $\chi(g^{-1})=\overline{\chi(g)}$. Deux représentations sont isomorphes si et seulement si elles ont le même caractère.
\end{itemize}
\subsection{Relations d'Orthogonalité}
\begin{itemize}
    \item \textbf{Produit scalaire hermitien} sur l'espace des fonctions centrales.
    \item \textbf{Théorème :} Les caractères des représentations irréductibles forment une base orthonormale de cet espace.
\end{itemize}
\subsection{La Table des Caractères : Carte d'Identité du Groupe}
\begin{itemize}
    \item \textbf{Propriétés structurelles :}
        \begin{itemize}
            \item Le nombre d'irreps = le nombre de classes de conjugaison.
            \item $\sum (\dim V_i)^2 = |G|$.
        \end{itemize}
    \item \textbf{Exemple : Table de $\mathfrak{S}_3$.}
    \item \textbf{Lecture d'informations sur le groupe :} On peut y lire les sous-groupes distingués, le centre, le groupe dérivé...
\end{itemize}

\section{Applications}
\subsection{Décomposition d'une Représentation Quelconque}
\begin{itemize}
    \item La multiplicité d'une irrep $V_i$ dans une représentation $V$ est donnée par le produit scalaire $\langle \chi_V, \chi_{V_i} \rangle$.
\end{itemize}
\subsection{Un Outil pour la Théorie des Groupes}
\begin{itemize}
    \item \textbf{Théorème $p^a q^b$ de Burnside :} Tout groupe d'ordre $p^a q^b$ est résoluble. La preuve est un triomphe de la théorie des caractères.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Construction de la table des caractères de $\mathfrak{S}_4$ ou $\mathcal{A}_4$ :} Un excellent développement qui montre la maîtrise des outils de la théorie.
        \item \textbf{Les représentations irréductibles des groupes abéliens sont de dimension 1 :} Une application simple et élégante du lemme de Schur.
        \item \textbf{Théorème de Maschke :} Une preuve qui montre une bonne compréhension des notions de base.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Se perdre dans les preuves techniques :} La leçon doit mettre en avant la puissance des résultats.
        \item \textbf{Ne pas savoir construire une table simple :} Celle de $\mathfrak{S}_3$ ou $D_4$ est un minimum.
        \item \textbf{Confondre les relations d'orthogonalité} (des lignes et des colonnes).
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 108 : Exemples de parties génératrices d'un groupe. Applications.}

\begin{philosophie}
    C'est une leçon transversale qui teste la culture et la capacité à naviguer entre différents types de groupes. L'objectif est de montrer que l'on peut souvent décrire un groupe, même infini ou très grand, par un petit nombre d'éléments "fondateurs". La leçon doit être une galerie d'exemples variés, allant des groupes finis aux groupes de matrices, en passant par les groupes de Lie.
\end{philosophie}

\section{Groupes Monogènes et de Type Fini}
\subsection{Le Cas le plus Simple : les Groupes Monogènes (Cycliques)}
\begin{itemize}
    \item \textbf{Définition :} Un groupe est monogène s'il est engendré par un seul élément.
    \item \textbf{Classification :} Ils sont isomorphes à $\mathbb{Z}$ ou $\mathbb{Z}/n\mathbb{Z}$.
    \item \textbf{Générateurs de $\mathbb{Z}/n\mathbb{Z}$ :} Ce sont les classes des entiers premiers avec $n$. Il y en a $\varphi(n)$.
\end{itemize}
\subsection{Groupes de Type Fini}
\begin{itemize}
    \item \textbf{Définition :} Un groupe est de type fini s'il admet une partie génératrice finie.
    \item \textbf{Théorème de Structure (Groupes abéliens) :} Un groupe abélien de type fini est isomorphe à $\mathbb{Z}^r \times (\text{partie finie})$.
    \item \textbf{Contre-exemple :} Le groupe $(\mathbb{Q},+)$ n'est pas de type fini.
\end{itemize}

\section{Générateurs de Groupes Finis Classiques}
\subsection{Le Groupe Symétrique $\mathfrak{S}_n$}
\begin{itemize}
    \item \textbf{Générateurs "naturels" :} Les transpositions.
    \item \textbf{Générateurs "économiques" :}
        \begin{itemize}
            \item Les transpositions adjacentes $(i, i+1)$.
            \item Deux éléments : une transposition et un $n$-cycle (e.g., $(1,2)$ et $(1,2,\dots,n)$).
        \end{itemize}
\end{itemize}
\subsection{Le Groupe Alterné $\mathcal{A}_n$}
\begin{itemize}
    \item \textbf{Théorème :} $\mathcal{A}_n$ est engendré par les 3-cycles pour $n \ge 3$.
\end{itemize}
\subsection{Les $p$-groupes}
\begin{itemize}
    \item \textbf{Théorème de Burnside :} Dans un $p$-groupe $G$, le nombre minimal de générateurs est donné par la dimension du $G/D(G)$-espace vectoriel $G/\Phi(G)$, où $\Phi(G)$ est le sous-groupe de Frattini.
\end{itemize}

\section{Générateurs de Groupes de Matrices}
\subsection{Le Groupe Linéaire $GL_n(K)$}
\begin{itemize}
    \item \textbf{Théorème :} $GL_n(K)$ est engendré par les matrices de transvection et de dilatation.
\end{itemize}
\subsection{Le Groupe Spécial Linéaire $SL_n(K)$}
\begin{itemize}
    \item \textbf{Théorème :} $SL_n(K)$ est engendré par les transvections (sauf cas exceptionnels).
\end{itemize}
\subsection{Le Groupe Orthogonal $O(n)$}
\begin{itemize}
    \item \textbf{Théorème de Cartan-Dieudonné :} Le groupe orthogonal d'une forme quadratique non-dégénérée sur un espace de dimension $n$ est engendré par au plus $n$ réflexions.
\end{itemize}

\section{Application à la Topologie des Groupes}
\subsection{Connexité des Groupes Topologiques}
\begin{itemize}
    \item \textbf{Théorème :} Un groupe topologique est connexe si et seulement s'il est engendré par tout voisinage de l'élément neutre.
    \item \textbf{Application :} $SO(n)$ est connexe. On montre que tout voisinage de l'identité engendre le groupe. Une rotation peut être "découpée" en une succession de petites rotations.
    \item \textbf{Contre-exemple :} $O(n)$ n'est pas connexe. Il a deux composantes connexes : $SO(n)$ et l'ensemble des isométries indirectes.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{$\mathfrak{S}_n$ est engendré par une transposition et un $n$-cycle :} Un développement combinatoire classique et élégant.
        \item \textbf{$SL_n(K)$ est engendré par les transvections :} Un développement d'algèbre linéaire un peu calculatoire mais très formateur.
        \item \textbf{Connexité de $SO(n)$ :} Un superbe développement qui mêle algèbre, géométrie et topologie.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Ne donner que des exemples de groupes finis :} Le titre est général. Les groupes de matrices et les groupes de Lie sont des exemples essentiels.
        \item \textbf{Manquer de structure :} Le plan doit regrouper les exemples par type de groupe (fini, abélien, matriciel...).
        \item \textbf{Confondre "généré" et "base" :} Un groupe n'a pas de "base" en général. La décomposition sur les générateurs n'est pas unique.
    \end{itemize}
\end{erreurs}

\chapter{Leçon 120 : Anneau Z/nZ. Applications.}

\begin{philosophie}
    Cette leçon est au carrefour de l'arithmétique, de la théorie des groupes et de la théorie des anneaux. L'objectif est de montrer que cet objet d'apparence simple, $\mathbb{Z}/n\mathbb{Z}$, est en réalité une structure extraordinairement riche. Il faut explorer sa structure d'anneau (idéaux, inversibles), son groupe multiplicatif, et ses applications spectaculaires en arithmétique et en cryptographie.
\end{philosophie}

\section{Structure de l'Anneau $\mathbb{Z}/n\mathbb{Z}$}

\subsection{Construction et Propriétés Fondamentales}
\begin{itemize}
    \item \textbf{Construction :} L'anneau $\mathbb{Z}/n\mathbb{Z}$ est l'anneau quotient de l'anneau principal $\mathbb{Z}$ par son idéal $n\mathbb{Z}$. C'est l'exemple prototypique d'anneau quotient.
    \item \textbf{Idéaux de $\mathbb{Z}/n\mathbb{Z}$ :} Par le théorème de correspondance, ils sont en bijection avec les idéaux de $\mathbb{Z}$ contenant $n\mathbb{Z}$. Ces idéaux sont de la forme $d\mathbb{Z}$ avec $d|n$. Il y a donc une bijection entre les idéaux de $\mathbb{Z}/n\mathbb{Z}$ et les diviseurs de $n$.
    \item \textbf{Diviseurs de zéro et intégrité :} Un élément $\bar{k}$ est un diviseur de zéro si et seulement si $\mathrm{pgcd}(k,n) \neq 1$ et $k \not\equiv 0 \pmod n$. Il s'ensuit que $\mathbb{Z}/n\mathbb{Z}$ est un corps si et seulement s'il est intègre, si et seulement si $n$ est un nombre premier. Ce corps est noté $\mathbb{F}_n$.
\end{itemize}

\subsection{Le Groupe des Inversibles $(\mathbb{Z}/n\mathbb{Z})^\times$}
\begin{itemize}
    \item \textbf{Caractérisation :} Un élément $\bar{k}$ est inversible si et seulement si $\mathrm{pgcd}(k,n)=1$. La preuve repose sur l'identité de Bézout.
    \item \textbf{Cardinal et Indicateur d'Euler :} Le cardinal du groupe des inversibles est, par définition, $\varphi(n)$.
    \item \textbf{Théorème d'Euler :} C'est une application directe du théorème de Lagrange à ce groupe. Si $\mathrm{pgcd}(a,n)=1$, alors $a^{\varphi(n)} \equiv 1 \pmod n$. Le petit théorème de Fermat en est le cas particulier où $n=p$ est premier.
    \item \textbf{Structure de $(\mathbb{Z}/n\mathbb{Z})^\times$ :} C'est un groupe abélien fini. On peut se demander quand il est cyclique.
        \begin{itemize}
            \item \textbf{Théorème :} $(\mathbb{Z}/n\mathbb{Z})^\times$ est cyclique si et seulement si $n=2, 4, p^k$ ou $2p^k$ avec $p$ premier impair.
            \item \textbf{Contre-exemple :} $(\mathbb{Z}/8\mathbb{Z})^\times \cong \mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ n'est pas cyclique.
        \end{itemize}
\end{itemize}

\subsection{Le Théorème des Restes Chinois}
\begin{itemize}
    \item \textbf{Énoncé (Isomorphisme d'anneaux) :} Si $n = \prod p_i^{a_i}$ est la décomposition de $n$ en facteurs premiers, alors on a un isomorphisme d'anneaux :
    $$ \mathbb{Z}/n\mathbb{Z} \cong \prod_{i} \mathbb{Z}/p_i^{a_i}\mathbb{Z} $$
    \item \textbf{Conséquence (Isomorphisme de groupes) :} En passant aux inversibles, on obtient :
    $$ (\mathbb{Z}/n\mathbb{Z})^\times \cong \prod_{i} (\mathbb{Z}/p_i^{a_i}\mathbb{Z})^\times $$
    Ceci permet de ramener l'étude de la structure du groupe des inversibles au cas des puissances de nombres premiers.
\end{itemize}

\section{Applications}

\subsection{Applications Arithmétiques}
\begin{itemize}
    \item \textbf{Résolution de systèmes de congruences :} Le théorème des restes chinois garantit l'existence et l'unicité de la solution modulo $n$ d'un système $x \equiv a_i \pmod{n_i}$ où les $n_i$ sont premiers entre eux deux à deux.
    \item \textbf{Tests de primalité :} Le petit théorème de Fermat est à la base de tests de primalité probabilistes (Fermat, Miller-Rabin).
    \item \textbf{Résolution d'équations diophantiennes :} La réduction modulo $n$ est un outil puissant pour montrer qu'une équation n'a pas de solution entière.
\end{itemize}

\subsection{Applications en Cryptographie}
\begin{itemize}
    \item \textbf{Cryptosystème RSA :} La sécurité repose sur la difficulté de factoriser un grand entier $n=pq$. La connaissance de $\varphi(n)=(p-1)(q-1)$ permet de calculer l'inverse modulaire de l'exposant public, ce qui est la clé de déchiffrement. Le fonctionnement repose sur le théorème d'Euler.
    \item \textbf{Échange de clés Diffie-Hellman :} Repose sur la difficulté du problème du logarithme discret dans le groupe cyclique $(\mathbb{Z}/p\mathbb{Z})^\times$ pour un grand premier $p$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème des Restes Chinois (Couteau-suisse) :} Preuve et application à la résolution d'un système. Utile en algèbre générale et arithmétique.
        \item \textbf{Démonstration que $(\mathbb{Z}/p\mathbb{Z})^\times$ est cyclique :} Un grand classique de la théorie des corps finis, qui utilise le fait que dans un corps, un polynôme de degré $d$ a au plus $d$ racines.
        \item \textbf{Description du protocole RSA :} Un développement très moderne et appliqué, qui montre une bonne culture.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les structures :} Il faut être très clair quand on parle de l'anneau $(\mathbb{Z}/n\mathbb{Z}, +, \times)$, du groupe additif $(\mathbb{Z}/n\mathbb{Z}, +)$ (toujours cyclique), et du groupe multiplicatif $(\mathbb{Z}/n\mathbb{Z})^\times$ (pas toujours cyclique).
        \item \textbf{Mal énoncer le théorème des restes chinois :} Oublier la condition "premiers entre eux" ou présenter un isomorphisme de groupes au lieu d'un isomorphisme d'anneaux.
        \item \textbf{Ne pas connaître la structure de $(\mathbb{Z}/n\mathbb{Z})^\times$ :} Savoir quand ce groupe est cyclique est un résultat important.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 121 : Nombres premiers. Applications.}

\begin{philosophie}
    Les nombres premiers sont les "atomes" de l'arithmétique. Cette leçon doit célébrer leur statut fondamental. Le plan doit s'articuler autour de trois axes : leur existence et leur infinitude, la difficulté de comprendre leur distribution, et enfin leur rôle central comme briques de base en algèbre (corps finis) et en cryptographie.
\end{philosophie}

\section{Propriétés Fondamentales et Infinitude}

\subsection{Définitions et Théorème Fondamental}
\begin{itemize}
    \item \textbf{Définition (Irréductible vs Premier) :} Dans $\mathbb{Z}$, un entier $p>1$ est premier s'il est irréductible. La distinction devient cruciale dans des anneaux plus généraux.
    \item \textbf{Lemme d'Euclide :} $p$ est premier $\iff (p|ab \implies p|a \text{ ou } p|b)$. C'est la propriété clé.
    \item \textbf{Théorème Fondamental de l'Arithmétique :} Existence et unicité de la décomposition en facteurs premiers.
\end{itemize}

\subsection{Infinitude des Nombres Premiers}
\begin{itemize}
    \item \textbf{Preuve d'Euclide :} La preuve par l'absurde classique (considérer $N = p_1 \cdots p_k + 1$).
    \item \textbf{Preuve "analytique" d'Euler :} L'étude de la série harmonique $\sum 1/n$ et son lien avec le produit eulérien $\prod_p (1-1/p)^{-1}$ montre que la série $\sum 1/p$ des inverses des nombres premiers diverge.
    \item \textbf{Preuve "topologique" de Fürstenberg :} Une preuve très élégante qui utilise une topologie sur $\mathbb{Z}$.
\end{itemize}

\section{Distribution des Nombres Premiers}

\subsection{Fonctions de Comptage et Estimations}
\begin{itemize}
    \item \textbf{Fonction de comptage $\pi(x)$ :} $\pi(x) = \#\{p \le x \mid p \text{ premier}\}$.
    \item \textbf{Théorème des Nombres Premiers (Hadamard, de la Vallée Poussin) :} On a l'équivalent asymptotique $\pi(x) \sim \frac{x}{\ln(x)}$.
    \item \textbf{Postulat de Bertrand :} Pour tout $n \ge 1$, il existe un nombre premier entre $n$ et $2n$.
\end{itemize}

\subsection{Nombres Premiers dans les Progressions Arithmétiques}
\begin{itemize}
    \item \textbf{Théorème de la Progression Arithmétique (Dirichlet) :} Si $\mathrm{pgcd}(a,n)=1$, la progression arithmétique $a+kn$ contient une infinité de nombres premiers.
\end{itemize}

\subsection{Quelques Grandes Conjectures}
\begin{itemize}
    \item \textbf{Conjecture des nombres premiers jumeaux :} Il existe une infinité de couples de premiers $(p, p+2)$.
    \item \textbf{Conjecture de Goldbach :} Tout entier pair supérieur à 2 est la somme de deux nombres premiers.
    \item \textbf{Hypothèse de Riemann :} Concerne les zéros non-triviaux de la fonction Zêta, et donnerait des informations très fines sur la répartition des nombres premiers.
\end{itemize}
\begin{remark}
    Mentionner ces conjectures montre une culture mathématique et le fait que l'arithmétique est un domaine de recherche très actif.
\end{remark}

\section{Applications des Nombres Premiers}

\subsection{Briques de Base de l'Algèbre Finie}
\begin{itemize}
    \item \textbf{Corps Finis :} Tout corps fini a pour cardinal $p^n$ où $p$ est premier. Le plus simple est le corps premier $\mathbb{F}_p = \mathbb{Z}/p\mathbb{Z}$.
    \item \textbf{Théorèmes de Sylow :} La structure des groupes finis est largement contrainte par la factorisation de leur ordre en nombres premiers.
\end{itemize}

\subsection{Tests de Primalité et Factorisation}
\begin{itemize}
    \item \textbf{Test de Fermat :} Basé sur le petit théorème de Fermat. C'est un test probabiliste.
    \item \textbf{Test de Miller-Rabin :} Une version plus robuste qui évite les nombres de Carmichael.
    \item \textbf{Difficulté de la factorisation :} Alors que tester si un nombre est premier est "facile" (temps polynomial), trouver ses facteurs premiers est considéré comme "difficile". Cette dissymétrie est la base de la cryptographie moderne.
\end{itemize}

\subsection{Cryptographie à Clé Publique}
\begin{itemize}
    \item \textbf{RSA :} La sécurité repose sur la difficulté de factoriser $n=pq$.
    \item \textbf{Diffie-Hellman :} La sécurité repose sur la difficulté du logarithme discret dans $(\mathbb{Z}/p\mathbb{Z})^\times$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Divergence de la série des inverses des nombres premiers ($\sum 1/p$) :} Un superbe développement qui mêle arithmétique et analyse.
        \item \textbf{Théorème de Wilson ($ (p-1)! \equiv -1 \pmod p \iff p \text{ premier}$) :} Une preuve élégante utilise le fait que $(\mathbb{Z}/p\mathbb{Z})^\times$ est un groupe où chaque élément est son propre inverse, sauf 1 et -1.
        \item \textbf{Description du test de primalité de Miller-Rabin :} Un développement très pertinent qui montre une compréhension des aspects algorithmiques.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Se limiter à l'arithmétique de $\mathbb{Z}$ :} Le titre est "Nombres premiers". Leur rôle dans les corps finis et les groupes est essentiel.
        \item \textbf{Ne pas faire la distinction entre un test de primalité et un algorithme de factorisation.}
        \item \textbf{Mal énoncer le Théorème des Nombres Premiers :} C'est un équivalent, pas une égalité.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 122 : Anneaux principaux. Applications.}

\begin{philosophie}
    Les anneaux principaux sont le "paradis" de l'arithmétique des anneaux. Ils généralisent les deux exemples les plus importants, $\mathbb{Z}$ et $K[X]$, en extrayant la propriété clé qui fait fonctionner leur arithmétique : le fait que tout idéal soit engendré par un seul élément. Cette leçon doit montrer la puissance de cette unique propriété, qui implique l'identité de Bézout, la factorialité, et surtout, le théorème de structure des modules qui unifie des pans entiers de l'algèbre.
\end{philosophie}

\section{La Hiérarchie des Anneaux et la Place des Anneaux Principaux}
\subsection{Une Hiérarchie de "Bonnes Propriétés"}
\begin{itemize}
    \item \textbf{Rappel de la hiérarchie :} Euclidien $\implies$ Principal $\implies$ Factoriel $\implies$ Intègre.
    \item \textbf{Exemples Fondamentaux :} $\mathbb{Z}$ et $K[X]$ sont euclidiens, donc principaux. L'anneau des entiers de Gauss $\mathbb{Z}[i]$ est euclidien.
    \item \textbf{Contre-exemples Essentiels :}
        \begin{itemize}
            \item L'anneau $\mathbb{Z}[X]$ n'est pas principal. L'idéal $(2,X)$ n'est pas principal. Cela montre que $A$ principal $\nRightarrow A[X]$ principal.
            \item L'anneau $\mathbb{Z}[i\sqrt{5}]$ n'est pas factoriel, donc ne peut pas être principal.
        \end{itemize}
\end{itemize}

\section{Propriétés Arithmétiques des Anneaux Principaux}
\subsection{PGCD et Identité de Bézout}
\begin{itemize}
    \item \textbf{Définition du PGCD :} Dans un anneau principal, l'idéal $(a,b)$ est principal, engendré par un certain $d$. Ce $d$ est le PGCD de $a$ et $b$.
    \item \textbf{Théorème de Bézout :} Comme $d \in (a,b)$, il existe $u,v$ tels que $au+bv=d$.
\end{itemize}
\subsection{Factorialité des Anneaux Principaux}
\begin{itemize}
    \item \textbf{Lemme clé :} Dans un anneau principal, tout élément irréductible est premier (la preuve utilise Bézout).
    \item \textbf{Théorème :} Tout anneau principal est factoriel.
\end{itemize}
\begin{remark}
    C'est le résultat central. La condition "tout idéal est monogène", qui est une condition structurelle sur les idéaux, a une conséquence très forte sur l'arithmétique des éléments : l'unicité de la factorisation.
\end{remark}

\section{La Grande Application : Le Théorème de Structure des Modules}
\subsection{Le Langage des Modules}
\begin{itemize}
    \item \textbf{Rappel :} Un $A$-module est une généralisation d'un espace vectoriel où les scalaires sont dans un anneau $A$.
    \item \textbf{Modules de type fini :} Un module est de type fini s'il est engendré par un nombre fini d'éléments.
\end{itemize}
\subsection{Le Théorème de Structure}
\begin{itemize}
    \item \textbf{Théorème :} Soit $A$ un anneau principal et $M$ un $A$-module de type fini. Alors $M$ se décompose de manière unique en une somme directe :
    $$ M \cong A^r \oplus A/(a_1) \oplus \dots \oplus A/(a_k) $$
    où $r$ est le rang de $M$ et les $(a_i)$ sont les facteurs invariants de $M$ ($a_1 | a_2 | \dots | a_k$).
\end{itemize}
\begin{remark}
    Ce théorème est le point culminant de la théorie. C'est un résultat d'une puissance unificatrice phénoménale. Il faut le présenter comme le résultat principal de la leçon.
\end{remark}

\subsection{Deux Applications Majeures du Théorème de Structure}
\begin{itemize}
    \item \textbf{Application 1 : Classification des groupes abéliens de type fini.}
        \begin{itemize}
            \item \textbf{Le dictionnaire :} Un groupe abélien est un $\mathbb{Z}$-module. $\mathbb{Z}$ est un anneau principal.
            \item \textbf{Le résultat :} Le théorème de structure donne la classification complète : tout groupe abélien de type fini est isomorphe à $\mathbb{Z}^r \times \mathbb{Z}/n_1\mathbb{Z} \times \dots \times \mathbb{Z}/n_k\mathbb{Z}$.
        \end{itemize}
    \item \textbf{Application 2 : Réduction des endomorphismes.}
        \begin{itemize}
            \item \textbf{Le dictionnaire :} Un $K$-espace vectoriel $V$ muni d'un endomorphisme $u$ est un $K[X]$-module. $K[X]$ est un anneau principal.
            \item \textbf{Le résultat :} Le théorème de structure appliqué à ce module donne la décomposition de $V$ en sous-espaces cycliques, ce qui correspond à la \textbf{décomposition de Frobenius}. Les facteurs invariants sont les polynômes invariants de l'endomorphisme.
        \end{itemize}
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Tout anneau euclidien est principal :} Un développement court, classique et élégant.
        \item \textbf{L'anneau $\mathbb{Z}[X]$ n'est pas principal :} Un contre-exemple essentiel à savoir démontrer proprement.
        \item \textbf{Le Théorème de structure pour les groupes abéliens finis (partie existence) :} Un développement de haut niveau qui montre une compréhension profonde de la théorie des modules.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire une leçon sur les anneaux en général :} Le titre est "Anneaux principaux". Il faut se concentrer sur leurs propriétés spécifiques.
        \item \textbf{Passer à côté du théorème de structure :} C'est l'application la plus importante et la plus spectaculaire. Elle doit être le point culminant du plan.
        \item \textbf{Ne pas maîtriser les contre-exemples :} Savoir pourquoi $\mathbb{Z}[X]$ n'est pas principal est aussi important que de savoir pourquoi $\mathbb{Z}$ l'est.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 123 : Corps finis. Applications.}

\begin{philosophie}
    Les corps finis, ou corps de Galois, sont des objets "parfaits" où l'algèbre, l'arithmétique et la combinatoire se rencontrent. Contrairement aux corps infinis, leur structure est complètement connue et rigide. La leçon doit présenter cette classification complète (existence, unicité, structure) puis déployer la puissance de ces objets dans des domaines variés comme la théorie des nombres, la cryptographie et les codes correcteurs.
\end{philosophie}

\section{Existence et Unicité : La Classification Complète}
\subsection{Le Corps Premier $\mathbb{F}_p$}
\begin{itemize}
    \item \textbf{Caractéristique d'un corps :} Tout corps fini a une caractéristique $p$ première. Il contient donc une copie du corps premier $\mathbb{F}_p = \mathbb{Z}/p\mathbb{Z}$.
    \item \textbf{Structure d'espace vectoriel :} Tout corps fini $K$ est un $\mathbb{F}_p$-espace vectoriel de dimension finie $n$. Son cardinal est donc $p^n$.
\end{itemize}

\subsection{Construction et Unicité du Corps $\mathbb{F}_{p^n}$}
\begin{itemize}
    \item \textbf{Existence :} Pour tout $q=p^n$, le corps $\mathbb{F}_q$ existe. C'est le corps de décomposition du polynôme $P(X) = X^q-X$ sur $\mathbb{F}_p$.
    \item \textbf{Unicité :} Deux corps finis de même cardinal sont toujours isomorphes.
\end{itemize}
\begin{remark}
    C'est un résultat de classification extraordinaire : il y a exactement un corps (à isomorphisme près) pour chaque cardinal de la forme $p^n$. Le monde des corps finis est parfaitement connu.
\end{remark}

\section{Structure Algébrique des Corps Finis}
\subsection{L'Automorphisme de Frobenius}
\begin{itemize}
    \item \textbf{Définition :} L'application $\sigma: x \mapsto x^p$ est un automorphisme du corps $\mathbb{F}_q$ (où $q=p^n$).
    \item \textbf{Théorème (Théorie de Galois) :} L'extension $\mathbb{F}_q/\mathbb{F}_p$ est galoisienne. Son groupe de Galois est cyclique d'ordre $n$, engendré par l'automorphisme de Frobenius.
    \item \textbf{Conséquence :} Les sous-corps de $\mathbb{F}_{p^n}$ sont exactement les $\mathbb{F}_{p^d}$ pour $d$ divisant $n$.
\end{itemize}

\subsection{La Structure du Groupe Multiplicatif}
\begin{itemize}
    \item \textbf{Théorème :} Le groupe multiplicatif $(\mathbb{F}_q^*, \times)$ d'un corps fini est toujours cyclique.
    \item \textbf{Corollaire (Existence de racines primitives) :} Il existe des générateurs de $\mathbb{F}_q^*$.
\end{itemize}
\begin{remark}
    C'est un résultat très puissant et non-intuitif. Par exemple, $(\mathbb{Z}/15\mathbb{Z})^*$ n'est pas cyclique, mais $\mathbb{F}_{16}^*$ l'est.
\end{remark}

\subsection{Polynômes sur les Corps Finis}
\begin{itemize}
    \item \textbf{Polynômes irréductibles :} Pour tout degré $d \ge 1$, il existe des polynômes irréductibles de degré $d$ sur $\mathbb{F}_p$. C'est ce qui permet de construire $\mathbb{F}_{p^d}$ comme $\mathbb{F}_p[X]/(P)$ où $P$ est irréductible de degré $d$.
\end{itemize}

\section{Applications}
\subsection{Théorie des Nombres}
\begin{itemize}
    \item \textbf{Loi de Réciprocité Quadratique :} Une des preuves les plus élégantes passe par le calcul du symbole de Legendre en comptant les points de certaines courbes sur les corps finis.
    \item \textbf{Sommes de Gauss et de Jacobi :} Des outils puissants pour compter le nombre de solutions d'équations sur les corps finis.
\end{itemize}

\subsection{Codes Correcteurs d'Erreurs}
\begin{itemize}
    \item \textbf{Principe :} Un code est un sous-espace d'un espace vectoriel $(\mathbb{F}_q)^n$. L'utilisation de corps finis (et non de $\mathbb{R}$) est naturelle pour la transmission de données binaires.
    \item \textbf{Codes de Reed-Solomon :} Un des codes les plus utilisés (CD, QR codes...). Le message est encodé dans les coefficients d'un polynôme sur $\mathbb{F}_q$. Le mot de code est l'évaluation de ce polynôme en tous les points du corps. La redondance permet de corriger les erreurs par interpolation polynomiale.
\end{itemize}

\subsection{Cryptographie}
\begin{itemize}
    \item \textbf{Cryptographie sur les Courbes Elliptiques :} La cryptographie moderne (ECC) est largement basée sur la difficulté du problème du logarithme discret dans le groupe des points d'une courbe elliptique définie sur un corps fini.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Le groupe multiplicatif d'un corps fini est cyclique :} Un grand classique, très élégant, qui peut servir dans de nombreuses leçons (groupes, corps, anneaux).
        \item \textbf{Théorème de Chevalley-Warning :} Un théorème qui donne des conditions pour l'existence de zéros pour des polynômes sur les corps finis.
        \item \textbf{Construction d'un corps fini (e.g., $\mathbb{F}_4$ ou $\mathbb{F}_8$) :} Un développement concret qui montre une maîtrise de la construction par quotient d'anneau de polynômes.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre $\mathbb{Z}/n\mathbb{Z}$ et $\mathbb{F}_n$ :} $\mathbb{F}_n$ n'a de sens que si $n$ est premier. Pour $n=4$, le corps $\mathbb{F}_4$ n'est pas $\mathbb{Z}/4\mathbb{Z}$ (qui n'est pas un corps).
        \item \textbf{Oublier la cyclicité du groupe multiplicatif :} C'est le théorème de structure le plus important.
        \item \textbf{Manquer d'applications concrètes :} C'est une leçon où les applications sont particulièrement riches et attendues.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 125 : Extensions de corps. Exemples et applications.}

\begin{philosophie}
    C'est la leçon qui pose le langage et la motivation de la théorie de Galois. L'objectif est de montrer comment l'étude des extensions de corps, armée des outils de l'algèbre linéaire, permet de résoudre des problèmes géométriques et algébriques classiques. Le plan doit être une montée en puissance, partant de la notion de degré pour arriver aux problèmes de constructibilité et à l'aube de la théorie de Galois.
\end{philosophie}

\section{Le Degré d'une Extension : Mesurer la Complexité}
\subsection{Corps comme Espace Vectoriel}
\begin{itemize}
    \item \textbf{Définition :} Une extension $L/K$ munit $L$ d'une structure de $K$-espace vectoriel. Le degré $[L:K]$ est sa dimension.
    \item \textbf{Théorème (Tour de Multiplicativité) :} $[M:K] = [M:L] \times [L:K]$.
\end{itemize}
\subsection{Éléments Algébriques et Transcendants}
\begin{itemize}
    \item \textbf{Définition :} $\alpha \in L$ est algébrique sur $K$ s'il est racine d'un polynôme de $K[X]$.
    \item \textbf{Polynôme Minimal $\pi_\alpha$ :} Le générateur unitaire de l'idéal annulateur de $\alpha$. C'est un polynôme irréductible.
    \item \textbf{Structure de $K(\alpha)$ :} L'extension simple $K(\alpha)$ est un corps et $[K(\alpha):K] = \deg(\pi_\alpha)$. On a $K(\alpha) \cong K[X]/(\pi_\alpha)$.
\end{itemize}

\section{Constructions Géométriques à la Règle et au Compas}
\subsection{Le Corps des Nombres Constructibles}
\begin{itemize}
    \item \textbf{Définition :} Un point est constructible s'il peut être obtenu à partir de $(0,0)$ et $(1,0)$ par une suite finie d'intersections de droites et de cercles. Un nombre est constructible s'il est une coordonnée d'un point constructible.
    \item \textbf{Caractérisation Algébrique :} Le corps $\mathcal{C}$ des nombres constructibles est le plus petit sous-corps de $\mathbb{R}$ stable par racine carrée.
\end{itemize}
\subsection{La Condition de Degré}
\begin{itemize}
    \item \textbf{Théorème de Wantzel :} Un nombre $\alpha \in \mathbb{R}$ est constructible si et seulement s'il existe une tour d'extensions $\mathbb{Q}=K_0 \subset K_1 \subset \dots \subset K_n$ telle que $\alpha \in K_n$ et $[K_{i+1}:K_i]=2$ pour tout $i$.
    \item \textbf{Corollaire :} Si $\alpha$ est constructible, alors son degré $[\mathbb{Q}(\alpha):\mathbb{Q}]$ doit être une puissance de 2.
\end{itemize}
\subsection{Résolution des Problèmes Antiques}
\begin{itemize}
    \item \textbf{Duplication du cube :} Reviendrait à construire $\sqrt[3]{2}$. Mais $[\mathbb{Q}(\sqrt[3]{2}):\mathbb{Q}]=3$, qui n'est pas une puissance de 2. Impossible.
    \item \textbf{Trisection de l'angle :} Reviendrait à construire $\cos(\theta/3)$ à partir de $\cos(\theta)$. L'identité $\cos(3\alpha)=4\cos^3\alpha - 3\cos\alpha$ montre que pour un angle général (e.g., 60°), cela nécessite de résoudre une équation de degré 3 irréductible. Impossible.
    \item \textbf{Quadrature du cercle :} Reviendrait à construire $\sqrt{\pi}$. Impossible car $\pi$ est transcendant (admis, Lindemann).
\end{itemize}

\section{Vers la Théorie de Galois}
\subsection{Corps de Rupture et Corps de Décomposition}
\begin{itemize}
    \item \textbf{Corps de rupture :} $K[X]/(P)$ où $P$ est irréductible. Contient au moins une racine.
    \item \textbf{Corps de décomposition :} Plus petite extension contenant toutes les racines. C'est l'arène naturelle pour étudier les symétries des racines.
\end{itemize}
\subsection{Clôture Algébrique}
\begin{itemize}
    \item \textbf{Définition :} Un corps $K$ est algébriquement clos si tout polynôme de $K[X]$ y est scindé.
    \item \textbf{Théorème (Steinitz, admis) :} Tout corps admet une clôture algébrique, unique à isomorphisme près.
\end{itemize}
\subsection{Introduction à l'Idée de Galois}
\begin{itemize}
    \item \textbf{Motivation :} Étudier les symétries des racines d'un polynôme $P$ via le groupe des automorphismes de son corps de décomposition $L$ qui fixent le corps de base $K$. Ce groupe est le groupe de Galois $\mathrm{Gal}(L/K)$. La leçon sur la théorie de Galois explore la correspondance entre la structure de ce groupe et la structure des sous-extensions.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Impossibilité de la duplication du cube :} Un développement très classique, clair et qui montre une belle application du concept de degré.
        \item \textbf{Le théorème de l'élément primitif :} Un résultat technique mais fondamental, dont la preuve est instructive.
        \item \textbf{Construction d'un corps fini comme extension de corps (e.g. $\mathbb{F}_4$) :} On part de $\mathbb{F}_2$ et on lui adjoint une racine d'un polynôme irréductible de degré 2.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre corps de rupture et corps de décomposition :} Une erreur classique. $\mathbb{Q}(\sqrt[3]{2})$ est un corps de rupture de $X^3-2$ mais pas son corps de décomposition.
        \item \textbf{Faire une leçon de théorie de Galois :} Le titre est "Extensions de corps". La théorie de Galois est une perspective, une application, mais pas le cœur de la leçon.
        \item \textbf{Mal énoncer la condition de Wantzel :} Le fait que le degré soit une puissance de 2 est une condition nécessaire, mais pas suffisante. Il faut l'existence de la tour d'extensions quadratiques.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 126 : Exemples d'équations en arithmétique.}

\begin{philosophie}
    C'est une leçon de "culture" arithmétique. Le jury attend de voir une galerie d'exemples variés, illustrant une large palette de méthodes de résolution. Le plan doit s'organiser non pas par type d'équation, mais par \textbf{type de méthode}, pour montrer que l'on a une vision stratégique de la résolution de problèmes diophantiens.
\end{philosophie}

\section{Méthodes Élémentaires : Congruences et Descente Infinie}
\subsection{L'Outil de la Réduction Modulaire}
\begin{itemize}
    \item \textbf{Principe :} Si une équation a une solution dans $\mathbb{Z}$, elle doit en avoir une dans $\mathbb{Z}/n\mathbb{Z}$ pour tout $n$. L'absence de solution modulo $n$ pour un $n$ bien choisi prouve l'absence de solution entière.
    \item \textbf{Exemple 1 :} L'équation $x^2 - 5y^2 = 3$ n'a pas de solution. En regardant modulo 4, les carrés sont 0 et 1. $x^2-y^2 \pmod 4$ ne peut jamais valoir 3.
    \item \textbf{Exemple 2 :} Une somme de deux carrés ne peut pas être de la forme $4k+3$.
\end{itemize}
\subsection{La Méthode de la Descente Infinie de Fermat}
\begin{itemize}
    \item \textbf{Principe :} Pour montrer qu'une équation n'a pas de solution entière non triviale, on suppose qu'il en existe une et on en construit une autre "plus petite" (pour une certaine norme). Ceci contredit le principe que toute partie non vide de $\mathbb{N}$ admet un plus petit élément.
    \item \textbf{Exemple Célèbre :} Le cas $n=4$ du Grand Théorème de Fermat. L'équation $x^4+y^4=z^2$ n'a pas de solution entière non triviale.
\end{itemize}

\section{Méthodes Algébriques : Factorisation dans des Anneaux d'Entiers}
\subsection{Les Entiers de Gauss pour les Sommes de Deux Carrés}
\begin{itemize}
    \item \textbf{Le cadre :} On travaille dans l'anneau euclidien (donc factoriel) $\mathbb{Z}[i]$.
    \item \textbf{Théorème des deux carrés de Fermat :} Un premier $p$ est une somme de deux carrés si et seulement si $p \equiv 1 \pmod 4$.
    \item \textbf{Idée de la preuve :} $p = a^2+b^2 = (a+ib)(a-ib)$. $p$ est une somme de deux carrés ssi il n'est plus irréductible dans $\mathbb{Z}[i]$. On montre que cela arrive ssi $-1$ est un carré modulo $p$.
\end{itemize}
\subsection{L'Équation de Pell-Fermat $x^2 - d y^2 = 1$}
\begin{itemize}
    \item \textbf{Le cadre :} On travaille dans l'anneau $\mathbb{Z}[\sqrt{d}]$. L'équation se réécrit $(x-y\sqrt{d})(x+y\sqrt{d})=1$. Les solutions correspondent aux \textbf{unités} (éléments inversibles) de cet anneau.
    \item \textbf{Théorème des Unités de Dirichlet :} Le groupe des unités de $\mathbb{Z}[\sqrt{d}]$ (pour $d>1$ sans facteur carré) est de la forme $\{\pm 1\} \times \varepsilon^\mathbb{Z}$, où $\varepsilon$ est l'unité fondamentale.
    \item \textbf{Conséquence :} Si on connaît la solution fondamentale $(x_1, y_1)$, toutes les autres solutions $(x_n, y_n)$ sont obtenues par $(x_n+y_n\sqrt{d}) = (x_1+y_1\sqrt{d})^n$.
\end{itemize}

\section{Méthodes Géométriques : Points Rationnels sur les Courbes}
\subsection{Les Coniques : Paramétrisation Rationnelle}
\begin{itemize}
    \item \textbf{Principe :} Si l'on connaît un point rationnel sur une conique à coefficients rationnels, on peut trouver tous les autres en considérant le faisceau des droites passant par ce point.
    \item \textbf{Application : Triplets Pythagoriciens.} L'équation $x^2+y^2=z^2$ est homogène à l'équation du cercle unité $X^2+Y^2=1$. En partant du point rationnel $(-1,0)$, on trouve la paramétrisation rationnelle bien connue.
\end{itemize}
\subsection{Les Courbes Elliptiques}
\begin{itemize}
    \item \textbf{Définition :} Ce sont des courbes d'équation $y^2 = x^3+ax+b$.
    \item \textbf{Loi de Groupe :} L'ensemble des points rationnels d'une courbe elliptique, muni d'un point à l'infini, peut être doté d'une structure de groupe abélien. L'addition de points se fait par une construction géométrique ("la droite qui passe par P et Q recoupe la courbe en un troisième point...").
    \item \textbf{Théorème de Mordell-Weil :} Le groupe des points rationnels d'une courbe elliptique est un groupe abélien de type fini.
    \item \textbf{Le Grand Théorème de Fermat :} La preuve de Wiles repose sur la démonstration d'une conjecture (Shimura-Taniyama-Weil) qui lie les courbes elliptiques aux formes modulaires.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Le cas $n=4$ du Grand Théorème de Fermat par descente infinie :} Un développement très classique et formateur.
        \item \textbf{Théorème des deux carrés de Fermat via les entiers de Gauss :} Un bijou qui montre la puissance de la factorisation dans des anneaux plus grands.
        \item \textbf{Structure des solutions de l'équation de Pell-Fermat :} Un développement qui introduit la notion d'unité d'un anneau quadratique.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Manquer de variété :} La leçon doit présenter un panel de méthodes différentes.
        \item \textbf{Faire des preuves "à la main" sans citer la structure :} Pour Pell-Fermat, il faut parler d'unités. Pour les deux carrés, de $\mathbb{Z}[i]$.
        \item \textbf{Penser que l'on peut tout résoudre :} Il faut mentionner que de nombreuses équations diophantiennes sont des problèmes ouverts (10ème problème de Hilbert).
    \end{itemize}
\end{erreurs}


\part{Leçons sur les Anneaux, Polynômes, Algèbre Linéaire et Géométrie}

\chapter{Leçon 140 : Corps des fractions rationnelles. Exemples et applications.}

\begin{philosophie}
    Cette leçon est au carrefour de l'algèbre (construction de corps, arithmétique des polynômes) et de l'analyse (intégration, développements en série). L'objet d'étude, le corps $K(X)$, doit être présenté sous cette double facette. Il faut d'abord le construire rigoureusement comme le plus petit corps contenant les polynômes, puis montrer que l'outil principal pour l'étudier, la décomposition en éléments simples, est la clé de ses applications les plus importantes, notamment en calcul intégral.
\end{philosophie}

\section{Construction et Structure Algébrique de $K(X)$}

\subsection{Le Corps des Fractions d'un Anneau Intègre}
\begin{itemize}
    \item \textbf{Construction générale :} Soit $A$ un anneau intègre. On construit son corps des fractions $Fr(A)$ comme l'ensemble des classes d'équivalence de paires $(a,b) \in A \times A \setminus \{0\}$ pour la relation $(a,b) \sim (c,d) \iff ad=bc$.
    \item \textbf{Propriété Universelle :} $Fr(A)$ est le "plus petit" corps contenant $A$. Tout morphisme injectif d'anneaux de $A$ dans un corps $L$ se factorise de manière unique à travers l'injection canonique $A \to Fr(A)$.
    \item \textbf{Application à $K[X]$ :} L'anneau des polynômes $K[X]$ est intègre. Son corps des fractions est le corps des fractions rationnelles, noté $K(X)$.
\end{itemize}

\subsection{Structure de $K(X)$}
\begin{itemize}
    \item \textbf{Degré d'une fraction rationnelle :} $\deg(P/Q) = \deg(P) - \deg(Q)$.
    \item \textbf{Structure d'espace vectoriel :} $K(X)$ est un $K$-espace vectoriel de dimension infinie.
    \item \textbf{Automorphismes de $K(X)$ (Théorème de Lüroth) :} Les automorphismes de $K(X)$ qui fixent $K$ sont les homographies $X \mapsto \frac{aX+b}{cX+d}$.
\end{itemize}

\section{Décomposition en Éléments Simples : L'Outil Central}

\subsection{Pôles et Partie Entière}
\begin{itemize}
    \item \textbf{Définition (Pôle) :} Un scalaire $a \in K$ est un pôle d'une fraction $F=P/Q$ si $a$ est une racine de $Q$. L'ordre du pôle est la multiplicité de la racine.
    \item \textbf{Théorème (Division Euclidienne) :} Toute fraction rationnelle $F$ se décompose de manière unique en $F = E + H$, où $E$ est un polynôme (la \textbf{partie entière}) et $H$ est une fraction propre ($\deg H < 0$).
\end{itemize}

\subsection{Théorème de Décomposition}
\begin{itemize}
    \item \textbf{Sur $\mathbb{C}(X)$ :} Toute fraction rationnelle propre se décompose de manière unique en une somme de termes de la forme $\frac{c}{(X-a)^k}$, où les $a$ sont les pôles.
    \item \textbf{Sur $\mathbb{R}(X)$ :} La décomposition se fait sur les facteurs irréductibles de $\mathbb{R}[X]$ (degré 1 et 2). Toute fraction rationnelle propre se décompose de manière unique en une somme de termes de la forme $\frac{c}{(X-a)^k}$ et $\frac{cX+d}{(X^2+\alpha X+\beta)^k}$.
\end{itemize}
\begin{remark}
    La preuve de ce théorème repose sur l'arithmétique de l'anneau principal $K[X]$ (identité de Bézout). C'est un pur résultat d'algèbre.
\end{remark}

\section{Applications}

\subsection{Application Principale : Calcul de Primitives}
\begin{itemize}
    \item \textbf{Principe :} La décomposition en éléments simples est la stratégie systématique pour calculer une primitive de n'importe quelle fraction rationnelle.
    \item \textbf{Primitives des éléments simples sur $\mathbb{R}$ :}
        \begin{itemize}
            \item $\int \frac{dx}{(x-a)^k}$ se calcule directement.
            \item $\int \frac{cx+d}{(x^2+\alpha x+\beta)^k}dx$ se ramène par des changements de variables à des primitives de la forme $\int \frac{du}{(u^2+1)^k}$ et $\int \frac{u du}{(u^2+1)^k}$.
        \end{itemize}
\end{itemize}

\subsection{Applications en Analyse}
\begin{itemize}
    \item \textbf{Développement en série entière :} Une fraction rationnelle dont les pôles sont de module $> R$ est développable en série entière sur le disque $D(0,R)$.
    \item \textbf{Application aux suites récurrentes linéaires :} La série génératrice d'une suite récurrente linéaire est une fraction rationnelle. La décomposer en éléments simples permet de trouver la forme explicite du terme général de la suite.
\end{itemize}

\subsection{Applications en Algèbre Linéaire}
\begin{itemize}
    \item \textbf{Réduction des endomorphismes :} L'inverse de l'opérateur $(u-\lambda \mathrm{Id})$ est une fraction rationnelle en $u$. La décomposition en éléments simples de cette fraction (la résolvante) est liée à la décomposition de l'espace en sous-espaces caractéristiques.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration du théorème de décomposition en éléments simples :} Un développement très formateur qui montre la maîtrise de l'arithmétique de $K[X]$.
        \item \textbf{Calcul d'une primitive d'une fraction rationnelle non triviale :} Par exemple, $\int \frac{dx}{x^4+1}$.
        \item \textbf{Lien entre séries génératrices et suites récurrentes linéaires :} Un très beau développement qui connecte cette leçon à l'analyse et aux probabilités (chaînes de Markov).
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier la partie entière !} C'est l'erreur la plus classique.
        \item \textbf{Se tromper dans la forme de la décomposition sur $\mathbb{R}$ :} Les termes de seconde espèce sont essentiels.
        \item \textbf{Faire une leçon qui n'est qu'une recette de calcul :} Il faut insister sur la structure algébrique (corps des fractions, arithmétique de $K[X]$) qui sous-tend la décomposition.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 141 : Polynômes irréductibles à une indéterminée. Corps de rupture. Exemples et applications.}

\begin{philosophie}
    La notion d'irréductibilité est l'analogue de la primalité pour les polynômes. C'est le concept qui permet de distinguer les "atomes" indivisibles dans l'anneau $K[X]$. Cette leçon est fondamentale car elle est la porte d'entrée de la théorie des extensions de corps : on construit des corps plus grands en "adjoignant" des racines à des polynômes irréductibles. Le plan doit donc montrer cette double facette : d'abord, comment tester l'irréductibilité, ensuite, à quoi cela sert (construire des corps).
\end{philosophie}

\section{Arithmétique de l'Anneau des Polynômes $K[X]$}
\subsection{Une Structure Euclidienne}
\begin{itemize}
    \item \textbf{Théorème :} Si $K$ est un corps, l'anneau $K[X]$ est euclidien pour le degré.
    \item \textbf{Conséquences :} $K[X]$ est principal, donc factoriel. L'arithmétique y est donc "gentille" : on dispose de la division euclidienne, de l'identité de Bézout, du lemme de Gauss, et de l'unicité de la décomposition en facteurs irréductibles.
\end{itemize}
\subsection{Irréductibilité : la Notion Clé}
\begin{itemize}
    \item \textbf{Définition :} Un polynôme $P$ est irréductible s'il n'est pas constant et si ses seuls diviseurs sont les constantes et les polynômes qui lui sont associés.
    \item \textbf{Lien avec les idéaux :} $P$ est irréductible $\iff$ l'idéal $(P)$ est maximal parmi les idéaux principaux propres. Comme $K[X]$ est principal, $P$ est irréductible $\iff (P)$ est un idéal maximal.
\end{itemize}

\section{Critères Pratiques d'Irréductibilité}
\subsection{Le Cas des Petits Degrés}
\begin{itemize}
    \item Un polynôme de degré 2 ou 3 est irréductible si et seulement s'il n'a pas de racine dans $K$.
    \item \textbf{Exemple :} $X^2+X+1$ est irréductible sur $\mathbb{F}_2[X]$, mais pas sur $\mathbb{F}_3[X]$ (car 1 est racine).
\end{itemize}
\subsection{Le Pont entre $A[X]$ et $K[X]$ : le Lemme de Gauss}
\begin{itemize}
    \item \textbf{Théorème :} Si $A$ est un anneau factoriel de corps des fractions $K$, alors un polynôme de $A[X]$ est irréductible sur $K[X]$ si et seulement s'il est primitif et irréductible sur $A[X]$.
    \item \textbf{Application fondamentale :} Pour tester l'irréductibilité d'un polynôme de $\mathbb{Z}[X]$ sur $\mathbb{Q}$, il suffit de le faire sur $\mathbb{Z}$.
\end{itemize}
\subsection{Les Outils Classiques sur $\mathbb{Z}[X]$}
\begin{itemize}
    \item \textbf{Critère d'Eisenstein :} Soit $P(X) = a_n X^n + \dots + a_0 \in \mathbb{Z}[X]$. S'il existe un premier $p$ tel que $p \nmid a_n$, $p | a_i$ pour $i<n$, et $p^2 \nmid a_0$, alors $P$ est irréductible sur $\mathbb{Q}$.
    \item \textbf{Réduction modulo $p$ :} Si la réduction $\bar{P}$ de $P$ modulo $p$ est irréductible dans $\mathbb{F}_p[X]$ (et $\deg(\bar{P})=\deg(P)$), alors $P$ est irréductible sur $\mathbb{Q}$.
\end{itemize}

\section{Construction de Corps : le Corps de Rupture}
\subsection{Construction du Corps de Rupture}
\begin{itemize}
    \item \textbf{Théorème :} Si $P$ est irréductible sur $K$, alors l'anneau quotient $L = K[X]/(P)$ est un corps.
    \item \textbf{Propriétés :} $L$ est une extension de $K$ de degré $\deg(P)$. La classe de $X$ dans $L$ est une racine de $P$. $L$ est un corps de rupture de $P$.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Construction des Corps Finis :} On construit $\mathbb{F}_{p^n}$ comme le corps de rupture d'un polynôme irréductible de degré $n$ sur $\mathbb{F}_p$. Par exemple, $\mathbb{F}_4 \cong \mathbb{F}_2[X]/(X^2+X+1)$.
    \item \textbf{Vers la Théorie de Galois :} La construction itérée de corps de rupture permet de construire le corps de décomposition d'un polynôme, qui est le cadre de la théorie de Galois.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Critère d'Eisenstein :} La preuve est élégante et repose sur une réduction modulo $p$.
        \item \textbf{Irréductibilité des polynômes cyclotomiques $\Phi_p(X)$ sur $\mathbb{Q}$ :} Un grand classique qui utilise une astuce avec le critère d'Eisenstein translaté.
        \item \textbf{Construction et table de multiplication du corps $\mathbb{F}_8$ :} Un développement concret qui montre une maîtrise de la construction et de l'arithmétique des corps finis.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre irréductible sur $\mathbb{Z}[X]$ et sur $\mathbb{Q}[X]$ :} Le lemme de Gauss est là pour faire le lien, il faut le maîtriser.
        \item \textbf{Oublier les hypothèses :} La réduction modulo $p$ ne fonctionne que si le degré ne baisse pas.
        \item \textbf{Confondre corps de rupture et corps de décomposition :} C'est une erreur classique, il faut savoir donner des exemples où les deux diffèrent.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 142 : Algèbre des polynômes à plusieurs indéterminées. Applications.}

\begin{philosophie}
    Cette leçon marque une montée en abstraction par rapport à l'anneau $K[X]$. L'objectif est de montrer que, si l'on perd des propriétés fortes (la principalité, l'algorithme d'Euclide), on gagne un pouvoir de modélisation immense, notamment en géométrie. La leçon doit s'articuler autour des deux grands théorèmes de structure : la factorialité (si l'anneau de base l'est) et surtout la noethérianité (Théorème de la base de Hilbert), qui est la clé de la géométrie algébrique.
\end{philosophie}

\section{Construction et Structure de l'Anneau $A[X_1, \dots, X_n]$}

\subsection{Construction et Propriété Universelle}
\begin{itemize}
    \item \textbf{Construction par récurrence :} On définit $A[X_1, \dots, X_n] = (A[X_1, \dots, X_{n-1}])[X_n]$.
    \item \textbf{Propriété Universelle :} Pour tout morphisme d'anneaux $\phi: A \to B$ et tout $n$-uplet $(b_1, \dots, b_n) \in B^n$, il existe un unique morphisme $\Phi: A[X_1, \dots, X_n] \to B$ qui prolonge $\phi$ et envoie $X_i$ sur $b_i$. C'est l'objet "le plus libre" que l'on puisse construire.
\end{itemize}

\subsection{Propriétés Arithmétiques Fondamentales}
\begin{itemize}
    \item \textbf{Intégrité :} Si $A$ est un anneau intègre, alors $A[X_1, \dots, X_n]$ l'est aussi.
    \item \textbf{Factorialité (Théorème de Gauss généralisé) :} Si $A$ est un anneau factoriel, alors $A[X_1, \dots, X_n]$ l'est aussi.
    \item \textbf{Le Grand Absent : la Principalité.}
        \begin{itemize}
            \item \textbf{Théorème :} Si $n \ge 2$, l'anneau $K[X_1, \dots, X_n]$ (où $K$ est un corps) n'est \textbf{jamais} principal.
            \item \textbf{Contre-exemple canonique :} Dans $K[X,Y]$, l'idéal $(X,Y)$ engendré par $X$ et $Y$ n'est pas principal.
        \end{itemize}
\end{itemize}

\subsection{La Propriété Structurelle Clé : La Noethérianité}
\begin{itemize}
    \item \textbf{Définition (Anneau Noethérien) :} Un anneau est noethérien si toute suite croissante d'idéaux est stationnaire. De manière équivalente, tout idéal est de type fini.
    \item \textbf{Théorème de la Base de Hilbert :} Si $A$ est un anneau noethérien, alors l'anneau de polynômes $A[X]$ est noethérien.
    \item \textbf{Corollaire :} Les anneaux $K[X_1, \dots, X_n]$ et $\mathbb{Z}[X_1, \dots, X_n]$ sont noethériens.
\end{itemize}
\begin{remark}
    C'est le résultat le plus important de la théorie. La perte de la principalité est compensée par cette propriété plus faible mais extraordinairement puissante, qui garantit que tout système d'équations polynomiales peut être remplacé par un système fini.
\end{remark}

\section{Applications}

\subsection{Application Fondamentale : Le Pont vers la Géométrie Algébrique}
\begin{itemize}
    \item \textbf{Le Dictionnaire :} Il existe une correspondance (renversant l'inclusion) entre les idéaux de $K[X_1, \dots, X_n]$ et les ensembles algébriques (les "variétés") de $K^n$.
    \item \textbf{Théorème des Zéros de Hilbert (Nullstellensatz) :} Si $K$ est algébriquement clos, ce dictionnaire est une bijection entre les idéaux radicaux et les ensembles algébriques. Les idéaux maximaux correspondent aux points.
\end{itemize}

\subsection{Polynômes Symétriques}
\begin{itemize}
    \item \textbf{L'anneau des polynômes symétriques $\Lambda_n$ :} C'est un sous-anneau de $K[X_1, \dots, X_n]$.
    \item \textbf{Théorème Fondamental :} $\Lambda_n$ est lui-même un anneau de polynômes, isomorphe à $K[\sigma_1, \dots, \sigma_n]$, où les $\sigma_k$ sont les polynômes symétriques élémentaires.
\end{itemize}

\subsection{Bases de Gröbner : L'Algorithme d'Euclide Généralisé}
\begin{itemize}
    \item \textbf{Motivation :} Puisque les idéaux ne sont pas principaux, comment travailler avec eux ? Les bases de Gröbner sont de "bonnes" parties génératrices pour un idéal, qui permettent de généraliser l'algorithme de division.
    \item \textbf{Application :} Résolution explicite de systèmes d'équations polynomiales.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de la base de Hilbert :} Un développement majeur et un grand classique.
        \item \textbf{L'idéal $(X,Y)$ de $K[X,Y]$ n'est pas principal :} Un contre-exemple essentiel, très formateur à rédiger proprement.
        \item \textbf{Démonstration du théorème fondamental des polynômes symétriques :} Un développement élégant qui utilise la structure de l'anneau.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les propriétés de $K[X]$ et $K[X,Y]$ :} C'est l'erreur principale. Il faut insister sur la perte de la principalité.
        \item \textbf{Sous-estimer le théorème de la base de Hilbert :} C'est le cœur de la leçon. Il faut savoir l'énoncer et donner son importance.
        \item \textbf{Ne pas faire le lien avec la géométrie :} Les polynômes à plusieurs indéterminées sont l'alphabet de la géométrie algébrique. Ne pas le mentionner serait une faute de goût.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 144 : Racines d'un polynôme. Fonctions symétriques. Applications.}

\begin{philosophie}
    Cette leçon explore le lien "magique" entre les coefficients d'un polynôme (que l'on connaît) et ses racines (que l'on ne connaît généralement pas). Le théorème fondamental des polynômes symétriques est le dictionnaire qui permet cette traduction. La leçon doit montrer comment cet outil purement algébrique a des applications profondes en théorie des équations, en théorie de Galois, et permet de calculer des quantités qui semblent inaccessibles.
\end{philosophie}

\section{Relations Coefficients-Racines et Fonctions Symétriques}
\subsection{Polynômes Symétriques Élémentaires}
\begin{itemize}
    \item \textbf{Définition :} $\sigma_k = \sum_{1 \le i_1 < \dots < i_k \le n} X_{i_1} \cdots X_{i_k}$.
    \item \textbf{Relations de Viète :} Si $P(X) = \prod_{i=1}^n (X-x_i) = X^n + a_{n-1}X^{n-1} + \dots + a_0$, alors $a_{n-k} = (-1)^k \sigma_k(x_1, \dots, x_n)$. Les coefficients sont (au signe près) les évaluations des polynômes symétriques élémentaires sur les racines.
\end{itemize}
\subsection{Le Théorème Fondamental}
\begin{itemize}
    \item \textbf{Théorème :} Tout polynôme symétrique à coefficients dans un anneau $A$ peut s'écrire de manière \textbf{unique} comme un polynôme en les polynômes symétriques élémentaires à coefficients dans $A$.
    \item \textbf{Algorithme de preuve :} La preuve est constructive et fournit un algorithme (basé sur l'ordre lexicographique) pour trouver cette expression.
\end{itemize}
\begin{remark}
    Ce théorème est extraordinaire. Il implique que toute quantité symétrique dépendant des racines peut être calculée \textit{uniquement} à partir des coefficients du polynôme, sans jamais avoir à trouver les racines elles-mêmes.
\end{remark}

\section{Applications au Calcul sur les Racines}
\subsection{Les Sommes de Newton}
\begin{itemize}
    \item \textbf{Définition :} Les sommes de puissances $S_k = \sum_{i=1}^n x_i^k$.
    \item \textbf{Formules de Newton :} Ce sont des relations de récurrence qui lient les sommes de puissances $S_k$ aux polynômes symétriques élémentaires $\sigma_j$. Elles permettent de calculer les unes à partir des autres.
\end{itemize}
\subsection{Discriminant et Résultant}
\begin{itemize}
    \item \textbf{Discriminant :} $\Delta = \prod_{i<j} (x_i-x_j)^2$. C'est une fonction symétrique des racines. On peut donc l'exprimer en fonction des coefficients. Il est nul si et seulement si le polynôme a une racine multiple.
    \item \textbf{Résultant :} Le résultant de deux polynômes $P$ et $Q$ est une expression symétrique des racines des deux polynômes, qui s'exprime donc en fonction de leurs coefficients. Il est nul si et seulement si $P$ et $Q$ ont une racine commune.
\end{itemize}
\subsection{Transformation de Polynômes}
\begin{itemize}
    \item \textbf{Exemple :} Si on connaît le polynôme $P$ dont les racines sont $x_i$, on peut construire le polynôme $Q$ dont les racines sont $x_i^2$. Les coefficients de $Q$ sont les polynômes symétriques élémentaires des $x_i^2$, qui sont eux-mêmes des polynômes symétriques des $x_i$, donc calculables.
\end{itemize}

\section{Applications en Théorie de Galois et Théorie des Nombres}
\subsection{Entiers Algébriques}
\begin{itemize}
    \item \textbf{Théorème :} Un nombre algébrique est un entier algébrique (racine d'un polynôme unitaire à coefficients entiers) si et seulement si son polynôme minimal sur $\mathbb{Q}$ est à coefficients dans $\mathbb{Z}$.
    \item \textbf{Application :} L'ensemble des entiers algébriques forme un anneau. La preuve utilise les polynômes symétriques pour construire le polynôme minimal de la somme et du produit.
\end{itemize}
\subsection{Théorie de Galois}
\begin{itemize}
    \item \textbf{Discriminant et Groupe de Galois :} Pour un polynôme irréductible $P$, son groupe de Galois est un sous-groupe de $\mathcal{A}_n$ si et seulement si son discriminant est un carré dans le corps de base.
    \item \textbf{Exemple :} Pour un polynôme de degré 3, cela permet de distinguer entre les groupes de Galois $\mathcal{A}_3$ et $\mathfrak{S}_3$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration des formules de Newton :} Un développement calculatoire mais très classique.
        \item \textbf{Le discriminant d'un polynôme de degré 3 :} Calcul explicite et application à la détermination du nombre de racines réelles.
        \item \textbf{L'ensemble des entiers algébriques est un anneau :} Un développement plus abstrait qui montre une grande maîtrise de la théorie.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier l'unicité} dans le théorème fondamental.
        \item \textbf{Ne pas savoir mener un calcul} d'expression d'un polynôme symétrique en fonction des $\sigma_k$ sur un exemple.
        \item \textbf{Confondre résultant et discriminant.}
    \end{itemize}
\end{erreurs}

\newpage

\chapter{Leçon 150 : Exemples d'actions de groupes sur les espaces de matrices.}

\begin{philosophie}
    C'est une leçon transversale par excellence, qui teste la capacité à unifier plusieurs chapitres de l'algèbre linéaire sous une même bannière : la théorie des actions de groupes. L'objectif est de montrer que des problèmes fondamentaux comme la réduction des endomorphismes ou la classification des formes quadratiques peuvent être réinterprétés de manière très élégante comme des problèmes de classification d'orbites. Le plan doit s'organiser par type d'action.
\end{philosophie}

\section{Action par Similitude : Classification des Endomorphismes}

\subsection{Le Cadre de l'Action}
\begin{itemize}
    \item \textbf{L'action :} Le groupe $GL_n(K)$ agit sur l'espace $\mathcal{M}_n(K)$ par \textbf{conjugaison} (ou similitude) : $(P,A) \mapsto PAP^{-1}$.
    \item \textbf{Les orbites :} Les orbites de cette action sont les \textbf{classes de similitude}. Deux matrices sont dans la même orbite si et seulement si elles représentent le même endomorphisme dans des bases différentes.
    \item \textbf{Les invariants :} Les invariants de l'action sont les quantités qui sont constantes sur une orbite : le rang, le déterminant, la trace, le polynôme caractéristique, le polynôme minimal.
\end{itemize}

\subsection{Le Problème de Classification des Orbites}
\begin{itemize}
    \item \textbf{Objectif :} Trouver un représentant "simple" et unique dans chaque orbite.
    \item \textbf{Sur un corps algébriquement clos (e.g., $\mathbb{C}$) :} Les invariants ne suffisent pas à classifier (penser aux matrices nilpotentes). La classification complète est donnée par la \textbf{réduction de Jordan}. La forme normale de Jordan est le représentant canonique de chaque classe de similitude. Le nombre et la taille des blocs de Jordan sont des invariants complets de l'orbite.
    \item \textbf{Sur $\mathbb{R}$ :} La classification est plus complexe (réduction de Frobenius avec les matrices compagnons).
\end{itemize}

\section{Action par Congruence : Classification des Formes Quadratiques}

\subsection{Le Cadre de l'Action}
\begin{itemize}
    \item \textbf{L'action :} Le groupe $GL_n(K)$ agit sur l'espace des matrices symétriques $\mathcal{S}_n(K)$ par \textbf{congruence} : $(P,S) \mapsto P^T S P$.
    \item \textbf{Les orbites :} Les orbites sont les \textbf{classes de congruence}. Deux matrices sont dans la même orbite si et seulement si elles représentent la même forme quadratique dans des bases différentes.
\end{itemize}

\subsection{Le Problème de Classification des Orbites}
\begin{itemize}
    \item \textbf{Invariant général :} Le rang de la matrice.
    \item \textbf{Sur $\mathbb{C}$ :} Le rang est le seul invariant. Il y a $n+1$ orbites.
    \item \textbf{Sur $\mathbb{R}$ :} Le rang ne suffit plus. La classification complète est donnée par la \textbf{Loi d'Inertie de Sylvester}. L'invariant complet est la \textbf{signature} $(s,t)$. Il y a $\frac{(n+1)(n+2)}{2}$ orbites.
\end{itemize}

\section{Action par Équivalence : Classification par le Rang}

\subsection{Le Cadre de l'Action}
\begin{itemize}
    \item \textbf{L'action :} Le groupe $GL_n(K) \times GL_p(K)$ agit sur $\mathcal{M}_{n,p}(K)$ par \textbf{équivalence} : $((Q,P), A) \mapsto Q^{-1}AP$.
    \item \textbf{Les orbites :} Deux matrices sont dans la même orbite si et seulement si elles représentent la même application linéaire dans des bases différentes au départ et à l'arrivée.
\end{itemize}

\subsection{Le Problème de Classification des Orbites}
\begin{itemize}
    \item \textbf{Théorème du rang :} Le seul invariant de cette action est le \textbf{rang}.
    \item \textbf{Classification :} Il y a $\min(n,p)+1$ orbites. Le représentant canonique de l'orbite de rang $r$ est la matrice $J_r = \begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Loi d'inertie de Sylvester :} Un développement parfait pour cette leçon, qui est une classification d'orbites.
        \item \textbf{La réduction de Jordan comme classification des orbites nilpotentes :} Un développement de haut niveau qui montre une grande compréhension de la structure des orbites par similitude.
        \item \textbf{Théorème de Witt :} Un théorème de prolongement pour les isométries relatives à une forme quadratique, qui s'interprète en termes d'actions de groupes.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les différentes actions :} C'est l'erreur capitale. Il faut être très clair sur quel groupe agit, sur quel ensemble, et comment.
        \item \textbf{Ne pas connaître les invariants de chaque action :} C'est le cœur de la leçon.
        \item \textbf{Présenter les résultats (Jordan, Sylvester) sans le vocabulaire des actions :} Cela montrerait qu'on n'a pas compris le titre de la leçon. Il faut parler d'orbites, d'invariants, de représentants.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 151 : Dimension d'un espace vectoriel, rang. Exemples et applications.}

\begin{philosophie}
    Cette leçon semble élémentaire, mais c'est un piège. Le jury attend un traitement de très haut niveau sur un concept fondamental. L'objectif n'est pas de refaire un cours de L1, mais de montrer la \textbf{puissance} et l'\textbf{universalité} de la notion de dimension. Le plan doit partir des fondements théoriques pour rapidement s'envoler vers des applications riches et variées, montrant que la dimension et le rang sont des outils qui irriguent toute l'algèbre, mais aussi l'analyse et la géométrie.
\end{philosophie}

\section{Le Concept de Dimension : un Invariant Fondamental}
\subsection{Existence et Unicité}
\begin{itemize}
    \item \textbf{Théorème de la base incomplète :} Le lemme fondamental qui garantit l'existence de bases.
    \item \textbf{Théorème de la Dimension :} Toutes les bases d'un même espace vectoriel ont le même cardinal. La dimension est donc bien définie.
    \item \textbf{Conséquence :} Deux espaces vectoriels de dimension finie sont isomorphes si et seulement s'ils ont la même dimension. La dimension classifie entièrement les espaces vectoriels à isomorphisme près.
\end{itemize}
\subsection{Propriétés en Dimension Finie}
\begin{itemize}
    \item \textbf{Formule de Grassmann :} $\dim(F+G) = \dim(F) + \dim(G) - \dim(F \cap G)$.
    \item \textbf{Codimension :} $\dim(E/F) = \dim(E) - \dim(F)$.
\end{itemize}

\section{Le Rang : une Notion Unificatrice}
\subsection{Les Trois Visages du Rang}
\begin{itemize}
    \item \textbf{Rang d'une application linéaire $f$ :} $\mathrm{rg}(f) = \dim(\mathrm{Im}(f))$.
    \item \textbf{Rang d'une famille de vecteurs :} La dimension du sous-espace qu'ils engendrent.
    \item \textbf{Rang d'une matrice $A$ :} La dimension de l'espace engendré par ses colonnes (ou ses lignes).
\end{itemize}
\subsection{Théorèmes Fondamentaux}
\begin{itemize}
    \item \textbf{Théorème du Rang :} Pour $f \in \mathcal{L}(E,F)$, $\dim(E) = \dim(\ker(f)) + \mathrm{rg}(f)$.
    \item \textbf{Théorème d'Égalité des Rangs :} Pour une matrice, le rang des colonnes est égal au rang des lignes.
\end{itemize}
\begin{remark}
    Le rang est la notion qui connecte l'objet algébrique (matrice, famille) à l'objet géométrique (image, sous-espace). Le théorème du rang est une "loi de conservation de la dimension" au cours d'une transformation linéaire.
\end{remark}

\section{Applications Variées de la Dimension et du Rang}
\subsection{Applications en Algèbre Linéaire}
\begin{itemize}
    \item \textbf{Systèmes Linéaires :} Le rang de la matrice d'un système $Ax=b$ détermine l'existence et la structure de l'ensemble des solutions.
    \item \textbf{Interpolation de Lagrange :} Le rang de la matrice de Vandermonde garantit l'existence et l'unicité du polynôme interpolateur.
\end{itemize}
\subsection{Applications en Analyse}
\begin{itemize}
    \item \textbf{Équations Différentielles Linéaires :} L'ensemble des solutions d'une EDO linéaire homogène d'ordre $n$ est un espace vectoriel de dimension $n$.
    \item \textbf{Espaces de Fonctions :} La dimension peut être infinie. On peut trouver des familles libres de cardinal infini dans $\mathcal{C}([0,1])$.
\end{itemize}
\subsection{Applications en Géométrie}
\begin{itemize}
    \item \textbf{Dimension de l'intersection de sous-espaces affines.}
    \item \textbf{Variétés Grassmaniennes :} L'ensemble des sous-espaces de dimension $k$ d'un espace de dimension $n$ forme une variété dont on peut calculer la dimension.
\end{itemize}
\subsection{Applications aux Codes Correcteurs d'Erreurs}
\begin{itemize}
    \item Un code linéaire est un sous-espace vectoriel de $(\mathbb{F}_q)^n$. Sa dimension $k$ est le nombre de bits d'information. La distance minimale du code est liée au rang de la matrice de contrôle.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration du théorème du rang :} Un grand classique indispensable.
        \item \textbf{Égalité du rang des lignes et du rang des colonnes :} Une preuve élégante utilise la dualité.
        \item \textbf{Dimension de l'espace des matrices symétriques et antisymétriques :} Un développement simple mais efficace.
        \item \textbf{Majoration du rang de la somme de deux matrices :} $\mathrm{rg}(A+B) \le \mathrm{rg}(A)+\mathrm{rg}(B)$.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un plan de niveau L1 :} Il faut impérativement aller au-delà des définitions de base et montrer des applications riches et variées.
        \item \textbf{Ne pas savoir prouver les grands théorèmes :} La preuve du théorème du rang ou de l'égalité des rangs est un attendu.
        \item \textbf{Confondre les différentes notions de rang :} Il faut savoir expliquer comment elles sont toutes unifiées par le rang de l'application linéaire associée.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 152 : Déterminant. Exemples et applications.}

\begin{philosophie}
    Le déterminant est un objet protéiforme. Il est à la fois un outil de calcul (formules), un objet algébrique (forme n-linéaire alternée) et un concept géométrique (volume orienté). Une bonne leçon sur le déterminant doit mettre en lumière ces trois facettes et montrer comment elles s'éclairent mutuellement. Le plan doit partir de la définition la plus théorique pour en déduire les propriétés et les applications.
\end{philosophie}

\section{Le Déterminant comme Forme n-linéaire Alternée}
\subsection{Espace des Formes n-linéaires Alternées}
\begin{itemize}
    \item \textbf{Définition :} Forme n-linéaire, forme alternée.
    \item \textbf{Théorème :} Sur un $K$-espace vectoriel de dimension $n$, l'espace des formes n-linéaires alternées est une droite vectorielle (un espace de dimension 1).
\end{itemize}
\begin{remark}
    C'est le point de départ conceptuel. Il nous dit qu'il n'y a, à un facteur d'échelle près, qu'une seule manière "naturelle" de mesurer un volume orienté en dimension $n$.
\end{remark}
\subsection{Définition Intrinsèque du Déterminant}
\begin{itemize}
    \item \textbf{Déterminant d'une famille de vecteurs :} Soit $\mathcal{B}$ une base de $E$. Le déterminant dans la base $\mathcal{B}$ est l'unique forme n-linéaire alternée qui vaut 1 sur $\mathcal{B}$.
    \item \textbf{Déterminant d'un endomorphisme :} Soit $u \in \mathcal{L}(E)$. Pour toute forme n-linéaire alternée non nulle $\phi$, l'application $(x_1, \dots, x_n) \mapsto \phi(u(x_1), \dots, u(x_n))$ est aussi n-linéaire alternée, donc proportionnelle à $\phi$. Le facteur de proportionnalité ne dépend que de $u$. C'est le déterminant de $u$.
\end{itemize}

\section{Propriétés et Méthodes de Calcul}
\subsection{Propriétés Fondamentales}
\begin{itemize}
    \item $\det(u \circ v) = \det(u)\det(v)$.
    \item $u$ est un automorphisme $\iff \det(u) \neq 0$.
    \item $\det(u^t) = \det(u)$.
\end{itemize}
\subsection{Expression en Coordonnées et Calculs}
\begin{itemize}
    \item \textbf{Formule de Leibniz :} $\det(A) = \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod a_{i, \sigma(i)}$.
    \item \textbf{Développement par rapport à une ligne/colonne :} Utilisation de la comatrice.
    \item \textbf{Opérations élémentaires :} L'effet des opérations sur les lignes/colonnes (pivot de Gauss) est la méthode la plus efficace en pratique pour calculer un déterminant.
\end{itemize}
\subsection{Polynôme Caractéristique}
\begin{itemize}
    \item \textbf{Définition :} $P_{car,u}(X) = \det(X \mathrm{Id} - u)$. Ses racines sont les valeurs propres de $u$.
\end{itemize}

\section{Applications}
\subsection{Applications Algébriques}
\begin{itemize}
    \item \textbf{Systèmes de Cramer :} Formules de Cramer pour la résolution de systèmes linéaires $Ax=b$ quand $A$ est inversible.
    \item \textbf{Calcul de l'inverse :} $A^{-1} = \frac{1}{\det A} (\mathrm{Com A})^T$.
\end{itemize}
\subsection{Applications Géométriques}
\begin{itemize}
    \item \textbf{Volume orienté :} Dans un espace euclidien orienté, le déterminant d'une famille de vecteurs dans une base orthonormée directe est le volume orienté du parallélépipède qu'ils engendrent.
    \item \textbf{Changement de variables dans les intégrales multiples :} La formule du changement de variables fait apparaître le déterminant de la matrice Jacobienne.
    \item \textbf{Caractérisation des isométries :} Les isométries sont les endomorphismes de déterminant $\pm 1$.
\end{itemize}
\subsection{Applications en Analyse}
\begin{itemize}
    \item \textbf{Wronskien :} Le wronskien d'une famille de solutions d'une EDO linéaire est un déterminant. Sa non-nullité caractérise l'indépendance des solutions.
    \item \textbf{Théorème d'inversion locale :} Une application est un difféomorphisme local si son déterminant jacobien est non nul.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Le déterminant de la comatrice $\det(\mathrm{Com A}) = (\det A)^{n-1}$ :} Un développement calculatoire classique.
        \item \textbf{Déterminants de Vandermonde :} Calcul et applications (interpolation de Lagrange, unicité).
        \item \textbf{Le déterminant comme volume orienté :} Un développement plus géométrique qui montre une compréhension profonde du concept.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Ne connaître que la formule de Leibniz :} Il faut maîtriser la définition intrinsèque par les formes n-linéaires, qui est la "bonne" définition.
        \item \textbf{Confondre déterminant d'une famille de vecteurs et d'un endomorphisme :} Le premier dépend d'une base, le second est intrinsèque.
        \item \textbf{Penser que les formules de Cramer sont une méthode de calcul efficace :} Elles sont d'un grand intérêt théorique, mais numériquement désastreuses.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 153 : Polynômes d'endomorphisme. Applications.}

\begin{philosophie}
    Cette leçon est au cœur de la réduction. La philosophie est de montrer que l'étude d'un endomorphisme $u$ peut être considérablement simplifiée en étudiant la structure de l'algèbre $K[u]$ qu'il engendre. Le morphisme d'évaluation $P \mapsto P(u)$ est l'objet central qui fait le pont entre l'arithmétique de l'anneau (principal) $K[X]$ et la géométrie de l'espace vectoriel $E$. La leçon doit montrer comment les propriétés des polynômes (divisibilité, factorisation) se traduisent en propriétés géométriques (stabilité, décomposition), culminant avec le lemme des noyaux.
\end{philosophie}

\section{L'Algèbre $K[u]$ et le Polynôme Minimal}

\subsection{Le Morphisme d'Évaluation}
\begin{itemize}
    \item \textbf{Définition :} Pour $u \in \mathcal{L}(E)$, on définit le morphisme d'algèbres (d'évaluation) $\Phi_u: K[X] \to \mathcal{L}(E)$ par $\Phi_u(P) = P(u)$.
    \item \textbf{Image et Noyau :} L'image de $\Phi_u$ est l'algèbre $K[u]$ des polynômes en $u$. Le noyau $\ker(\Phi_u)$ est l'idéal des polynômes annulateurs de $u$.
\end{itemize}

\subsection{Le Polynôme Minimal}
\begin{itemize}
    \item \textbf{Théorème (Existence et Unicité) :} L'idéal des polynômes annulateurs n'est pas trivial en dimension finie. Comme $K[X]$ est principal, cet idéal est engendré par un unique polynôme unitaire, le \textbf{polynôme minimal} de $u$, noté $\pi_u$.
    \item \textbf{Propriétés Fondamentales :}
        \begin{itemize}
            \item Pour tout polynôme $P$, $P(u)=0 \iff \pi_u | P$.
            \item L'algèbre $K[u]$ est isomorphe à l'algèbre quotient $K[X]/(\pi_u)$. Sa dimension est $\deg(\pi_u)$.
        \end{itemize}
\end{itemize}

\section{Théorèmes Fondamentaux et Liens avec la Réduction}

\subsection{Le Théorème de Cayley-Hamilton}
\begin{itemize}
    \item \textbf{Énoncé :} Tout endomorphisme annule son propre polynôme caractéristique : $P_{car,u}(u) = 0$.
    \item \textbf{Conséquence Fondamentale :} Le polynôme minimal divise le polynôme caractéristique. Ils ont donc les mêmes racines : les valeurs propres de $u$.
\end{itemize}

\subsection{Le Lemme des Noyaux : Le Scalpel du Chirurgien}
\begin{itemize}
    \item \textbf{Théorème :} Soit $P$ un polynôme annulateur de $u$. Si $P = P_1 \cdots P_k$ est une factorisation de $P$ en polynômes deux à deux premiers entre eux, alors l'espace $E$ se décompose en une somme directe de sous-espaces stables :
    $$ E = \bigoplus_{i=1}^k \ker(P_i(u)) $$
    De plus, le projecteur sur chaque sous-espace est un polynôme en $u$.
\end{itemize}
\begin{remark}
    C'est le résultat le plus puissant de la leçon. Il traduit une propriété arithmétique (factorisation sans facteur commun) en une propriété géométrique (décomposition en somme directe). C'est le principe du "diviser pour régner" en algèbre linéaire.
\end{remark}

\section{Applications à la Réduction}

\subsection{Critères de Diagonalisabilité et Trigonalisabilité}
\begin{itemize}
    \item \textbf{Critère de Trigonalisabilité :} $u$ est trigonalisable $\iff P_{car,u}$ (ou $\pi_u$) est scindé sur $K$.
    \item \textbf{Critère de Diagonalisabilité :} $u$ est diagonalisable $\iff \pi_u$ est scindé à racines simples sur $K$.
\end{itemize}

\subsection{Caractérisation d'Endomorphismes Remarquables}
\begin{itemize}
    \item \textbf{Projecteurs :} Un projecteur $p$ est caractérisé par $p^2=p$. Son polynôme minimal divise $X^2-X = X(X-1)$. Il est donc toujours diagonalisable.
    \item \textbf{Symétries :} Une symétrie $s$ est caractérisée par $s^2=Id$. Son polynôme minimal divise $X^2-1$. Elle est diagonalisable (si car $K \neq 2$).
\end{itemize}

\subsection{Vers les Décompositions Fines}
\begin{itemize}
    \item \textbf{Sous-espaces Caractéristiques :} En appliquant le lemme des noyaux à la factorisation de $\pi_u$ en facteurs primaires, on obtient la décomposition de $E$ en somme directe de ses sous-espaces caractéristiques.
    \item \textbf{Décomposition de Dunford :} L'existence et l'unicité de la décomposition $u=d+n$ repose sur une application du lemme des restes chinois dans l'anneau $K[X]/(\pi_u)$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de Cayley-Hamilton :} La preuve utilisant les matrices compagnons ou la comatrice est un grand classique.
        \item \textbf{Lemme des noyaux :} Un développement fondamental, qui peut être présenté avec la preuve de l'expression des projecteurs comme polynômes.
        \item \textbf{Calcul du polynôme minimal d'une matrice :} Un développement plus pratique qui montre la maîtrise des concepts.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre polynôme minimal et caractéristique :} Il faut savoir donner des exemples où ils sont différents.
        \item \textbf{Mal énoncer le lemme des noyaux :} L'hypothèse "premiers entre eux" est cruciale.
        \item \textbf{Faire une leçon purement théorique :} Le titre est "Applications". Il faut montrer comment les polynômes permettent de décider si un endomorphisme est diagonalisable, de calculer ses puissances, etc.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 154 : Sous-espaces stables par un endomorphisme ou une famille d'endomorphismes.}

\begin{philosophie}
    Cette leçon est la plus "géométrique" de la réduction. Elle s'intéresse aux briques qui composent l'espace du point de vue d'un ou plusieurs endomorphismes. L'objectif est de montrer que la recherche de sous-espaces stables est la clé de la simplification des endomorphismes : trouver une droite stable, c'est trouver un vecteur propre ; trouver un drapeau de sous-espaces stables, c'est trigonaliser. Le cas d'une famille introduit la notion fondamentale de commutation.
\end{philosophie}

\section{Sous-espaces Stables pour un Seul Endomorphisme}

\subsection{Définitions et Exemples Fondamentaux}
\begin{itemize}
    \item \textbf{Définition :} $F$ est stable par $u$ si $u(F) \subset F$. L'endomorphisme $u$ induit alors un endomorphisme $u_F$ sur $F$ et un endomorphisme $u_{E/F}$ sur l'espace quotient $E/F$.
    \item \textbf{Exemples :}
        \begin{itemize}
            \item Les sous-espaces triviaux $\{0\}$ et $E$.
            \item Le noyau $\ker(u)$ et l'image $\mathrm{Im}(u)$.
            \item Les sous-espaces propres $E_\lambda = \ker(u-\lambda \mathrm{Id})$.
            \item Les sous-espaces caractéristiques $N_i = \ker(P_i(u)^{m_i})$.
        \end{itemize}
\end{itemize}

\subsection{Stabilité et Réduction}
\begin{itemize}
    \item \textbf{Décomposition en somme directe :} Si $E = F_1 \oplus \dots \oplus F_k$ où les $F_i$ sont stables, la matrice de $u$ dans une base adaptée est diagonale par blocs. C'est le but de la réduction.
    \item \textbf{Trigonalisation :} Un endomorphisme $u$ est trigonalisable si et seulement s'il existe un drapeau complet de sous-espaces stables $\{0\} = F_0 \subset F_1 \subset \dots \subset F_n = E$.
    \item \textbf{Théorème (Existence de sous-espaces stables) :} Sur $\mathbb{C}$, tout endomorphisme admet au moins une droite stable (un vecteur propre). Sur $\mathbb{R}$, il admet au moins une droite ou un plan stable.
\end{itemize}

\section{Sous-espaces Stables pour une Famille d'Endomorphismes}

\subsection{Le Rôle Central de la Commutation}
\begin{itemize}
    \item \textbf{Définition :} Un sous-espace $F$ est stable par une famille $(u_i)_{i \in I}$ si $u_i(F) \subset F$ pour tout $i$.
    \item \textbf{Lemme Fondamental :} Si $u$ et $v$ commutent ($uv=vu$), alors les sous-espaces propres de l'un sont stables par l'autre.
\end{itemize}

\subsection{Co-trigonalisation et Co-diagonalisation}
\begin{itemize}
    \item \textbf{Théorème (Co-trigonalisation) :} Une famille d'endomorphismes trigonalisables $(u_i)$ est co-trigonalisable (i.e. il existe une base où toutes leurs matrices sont triangulaires supérieures) si et seulement si les endomorphismes commutent deux à deux.
    \item \textbf{Théorème (Co-diagonalisation) :} Une famille d'endomorphismes diagonalisables $(u_i)$ est co-diagonalisable si et seulement si les endomorphismes commutent deux à deux.
\end{itemize}
\begin{remark}
    Ces deux théorèmes sont des résultats très puissants, qui montrent que la commutation est la condition algébrique qui garantit l'existence d'une géométrie commune simplifiée.
\end{remark}

\section{Endomorphismes Irréductibles et Applications}

\subsection{Le Point de Vue des Représentations}
\begin{itemize}
    \item \textbf{Définition :} Un endomorphisme $u$ est \textbf{irréductible} si les seuls sous-espaces stables par $u$ sont $\{0\}$ et $E$.
    \item \textbf{Exemple :} Sur $\mathbb{C}$, les endomorphismes irréductibles n'existent qu'en dimension 1 (homothéties). Sur $\mathbb{R}$, une rotation plane d'angle $\theta \notin \pi\mathbb{Z}$ est irréductible en dimension 2.
\end{itemize}

\subsection{Lemme de Schur}
\begin{itemize}
    \item \textbf{Énoncé :} Soit $u \in \mathcal{L}(E)$ irréductible. L'ensemble des endomorphismes qui commutent avec $u$ (le commutant) est un corps.
    \item \textbf{Application :} Permet de montrer que les seules matrices qui commutent avec toutes les matrices sont les homothéties.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Co-trigonalisation d'une famille d'endomorphismes qui commutent :} Un développement très élégant et un résultat important.
        \item \textbf{Démonstration que tout endomorphisme sur $\mathbb{C}$ admet une droite stable :} Une preuve classique qui utilise le polynôme caractéristique.
        \item \textbf{Lemme de Schur :} Un développement plus abstrait mais qui montre une grande hauteur de vue et des liens avec la théorie des représentations.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier le cas d'une famille :} C'est un point explicite et essentiel du titre de la leçon.
        \item \textbf{Confondre les conditions de co-trigonalisation et co-diagonalisation :} Pour la co-diagonalisation, il faut la commutation ET que chaque endomorphisme soit individuellement diagonalisable.
        \item \textbf{Ne pas faire le lien entre sous-espaces stables et réduction :} C'est le cœur de la leçon.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 155 : Endomorphismes diagonalisables. Exemples et applications.}

\begin{philosophie}
    La diagonalisation est le "cas idéal" de la réduction. C'est la recherche du "point de vue" (la base de vecteurs propres) qui rend l'action d'un endomorphisme la plus simple possible : une collection de dilatations. Le plan doit donc d'abord caractériser ce cas idéal, puis montrer pourquoi cette simplicité est si puissante dans de nombreux domaines (calcul de puissances, EDO, suites récurrentes).
\end{philosophie}

\section{Caractérisation des Endomorphismes Diagonalisables}

\subsection{Condition sur les Sous-Espaces Propres}
\begin{itemize}
    \item \textbf{Théorème :} $u$ est diagonalisable $\iff$ $E = \bigoplus_{\lambda \in \mathrm{Sp}(u)} E_\lambda$.
    \item \textbf{Corollaire :} Une condition suffisante (mais non nécessaire) est que $u$ admette $n$ valeurs propres distinctes.
\end{itemize}

\subsection{Caractérisation par les Polynômes}
\begin{itemize}
    \item \textbf{Théorème :} $u$ est diagonalisable $\iff$ son polynôme minimal est scindé à racines simples sur $K$.
    \item \textbf{Application :} Toute symétrie est diagonalisable (son polynôme minimal divise $X^2-1$). Tout projecteur est diagonalisable (son polynôme minimal divise $X^2-X$).
\end{itemize}

\subsection{Le Cas Euclidien : le Théorème Spectral}
\begin{itemize}
    \item \textbf{Théorème :} Dans un espace euclidien, un endomorphisme est diagonalisable dans une base \textbf{orthonormée} si et seulement s'il est auto-adjoint (symétrique).
    \item \textbf{Remarque :} C'est un résultat beaucoup plus fort, qui garantit en plus une structure géométrique à la base.
\end{itemize}

\section{Applications de la Diagonalisation}

\subsection{Calculs en Algèbre Linéaire}
\begin{itemize}
    \item \textbf{Calcul de puissances :} $A^k = PD^kP^{-1}$.
    \item \textbf{Calcul de l'exponentielle :} $e^A = Pe^DP^{-1}$.
\end{itemize}

\subsection{Systèmes Dynamiques Discrets et Continus}
\begin{itemize}
    \item \textbf{Suites récurrentes linéaires ($U_{n+1}=AU_n$) :} La diagonalisation permet de trouver une formule explicite pour $U_n = A^n U_0$.
    \item \textbf{Systèmes différentiels linéaires ($Y'=AY$) :} Le changement de base $Z=P^{-1}Y$ diagonalise le système, le ramenant à $n$ équations scalaires indépendantes $z_i' = \lambda_i z_i$.
\end{itemize}

\subsection{Probabilités : Chaînes de Markov Finies}
\begin{itemize}
    \item La matrice de transition $M$ d'une chaîne de Markov est stochastique. Si elle est diagonalisable, l'étude de son état stationnaire (vecteur propre associé à la valeur propre 1) et de la convergence vers cet état est grandement simplifiée.
\end{itemize}

\section{Topologie et Aspects Numériques}
\subsection{Densité des Endomorphismes Diagonalisables}
\begin{itemize}
    \item \textbf{Théorème :} Sur $\mathbb{C}$, l'ensemble des endomorphismes diagonalisables est \textbf{dense} dans $\mathcal{L}(E)$. (On peut perturber légèrement un endomorphisme trigonalisable pour rendre ses valeurs propres distinctes).
    \item \textbf{Théorème :} Sur $\mathbb{R}$, c'est \textbf{faux}. L'ensemble des endomorphismes $\mathbb{R}$-diagonalisables n'est pas dense. (Une rotation ne peut être approchée par des endomorphismes diagonalisables).
\end{itemize}
\subsection{Stabilité Numérique}
\begin{itemize}
    \item Le problème de la diagonalisation est numériquement instable. Une petite perturbation d'une matrice peut changer radicalement ses vecteurs propres (même si les valeurs propres sont peu affectées).
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème Spectral (Couteau-suisse) :} Un des développements les plus rentables. Casable aussi dans les leçons 158, 160, 170.
        \item \textbf{Résolution d'un système différentiel $Y'=AY$ et étude du portrait de phase :} Un grand classique qui connecte algèbre et analyse.
        \item \textbf{Densité des matrices diagonalisables sur $\mathbb{C}$ :} Un résultat topologique élégant.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre diagonalisable et trigonalisable.}
        \item \textbf{Oublier le critère par le polynôme minimal,} qui est le plus puissant et le plus élégant.
        \item \textbf{Mal maîtriser le Théorème Spectral,} qui est un attendu majeur de l'agrégation.
        \item \textbf{Penser que la diagonalisabilité est une propriété ouverte} (elle ne l'est pas).
    \end{itemize}
\end{erreurs}
\chapter{Leçon 156 : Exponentielle de matrices. Applications.}

\begin{philosophie}
    L'exponentielle de matrices est le pont le plus direct et le plus puissant entre l'analyse et l'algèbre linéaire. Elle généralise la fonction la plus importante de l'analyse ($x \mapsto e^x$) au monde non-commutatif des matrices. Cette leçon doit mettre en évidence cette double nature : d'abord, définir proprement cet objet analytique, puis montrer comment les outils de l'algèbre (la réduction) permettent de le calculer, et enfin, déployer sa puissance dans son application reine, la résolution des systèmes différentiels linéaires.
\end{philosophie}

\section{Définition et Propriétés Analytiques}
\subsection{Définition par Série Entière}
\begin{itemize}
    \item \textbf{Norme d'opérateur :} Sur l'espace $\mathcal{M}_n(K)$ ($K=\mathbb{R}$ ou $\mathbb{C}$), qui est un espace de Banach, on définit la norme d'opérateur subordonnée.
    \item \textbf{Définition :} Pour $A \in \mathcal{M}_n(K)$, $e^A = \exp(A) = \sum_{k=0}^\infty \frac{A^k}{k!}$.
    \item \textbf{Théorème :} La série converge normalement, donc absolument, pour toute matrice $A$. L'application $\exp: \mathcal{M}_n(K) \to \mathcal{M}_n(K)$ est de classe $\mathcal{C}^\infty$.
\end{itemize}
\subsection{Propriétés Fondamentales}
\begin{itemize}
    \item \textbf{Le problème de la non-commutativité :} En général, $e^{A+B} \neq e^A e^B$.
    \item \textbf{Théorème :} Si $A$ et $B$ commutent ($AB=BA$), alors $e^{A+B} = e^A e^B$.
    \item \textbf{Conséquences :} $(e^A)^{-1} = e^{-A}$, donc $e^A \in GL_n(K)$. L'application $\exp$ est à valeurs dans le groupe linéaire.
    \item \textbf{Lien déterminant-trace :} $\det(e^A) = e^{\mathrm{Tr}(A)}$.
\end{itemize}

\section{Méthodes de Calcul Explicite}
\subsection{Calcul par Réduction}
\begin{itemize}
    \item \textbf{Cas Diagonalisable :} Si $A=PDP^{-1}$, le calcul est simple : $e^A = P e^D P^{-1}$, où $e^D$ est la matrice diagonale des exponentielles des valeurs propres.
    \item \textbf{Cas Trigonalisable / Nilpotent :} Si $A$ est nilpotente, la somme est finie. Si $A$ est triangulaire, $e^A$ l'est aussi et ses coefficients diagonaux sont les exponentielles de ceux de $A$.
    \item \textbf{Méthode Générale : Décomposition de Dunford.} Soit $A=D+N$ la décomposition de Dunford. Comme $D,N$ commutent, $e^A = e^D e^N$. On sait calculer les deux termes. C'est la méthode la plus puissante et la plus générale.
\end{itemize}
\subsection{Calcul par les Polynômes}
\begin{itemize}
    \item \textbf{Principe :} Il existe un polynôme $P \in K[X]$ tel que $e^A=P(A)$.
    \item \textbf{Méthode :} On peut trouver ce polynôme en utilisant le polynôme minimal de $A$.
\end{itemize}

\section{Applications}
\subsection{Application Reine : Systèmes Différentiels Linéaires}
\begin{itemize}
    \item \textbf{Théorème :} La solution unique du problème de Cauchy $Y' = AY$ avec $Y(0)=Y_0$ est donnée par $Y(t) = e^{tA}Y_0$.
    \item \textbf{Exemple :} Résolution explicite d'un système $2 \times 2$ non diagonalisable.
\end{itemize}
\subsection{Application à la Théorie des Groupes de Lie}
\begin{itemize}
    \item \textbf{Définition :} Un groupe de Lie est un groupe qui est aussi une variété différentielle.
    \item \textbf{Algèbre de Lie :} L'espace tangent à l'identité d'un groupe de Lie matriciel $G$ est son algèbre de Lie $\mathfrak{g}$. C'est un espace vectoriel stable par crochet de Lie $[A,B]=AB-BA$.
    \item \textbf{Application Exponentielle :} L'application $\exp: \mathfrak{g} \to G$ est le pont entre l'algèbre (linéaire) et le groupe (courbe). Elle est un difféomorphisme local au voisinage de 0.
    \item \textbf{Exemples :}
        \begin{itemize}
            \item L'algèbre de Lie de $GL_n(\mathbb{R})$ est $\mathcal{M}_n(\mathbb{R})$.
            \item L'algèbre de Lie de $SL_n(\mathbb{R})$ est l'espace des matrices de trace nulle.
            \item L'algèbre de Lie du groupe orthogonal $O(n)$ est l'espace des matrices antisymétriques.
        \end{itemize}
\end{itemize}
\subsection{Surjectivité de l'Exponentielle}
\begin{itemize}
    \item \textbf{Sur $\mathbb{C}$ :} L'application $\exp: \mathcal{M}_n(\mathbb{C}) \to GL_n(\mathbb{C})$ est surjective.
    \item \textbf{Sur $\mathbb{R}$ :} C'est faux. Par exemple, une matrice de $GL_n(\mathbb{R})$ avec un déterminant négatif n'est pas dans l'image.
    \item \textbf{Théorème :} $\exp: S_n(\mathbb{R}) \to S_n^{++}(\mathbb{R})$ (matrices symétriques $\to$ matrices symétriques définies positives) est un homéomorphisme.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Calcul de l'exponentielle d'une matrice par Dunford et résolution d'un système (Couteau-suisse) :} Un bijou qui connecte la réduction et les EDO. Très rentable.
        \item \textbf{Surjectivité de l'exponentielle de $\mathcal{M}_n(\mathbb{C})$ dans $GL_n(\mathbb{C})$ :} Un développement de haut niveau qui utilise la réduction de Jordan.
        \item \textbf{Le groupe orthogonal est engendré par l'exponentielle des matrices antisymétriques (Connexité de $SO(n)$) :} Un très beau développement mêlant algèbre, analyse et topologie.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Croire que $e^{A+B}=e^A e^B$ en général :} C'est l'erreur la plus classique. Il faut insister sur la condition de commutation.
        \item \textbf{Ne pas savoir calculer l'exponentielle d'une matrice non-diagonalisable :} La décomposition de Dunford est l'outil à maîtriser.
        \item \textbf{Oublier le lien avec les EDO :} C'est l'application principale qui motive toute la théorie.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 157 : Endomorphismes trigonalisables. Endomorphismes nilpotents.}

\begin{philosophie}
    Cette leçon traite du cas général de la réduction sur un corps algébriquement clos. La trigonalisation est le meilleur résultat que l'on puisse espérer en général. L'objectif est de montrer que "l'obstruction" à la diagonalisabilité est capturée par la partie nilpotente de l'endomorphisme. La leçon doit donc articuler finement le lien entre trigonalisation et nilpotence, culminant avec les décompositions de Dunford et Jordan qui révèlent l'anatomie complète d'un endomorphisme.
\end{philosophie}

\section{Trigonalisation : une Forme Normale Générale}

\subsection{Caractérisation de la Trigonalisabilité}
\begin{itemize}
    \item \textbf{Définition :} Un endomorphisme $u$ est trigonalisable s'il existe une base où sa matrice est triangulaire supérieure.
    \item \textbf{Théorème (Critère par les polynômes) :} Un endomorphisme est trigonalisable si et seulement si son polynôme caractéristique est scindé sur le corps de base.
    \item \textbf{Corollaire :} Sur un corps algébriquement clos (comme $\mathbb{C}$), tout endomorphisme est trigonalisable.
\end{itemize}

\subsection{Lien avec les Sous-Espaces Stables}
\begin{itemize}
    \item \textbf{Théorème :} $u$ est trigonalisable si et seulement s'il existe un drapeau complet de sous-espaces stables.
\end{itemize}

\section{Endomorphismes Nilpotents : l'Anatomie de la Non-Diagonalisabilité}

\subsection{Propriétés des Endomorphismes Nilpotents}
\begin{itemize}
    \item \textbf{Définition :} $u$ est nilpotent s'il existe $k$ tel que $u^k=0$. L'indice de nilpotence est le plus petit tel $k$.
    \item \textbf{Caractérisation :} Un endomorphisme est nilpotent si et seulement si son polynôme caractéristique est $X^n$. (Ou si son polynôme minimal est $X^k$). Sa seule valeur propre est 0.
\end{itemize}

\subsection{Réduction des Endomorphismes Nilpotents}
\begin{itemize}
    \item \textbf{Théorème :} Soit $u$ un endomorphisme nilpotent. Alors il existe une base de Jordan pour $u$, dans laquelle la matrice de $u$ est diagonale par blocs, où chaque bloc est un bloc de Jordan pour la valeur propre 0.
    \item \textbf{Invariants de Similitude :} Le nombre et la taille des blocs de Jordan sont entièrement déterminés par la suite des dimensions des noyaux itérés $\dim(\ker(u^k))$. Ils forment une partition de l'entier $n$.
\end{itemize}

\section{Décompositions Fines et Applications}

\subsection{Décomposition de Dunford (Diagonalisable + Nilpotent)}
\begin{itemize}
    \item \textbf{Théorème :} Si $u$ est trigonalisable, il existe un unique couple $(d,n)$ tel que $u=d+n$, avec $d$ diagonalisable, $n$ nilpotent, et $dn=nd$. De plus, $d$ et $n$ sont des polynômes en $u$.
\end{itemize}
\begin{remark}
    C'est la décomposition conceptuelle la plus importante. Elle sépare le comportement "simple" (diagonalisable) du comportement "pathologique" (nilpotent).
\end{remark}

\subsection{Décomposition de Jordan (Forme Normale)}
\begin{itemize}
    \item \textbf{Théorème :} Tout endomorphisme trigonalisable est "jordanisable". Sa matrice dans une base de Jordan est une somme de blocs de Jordan.
    \item \textbf{Lien avec Dunford :} La décomposition de Jordan est une représentation matricielle de la décomposition de Dunford. La partie diagonale de la matrice de Jordan représente $d$, la partie extra-diagonale représente $n$.
\end{itemize}

\subsection{Applications}
\begin{itemize}
    \item \textbf{Calcul de l'exponentielle de matrice :} La décomposition de Dunford est l'outil le plus efficace. $e^A = e^{D+N} = e^D e^N$.
    \item \textbf{Étude des EDO linéaires :} Permet de comprendre le comportement des solutions même lorsque la matrice n'est pas diagonalisable.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Décomposition de Dunford (existence et unicité) :} Un développement de haut niveau, dont la preuve utilise le lemme des restes chinois.
        \item \textbf{Réduction de Jordan d'une matrice nilpotente :} Un développement qui montre une compréhension fine de la structure des endomorphismes nilpotents.
        \item \textbf{Co-trigonalisation d'une famille d'endomorphismes qui commutent :} Un résultat important qui se place bien dans cette leçon.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Penser que la trigonalisation est un résultat trivial :} Il faut savoir prouver qu'un endomorphisme sur $\mathbb{C}$ admet une droite stable.
        \item \textbf{Ne pas faire le lien entre nilpotence et Jordan :} La forme de Jordan est l'expression de la structure nilpotente sur les sous-espaces caractéristiques.
        \item \textbf{Confondre les décompositions de Dunford et Jordan :} L'une est une décomposition d'endomorphisme ($u=d+n$), l'autre est une forme matricielle.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 158 : Matrices symétriques réelles. Matrices hermitiennes.}

\begin{philosophie}
    Cette leçon est au cœur de la géométrie euclidienne et hermitienne. Le résultat central et absolu est le \textbf{Théorème Spectral}, qui est l'un des plus beaux de l'algèbre linéaire. Il affirme que ces matrices sont toujours diagonalisables dans une base orthonormée. Le plan doit tourner autour de ce théorème, en explorant ses différentes facettes (algébrique, géométrique, analytique), ses preuves, et ses applications profondes dans de nombreux domaines.
\end{philosophie}

\section{Le Théorème Spectral : le Résultat Central}

\subsection{Cadre Euclidien : Matrices Symétriques Réelles}
\begin{itemize}
    \item \textbf{Définition :} Une matrice $A \in \mathcal{M}_n(\mathbb{R})$ est symétrique si $A^T=A$. Elle représente un endomorphisme auto-adjoint dans une base orthonormée.
    \item \textbf{Théorème Spectral (Version matricielle) :} Pour toute matrice symétrique réelle $A$, il existe une matrice orthogonale $P \in O(n)$ et une matrice diagonale $D \in \mathcal{M}_n(\mathbb{R})$ telles que $A = PDP^T = PDP^{-1}$.
    \item \textbf{Théorème Spectral (Version endomorphisme) :} Un endomorphisme d'un espace euclidien est auto-adjoint si et seulement s'il est diagonalisable dans une base orthonormée.
\end{itemize}

\subsection{Cadre Hermitien : Matrices Hermitiennes}
\begin{itemize}
    \item \textbf{Définition :} Une matrice $A \in \mathcal{M}_n(\mathbb{C})$ est hermitienne si $A^*=A$ (où $A^*=\bar{A}^T$). Elle représente un endomorphisme auto-adjoint d'un espace hermitien.
    \item \textbf{Théorème Spectral (Version hermitienne) :} Pour toute matrice hermitienne $A$, il existe une matrice unitaire $U \in U(n)$ et une matrice diagonale réelle $D$ telles que $A = UDU^* = UDU^{-1}$. Les valeurs propres d'une matrice hermitienne sont réelles.
\end{itemize}
\begin{remark}
    La théorie est parfaitement parallèle dans les deux cadres. La transposée est remplacée par l'adjointe, orthogonale par unitaire.
\end{remark}

\section{Caractérisations et Propriétés}

\subsection{Caractérisation Variationnelle des Valeurs Propres}
\begin{itemize}
    \item \textbf{Quotient de Rayleigh :} Pour une matrice symétrique $A$, $R_A(x) = \frac{\langle Ax, x \rangle}{\|x\|^2}$.
    \item \textbf{Théorème Min-Max de Courant-Fischer :} Les valeurs propres d'une matrice symétrique peuvent être caractérisées comme des extrema du quotient de Rayleigh sur des sous-espaces bien choisis. Par exemple, la plus grande valeur propre est $\lambda_{\max} = \max_{x \neq 0} R_A(x)$.
\end{itemize}

\subsection{Positivité et Décomposition}
\begin{itemize}
    \item \textbf{Définition (Matrice Positive/Définie Positive) :} Une matrice symétrique $A$ est positive (resp. définie positive) si la forme quadratique associée est positive (resp. définie positive), i.e. $\langle Ax,x \rangle \ge 0$ (resp. $>0$) pour $x \neq 0$.
    \item \textbf{Caractérisation Spectrale :} Une matrice symétrique est positive (resp. définie positive) si et seulement si ses valeurs propres sont positives (resp. strictement positives).
    \item \textbf{Théorème (Racine Carrée) :} Toute matrice symétrique positive admet une unique racine carrée symétrique positive.
    \item \textbf{Décomposition de Cholesky :} Toute matrice symétrique définie positive s'écrit de manière unique $A=B B^T$ où $B$ est triangulaire inférieure à coefficients diagonaux stricts positifs.
\end{itemize}

\section{Applications}
\subsection{Géométrie : Réduction des Formes Quadratiques}
\begin{itemize}
    \item La recherche des axes principaux d'une conique ou d'une quadrique revient à diagonaliser la matrice symétrique de sa partie quadratique dans une base orthonormée.
\end{itemize}
\subsection{Analyse : Étude des Extrema Locaux}
\begin{itemize}
    \item La nature d'un point critique d'une fonction $\mathcal{C}^2$ est déterminée par la signature de sa matrice Hessienne, qui est symétrique. Le théorème spectral garantit que la Hessienne est diagonalisable, et le signe de ses valeurs propres détermine si le point est un minimum, un maximum ou un point selle.
\end{itemize}
\subsection{Analyse Numérique : Décomposition en Valeurs Singulières (SVD)}
\begin{itemize}
    \item \textbf{Théorème :} Pour toute matrice $A \in \mathcal{M}_{n,p}(\mathbb{R})$, il existe $U \in O(n), V \in O(p)$ et $\Sigma$ "diagonale" positive telles que $A=U\Sigma V^T$.
    \item \textbf{Lien avec le Théorème Spectral :} Les valeurs singulières (les diagonaux de $\Sigma$) sont les racines carrées des valeurs propres de la matrice symétrique positive $A^TA$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème Spectral (Couteau-suisse) :} Un développement absolument central. La preuve analytique par la compacité de la sphère est très élégante.
        \item \textbf{Décomposition de Cholesky :} Un développement constructif et important en analyse numérique.
        \item \textbf{Décomposition en valeurs singulières (SVD) et applications :} Un développement de très haut niveau qui montre une grande maturité et des liens avec les applications modernes.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre symétrique et hermitien,} ou orthogonal et unitaire.
        \item \textbf{Oublier "orthonormée" dans l'énoncé du Théorème Spectral.} C'est le cœur du résultat.
        \item \textbf{Ne pas savoir esquisser au moins une preuve du Théorème Spectral.}
        \item \textbf{Sous-estimer les applications :} Le lien avec les formes quadratiques et les extrema est fondamental.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 159 : Formes linéaires et dualité en dimension finie. Exemples et applications.}

\begin{philosophie}
    La dualité est un changement de point de vue : au lieu d'étudier un espace par ses "points" (les vecteurs), on l'étudie par les "fonctions" qui agissent dessus (les formes linéaires). Cet espace de fonctions, le dual, se révèle être un "miroir" parfait de l'espace original, encodant toute sa structure. La leçon doit construire ce dictionnaire et montrer comment la traduction entre les deux mondes (espace et dual) résout des problèmes.
\end{philosophie}

\section{Le Dual : Un Miroir de l'Espace}
\subsection{Définitions Fondamentales}
\begin{itemize}
    \item \textbf{Espace Dual $E^*$ et Base Duale :} Définition et existence/unicité de la base duale. Conséquence : $\dim E = \dim E^*$.
    \item \textbf{Isomorphisme $E \cong E^*$ :} Il dépend du choix d'une base, il est donc \textbf{non-canonique}.
\end{itemize}
\subsection{Le Bidual et la Réflexivité}
\begin{itemize}
    \item \textbf{Injection Canonique $J: E \to E^{**}$ :} Définition $J(x)(\varphi) = \varphi(x)$.
    \item \textbf{Théorème :} En dimension finie, $J$ est un isomorphisme \textbf{canonique}. On identifie $E$ et $E^{**}$.
\end{itemize}

\section{Orthogonalité : La Géométrie de la Dualité}
\subsection{Définition et Propriétés}
\begin{itemize}
    \item \textbf{Orthogonal (Annulateur) $F^\circ$ d'un s.e.v. $F \subset E$ :} $F^\circ$ est un s.e.v. de $E^*$.
    \item \textbf{Théorèmes Fondamentaux en Dimension Finie :}
        \begin{itemize}
            \item $\dim F + \dim F^\circ = \dim E$.
            \item $(F^\circ)^\perp = F$.
            \item $(F+G)^\circ = F^\circ \cap G^\circ$ et $(F \cap G)^\circ = F^\circ + G^\circ$.
        \end{itemize}
\end{itemize}
\subsection{Application : de la Représentation Paramétrique à la Représentation Cartésienne}
\begin{itemize}
    \item Un sous-espace $F$ donné par une base $(f_i)$ (paramétrique) est aussi l'intersection des noyaux des formes linéaires d'une base de $F^\circ$ (cartésienne). La dualité permet de passer de l'une à l'autre.
\end{itemize}

\section{Application Transposée : Le Miroir des Endomorphismes}
\subsection{Définition et Propriétés}
\begin{itemize}
    \item \textbf{Définition :} $u^t \in \mathcal{L}(F^*, E^*)$ est définie par $u^t(\psi) = \psi \circ u$.
    \item \textbf{Propriétés :} La matrice de $u^t$ dans les bases duales est la transposée. $\mathrm{rg}(u^t) = \mathrm{rg}(u)$.
\end{itemize}
\subsection{Relations d'Orthogonalité}
\begin{itemize}
    \item \textbf{Théorèmes :}
        \begin{itemize}
            \item $\ker(u^t) = (\mathrm{Im}(u))^\circ$
            \item $\mathrm{Im}(u^t) = (\ker(u))^\circ$
        \end{itemize}
\end{itemize}
\begin{remark}
    Ces relations sont le point culminant de la théorie, une symétrie parfaite : le noyau de la transposée est l'espace des équations de l'image.
\end{remark}

\section{Applications}
\subsection{Interpolation de Lagrange}
\begin{itemize}
    \item Les polynômes de base de Lagrange peuvent être vus comme la base duale de la base des évaluations $(x \mapsto \delta_{x_i})$.
\end{itemize}
\subsection{Formes Bilinéaires}
\begin{itemize}
    \item Une forme bilinéaire $\phi$ sur $E$ induit un morphisme $\Phi: E \to E^*$. $\phi$ est non-dégénérée si $\Phi$ est un isomorphisme.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration des relations $\ker(u^t) = (\mathrm{Im}(u))^\circ$ et $\mathrm{Im}(u^t) = (\ker(u))^\circ$ :} Un développement très formateur qui montre la maîtrise de la dualité.
        \item \textbf{Existence et unicité des polynômes de Lagrange :} Preuve via un argument de dualité.
        \item \textbf{L'isomorphisme canonique $E \cong E^{**}$ en dimension finie :} Un développement théorique fondamental.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre l'isomorphisme non-canonique $E \cong E^*$ et l'isomorphisme canonique $E \cong E^{**}$.} C'est l'erreur la plus grave.
        \item \textbf{Confondre l'orthogonalité duale et l'orthogonalité euclidienne.} La première existe sur tout espace, la seconde nécessite un produit scalaire.
        \item \textbf{Ne pas savoir interpréter $F^\circ$ comme un système d'équations.} C'est l'application la plus concrète de la théorie.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 160 : Endomorphismes remarquables d'un espace vectoriel euclidien.}

\begin{philosophie}
    Dans un espace euclidien, la présence d'un produit scalaire enrichit considérablement la structure. On peut classer les endomorphismes en fonction de leur interaction avec cette structure métrique. Cette leçon est une "galerie de portraits" des transformations les plus importantes de la géométrie euclidienne : celles qui conservent les longueurs (isométries), celles qui se comportent bien par rapport au produit scalaire (auto-adjoints), et leurs combinaisons.
\end{philosophie}

\section{L'Adjoint : un Outil de Classification Fondamental}
\subsection{Définition et Propriétés}
\begin{itemize}
    \item \textbf{Théorème (Existence et Unicité de l'Adjoint) :} Pour tout $u \in \mathcal{L}(E)$, il existe un unique $u^* \in \mathcal{L}(E)$ tel que $\forall x,y, \langle u(x), y \rangle = \langle x, u^*(y) \rangle$.
    \item \textbf{Propriétés :} Dans une base orthonormée, $\mathrm{Mat}(u^*) = (\mathrm{Mat}(u))^T$. L'application $u \mapsto u^*$ est un anti-automorphisme involutif de l'algèbre $\mathcal{L}(E)$.
\end{itemize}

\section{Les Isométries (Endomorphismes Orthogonaux) : les Transformations Rigides}
\subsection{Caractérisations}
\begin{itemize}
    \item \textbf{Définition :} $u$ est une isométrie si $u^*u = \mathrm{Id}$.
    \item \textbf{Équivalences :} Préserver le produit scalaire $\iff$ préserver la norme $\iff$ transformer une base orthonormée en une base orthonormée.
    \item \textbf{Le Groupe Orthogonal $O(E)$ :} C'est un sous-groupe compact de $GL(E)$. Son déterminant est $\pm 1$. Le sous-groupe des isométries directes (déterminant 1) est $SO(E)$.
\end{itemize}
\subsection{Classification en Petite Dimension}
\begin{itemize}
    \item \textbf{Dimension 2 :} Les isométries sont les rotations et les réflexions.
    \item \textbf{Dimension 3 :} Les isométries directes sont les rotations (autour d'un axe). Les isométries indirectes sont les rotations-réflexions.
\end{itemize}
\subsection{Réduction des Isométries}
\begin{itemize}
    \item \textbf{Théorème de Réduction :} Tout endomorphisme orthogonal se décompose en une somme directe orthogonale de sous-espaces stables de dimension 1 ou 2. Sur ces sous-espaces, l'isométrie agit comme $\pm \mathrm{Id}$ ou comme une rotation.
\end{itemize}

\section{Les Endomorphismes Auto-Adjoints (Symétriques) : Le Cas Diagonalisable Idéal}
\subsection{Définition et Propriétés}
\begin{itemize}
    \item \textbf{Définition :} $u$ est auto-adjoint (ou symétrique) si $u=u^*$.
    \item \textbf{Propriétés :} Les sous-espaces propres d'un endomorphisme auto-adjoint sont deux à deux orthogonaux.
\end{itemize}
\subsection{Le Théorème Spectral}
\begin{itemize}
    \item \textbf{Théorème :} Un endomorphisme est auto-adjoint si et seulement s'il est diagonalisable dans une base orthonormée.
\end{itemize}
\begin{remark}
    C'est le résultat central de la leçon, un pont magnifique entre algèbre (diagonalisation) et géométrie (orthogonalité).
\end{remark}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Réduction des formes quadratiques :} La recherche d'une base orthogonale pour une forme quadratique revient à diagonaliser un endomorphisme symétrique.
    \item \textbf{Mécanique :} Le tenseur d'inertie d'un solide est un endomorphisme symétrique. Le théorème spectral garantit l'existence des axes principaux d'inertie.
\end{itemize}

\section{Décomposition Polaire : Une Vision Unifiée}
\subsection{Théorème de Décomposition}
\begin{itemize}
    \item \textbf{Théorème :} Tout automorphisme $u \in GL(E)$ se décompose de manière unique en $u=os$ (ou $u=s'o$), où $o$ est une isométrie et $s, s'$ sont des endomorphismes auto-adjoints définis positifs.
\end{itemize}
\begin{remark}
    C'est l'analogue de la décomposition polaire $z = e^{i\theta} \rho$ des nombres complexes. L'isométrie joue le rôle de la rotation, l'endomorphisme symétrique défini positif celui de la dilatation.
\end{remark}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème Spectral :} C'est le développement le plus rentable de l'algèbre.
        \item \textbf{Classification des isométries en dimension 3 :} Un développement très géométrique, qui montre une bonne intuition.
        \item \textbf{Décomposition polaire :} Un développement de haut niveau qui fait une belle synthèse de la leçon.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre endomorphisme orthogonal et matrice orthogonale :} L'un est intrinsèque, l'autre dépend du choix d'une base (qui doit être orthonormée).
        \item \textbf{Confondre endomorphisme symétrique et matrice symétrique.}
        \item \textbf{Penser que le Théorème spectral s'applique à tout endomorphisme diagonalisable.} Il faut la symétrie pour garantir l'orthogonalité de la base.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 161 : Distances et isométries d'un espace affine euclidien.}

\begin{philosophie}
    C'est la leçon de géométrie euclidienne par excellence. L'objectif est de classifier complètement tous les "mouvements rigides" (les isométries) de l'espace qui nous entoure. La stratégie est purement algébrique : on montre que toute isométrie affine a une "partie linéaire" qui est une isométrie vectorielle (un endomorphisme orthogonal), on classifie ces dernières grâce à l'algèbre linéaire, puis on en déduit la classification complète des isométries affines.
\end{philosophie}

\section{Le Cadre : Espace Affine Euclidien et Distance}

\subsection{Définitions}
\begin{itemize}
    \item \textbf{Espace Affine Euclidien :} Un espace affine $\mathcal{E}$ dirigé par un espace vectoriel euclidien $(E, \langle \cdot, \cdot \rangle)$.
    \item \textbf{Distance Euclidienne :} Définie par $d(A,B) = \|\vec{AB}\|$. Elle vérifie les axiomes d'une distance et l'identité de Pythagore.
\end{itemize}

\subsection{Isométries Affines}
\begin{itemize}
    \item \textbf{Définition :} Une application $f: \mathcal{E} \to \mathcal{E}$ est une isométrie si elle conserve la distance : $d(f(A),f(B))=d(A,B)$.
    \item \textbf{Le Groupe des Isométries $\mathrm{Isom}(\mathcal{E})$ :} Les isométries forment un groupe pour la composition.
    \item \textbf{Théorème Fondamental (Caractérisation) :} Une application $f: \mathcal{E} \to \mathcal{E}$ est une isométrie si et seulement si elle est affine et sa partie linéaire $\vec{f}$ est un endomorphisme orthogonal ($\vec{f} \in O(E)$).
\end{itemize}
\begin{remark}
    Ce théorème est la clé de voûte de la leçon. Il ramène l'étude géométrique des isométries affines à l'étude algébrique du groupe orthogonal $O(E)$.
\end{remark}

\section{Classification des Isométries en Dimension 2}

\subsection{Rappel sur le Groupe Orthogonal $O(2)$}
\begin{itemize}
    \item Les éléments de $O(2)$ sont les rotations ($SO(2)$, $\det=1$) et les réflexions ($\det=-1$).
\end{itemize}
\subsection{Classification des Isométries Planes}
\begin{itemize}
    \item \textbf{Cas $\det(\vec{f})=1$ (Déplacements) :}
        \begin{itemize}
            \item Si $\vec{f} = \mathrm{Id}$, $f$ est une \textbf{translation}.
            \item Si $\vec{f}$ est une rotation d'angle $\theta \neq 0$, $f$ est une \textbf{rotation} affine de même angle, d'unique centre $\Omega$.
        \end{itemize}
    \item \textbf{Cas $\det(\vec{f})=-1$ (Antidéplacements) :}
        \begin{itemize}
            \item Si $\vec{f}$ a une droite de points fixes, $f$ est une \textbf{réflexion} orthogonale par rapport à une droite affine.
            \item Sinon, $f$ est une \textbf{réflexion glissée} (composition d'une réflexion et d'une translation de vecteur parallèle à l'axe de la réflexion).
        \end{itemize}
\end{itemize}

\section{Classification des Isométries en Dimension 3}

\subsection{Rappel sur le Groupe Orthogonal $O(3)$}
\begin{itemize}
    \item Les éléments de $SO(3)$ sont les rotations (autour d'un axe). Les éléments de $O(3) \setminus SO(3)$ sont les rotations-réflexions.
\end{itemize}
\subsection{Classification des Isométries de l'Espace}
\begin{itemize}
    \item \textbf{Décomposition de Chasles :} Toute isométrie affine se décompose de manière unique.
    \item \textbf{Cas $\det(\vec{f})=1$ (Déplacements) :}
        \begin{itemize}
            \item \textbf{Translation} (si $\vec{f} = \mathrm{Id}$).
            \item \textbf{Rotation} (si l'ensemble des points fixes est une droite).
            \item \textbf{Vissage} (composition d'une rotation et d'une translation de vecteur directeur l'axe de la rotation).
        \end{itemize}
    \item \textbf{Cas $\det(\vec{f})=-1$ (Antidéplacements) :}
        \begin{itemize}
            \item \textbf{Réflexion} (si l'ensemble des points fixes est un plan).
            \item \textbf{Antirotation} (composition d'une rotation et d'une réflexion par rapport à un plan orthogonal à l'axe).
            \item \textbf{Réflexion glissée}.
        \end{itemize}
\end{itemize}
\begin{theorem}
    Tout déplacement de l'espace est un vissage (les translations et rotations étant des cas particuliers).
\end{theorem}

\section{Applications}
\subsection{Sous-Groupes Finis d'Isométries}
\begin{itemize}
    \item \textbf{Théorème :} Tout sous-groupe fini d'isométries de l'espace a un point fixe commun.
    \item \textbf{Classification :} Les sous-groupes finis de $SO(3)$ sont les groupes cycliques, diédraux, et les groupes d'isométries des solides de Platon.
\end{itemize}
\subsection{Génération du Groupe des Isométries}
\begin{itemize}
    \item \textbf{Théorème de Cartan-Dieudonné (version affine) :} Toute isométrie de $\mathcal{E}_n$ est la composée d'au plus $n+1$ réflexions.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Classification des isométries planes :} Un développement très complet et géométrique, où il faut bien discuter de l'ensemble des points fixes.
        \item \textbf{Tout déplacement de l'espace est un vissage :} Un beau développement qui montre une compréhension fine de la structure des isométries en dimension 3.
        \item \textbf{Le groupe des isométries du tétraèdre régulier est isomorphe à $\mathfrak{S}_4$ :} Un développement qui connecte cette leçon à la théorie des groupes finis.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre l'isométrie affine $f$ et sa partie linéaire $\vec{f}$.} C'est l'erreur la plus grave.
        \item \textbf{Oublier des cas dans la classification :} Les vissages et les réflexions glissées sont souvent les grands oubliés.
        \item \textbf{Ne pas savoir prouver que la partie linéaire d'une isométrie est un endomorphisme orthogonal.} C'est le point de départ de toute la classification.
        \item \textbf{Raisonner uniquement avec des matrices :} La leçon porte sur la géométrie. Il faut parler de points, de droites, de plans, de points fixes.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 162 : Systèmes d'équations linéaires ; aspects théoriques et numériques.}

\begin{philosophie}
    Cette leçon, d'apparence élémentaire, est en réalité un carrefour de l'algèbre. Elle teste la capacité à naviguer entre trois points de vue : algébrique (structure de l'ensemble des solutions), géométrique (intersection d'hyperplans) et numérique (méthodes de résolution et stabilité). Un plan de haut niveau doit articuler ces trois aspects. Il ne s'agit pas de faire un cours sur le pivot de Gauss, mais de montrer que l'on a compris la théorie sous-jacente (théorème du rang), les outils de factorisation matricielle, et les défis concrets de la résolution numérique sur ordinateur.
\end{philosophie}

\section{Structure Théorique de l'Ensemble des Solutions}

\subsection{Le Point de Vue de l'Algèbre Linéaire}
\begin{itemize}
    \item \textbf{Formalisation :} Un système linéaire de $n$ équations à $p$ inconnues s'écrit $Ax=b$, où $A \in \mathcal{M}_{n,p}(K)$. C'est la recherche des antécédents de $b$ par l'application linéaire $u: K^p \to K^n$ de matrice $A$.
    \item \textbf{Théorème de Structure (Rouché-Fontené) :}
        \begin{itemize}
            \item Le système admet des solutions si et seulement si $b \in \mathrm{Im}(u)$, ce qui équivaut à $\mathrm{rg}(A) = \mathrm{rg}(A|b)$.
            \item Si une solution $x_0$ existe, l'ensemble des solutions est l'espace affine $x_0 + \ker(u)$.
        \end{itemize}
    \item \textbf{Dimension de l'espace des solutions :} Par le théorème du rang, la dimension de l'espace affine des solutions est $p - \mathrm{rg}(A)$.
\end{itemize}

\subsection{Interprétation Géométrique}
\begin{itemize}
    \item Chaque équation du système définit un hyperplan affine. Résoudre le système revient à chercher l'intersection de ces $n$ hyperplans.
    \item Le rang de la matrice $A$ correspond au nombre d'"équations indépendantes". La structure de l'intersection (vide, un point, une droite, un plan...) est directement liée au rang.
\end{itemize}

\section{Méthodes de Résolution Directes}

\subsection{Le Pivot de Gauss : un Algorithme Universel}
\begin{itemize}
    \item \textbf{L'algorithme :} Transformer le système en un système équivalent de forme échelonnée (triangulaire supérieure) par des opérations élémentaires sur les lignes.
    \item \textbf{Interprétation matricielle : la Décomposition LU.} L'algorithme de Gauss, s'il se déroule sans permutation de lignes, revient à factoriser la matrice $A$ sous la forme $A=LU$, où $L$ est triangulaire inférieure (avec des 1 sur la diagonale) et $U$ est triangulaire supérieure.
    \item \textbf{Résolution :} Résoudre $Ax=b$ revient alors à résoudre deux systèmes triangulaires (donc faciles) : $Ly=b$ (descente), puis $Ux=y$ (remontée).
\end{itemize}

\subsection{Stabilité Numérique et Stratégie du Pivot}
\begin{itemize}
    \item \textbf{Le problème :} L'algorithme de Gauss est numériquement instable si on rencontre des pivots nuls ou très petits.
    \item \textbf{Stratégie du Pivot Partiel :} A chaque étape, on choisit comme pivot le plus grand élément (en valeur absolue) de la colonne, et on permute les lignes en conséquence.
    \item \textbf{Interprétation matricielle :} La stratégie du pivot revient à trouver une factorisation $PA=LU$, où $P$ est une matrice de permutation.
\end{itemize}

\subsection{Le Cas Particulier des Matrices Symétriques Définies Positives}
\begin{itemize}
    \item \textbf{Décomposition de Cholesky :} Si $A$ est symétrique définie positive, elle admet une unique factorisation $A=B B^T$, où $B$ est triangulaire inférieure à diagonale strictement positive. L'algorithme est deux fois plus rapide et numériquement très stable, sans besoin de pivot.
\end{itemize}

\section{Aspects Numériques et Méthodes Itératives}

\subsection{Conditionnement d'un Système Linéaire}
\begin{itemize}
    \item \textbf{Définition :} Le \textbf{conditionnement} d'une matrice inversible $A$ est $\mathrm{cond}(A) = \|A\| \|A^{-1}\|$. Il est toujours $\ge 1$.
    \item \textbf{Interprétation :} Le conditionnement mesure l'amplification maximale des erreurs relatives. Si le second membre $b$ est perturbé en $b+\delta b$, l'erreur relative sur la solution est majorée par $\frac{\|\delta x\|}{\|x\|} \le \mathrm{cond}(A) \frac{\|\delta b\|}{\|b\|}$.
    \item \textbf{Problème mal conditionné :} Si $\mathrm{cond}(A)$ est grand, le système est intrinsèquement difficile à résoudre numériquement, quelle que soit la méthode.
\end{itemize}

\subsection{Méthodes Itératives pour les Grands Systèmes Creux}
\begin{itemize}
    \item \textbf{Motivation :} Pour les systèmes de très grande taille issus de la discrétisation d'EDP, les méthodes directes sont trop coûteuses en temps et en mémoire. On préfère des méthodes itératives.
    \item \textbf{Principe :} On décompose $A=M-N$ avec $M$ "facile" à inverser, et on construit la suite $x_{k+1} = M^{-1}(Nx_k + b)$.
    \item \textbf{Méthodes Classiques :}
        \begin{itemize}
            \item \textbf{Jacobi :} $M$ est la diagonale de $A$.
            \item \textbf{Gauss-Seidel :} $M$ est la partie triangulaire inférieure de $A$.
        \end{itemize}
    \item \textbf{Convergence :} La suite converge si et seulement si le rayon spectral de la matrice d'itération $J=M^{-1}N$ est strictement inférieur à 1. Une condition suffisante pratique est que $A$ soit à diagonale strictement dominante.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Interprétation matricielle du pivot de Gauss (Décomposition LU) :} Un développement classique qui montre une bonne compréhension de l'algorithme.
        \item \textbf{Décomposition de Cholesky :} Preuve de l'existence et de l'unicité. Un développement très apprécié pour sa spécificité et son importance pratique.
        \item \textbf{Conditionnement : exemple des matrices de Hilbert :} Montrer comment le conditionnement explose et l'impact sur la solution numérique.
        \item \textbf{Convergence de la méthode de Jacobi pour une matrice à diagonale strictement dominante :} Un développement qui montre une maîtrise des aspects itératifs.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Se limiter au pivot de Gauss "artisanal" :} Le jury attend une vision matricielle (LU, Cholesky) et une discussion sur la stabilité.
        \item \textbf{Oublier le conditionnement :} C'est un concept central en analyse numérique, qui explique pourquoi certains problèmes sont "difficiles".
        \item \textbf{Ne pas mentionner les méthodes itératives :} C'est un pan entier de la résolution numérique, indispensable pour les grands systèmes.
        \item \textbf{Confondre les différentes normes et le conditionnement associé.}
    \end{itemize}
\end{erreurs}
\chapter{Leçon 170 : Formes quadratiques sur un espace vectoriel de dimension finie.}

\begin{philosophie}
    Cette leçon est la porte d'entrée de la géométrie en algèbre linéaire. Une forme quadratique est une "fonction longueur au carré" généralisée. L'objectif est de montrer que l'étude de ces objets se ramène à un problème de réduction (la "diagonalisation" de la forme) qui permet de les classifier complètement, notamment sur $\mathbb{R}$ via la loi d'inertie de Sylvester. C'est une leçon où l'interaction entre l'objet algébrique (la forme) et sa représentation matricielle doit être parfaitement maîtrisée.
\end{philosophie}

\section{Définitions et Réduction Algébrique}

\subsection{Formes Bilinéaires Symétriques et Formes Quadratiques}
\begin{itemize}
    \item \textbf{Définition (Forme quadratique) :} Une application $q: E \to K$ est une forme quadratique s'il existe une forme bilinéaire $\phi$ telle que $q(x)=\phi(x,x)$.
    \item \textbf{Forme Polaire :} Si car$(K) \neq 2$, il existe une unique forme bilinéaire symétrique $\phi$ associée à $q$, donnée par la formule de polarisation.
    \item \textbf{Matrice d'une forme quadratique :} Dans une base, $q(X) = X^T S X$ où $S$ est une matrice symétrique.
    \item \textbf{Changement de base :} La formule de changement de base est $S' = P^T S P$. On dit que les matrices $S$ et $S'$ sont \textbf{congruentes}.
\end{itemize}

\subsection{Orthogonalité et Réduction de Gauss}
\begin{itemize}
    \item \textbf{Orthogonalité :} Définition de l'orthogonalité pour $\phi$. Noyau de $q$. Forme non-dégénérée. Cône isotrope.
    \item \textbf{Base Orthogonale :} Une base dans laquelle la matrice de $q$ est diagonale.
    \item \textbf{Théorème (Réduction de Gauss) :} Toute forme quadratique admet une base orthogonale. Dans cette base, $q(x) = \sum_{i=1}^n \lambda_i x_i^2$.
    \item \textbf{Algorithme de Gauss :} Une procédure effective (basée sur la complétion du carré) pour trouver une telle base et la forme réduite.
\end{itemize}

\section{Classification des Formes Quadratiques sur $\mathbb{R}$ et $\mathbb{C}$}

\subsection{Classification sur $\mathbb{C}$}
\begin{itemize}
    \item \textbf{Théorème :} Deux formes quadratiques sur un $\mathbb{C}$-ev sont équivalentes (ont des matrices congruentes) si et seulement si elles ont le même \textbf{rang}.
    \item \textbf{Forme normale :} Toute forme quadratique de rang $r$ est équivalente à $\sum_{i=1}^r x_i^2$.
\end{itemize}

\subsection{Classification sur $\mathbb{R}$ : Loi d'Inertie de Sylvester}
\begin{itemize}
    \item \textbf{Théorème :} Toute forme quadratique réelle $q$ est équivalente à une unique forme du type $\sum_{i=1}^s x_i^2 - \sum_{j=s+1}^{s+t} x_j^2$.
    \item \textbf{Définition (Signature) :} Le couple $(s,t)$ est la \textbf{signature} de la forme. C'est un invariant complet (avec le rang $r=s+t$).
    \item \textbf{Formes définies :} Caractérisation des formes définies positives/négatives par leur signature ($(n,0)$ ou $(0,n)$).
\end{itemize}
\begin{remark}
    La signature est l'invariant fondamental qui classifie toutes les géométries "bilinéaires" sur un espace vectoriel réel : euclidienne (signature $(n,0)$), lorentzienne (signature $(n-1,1)$), etc.
\end{remark}

\section{Lien avec la Réduction des Endomorphismes Symétriques}

\subsection{Le Cadre Euclidien}
\begin{itemize}
    \item Soit $E$ un espace euclidien. À toute forme quadratique $q$, on peut associer un unique endomorphisme auto-adjoint $u$ tel que $q(x) = \langle u(x), x \rangle$.
\end{itemize}
\subsection{Double Réduction}
\begin{itemize}
    \item \textbf{Théorème (Réduction simultanée) :} Soit $q$ une forme quadratique et $\psi$ une forme quadratique définie positive sur $E$. Il existe une base de $E$ qui est orthogonale à la fois pour $q$ et pour $\psi$.
    \item \textbf{Théorème Spectral :} En particulier, si $E$ est euclidien, il existe une base \textbf{orthonormée} qui est orthogonale pour $q$. Dans cette base, la matrice de $q$ est diagonale. C'est la diagonalisation de l'endomorphisme symétrique associé.
\end{itemize}
\begin{remark}
    La réduction de Gauss trouve une base orthogonale quelconque. Le Théorème Spectral, plus puissant mais ne s'appliquant qu'en présence d'une structure euclidienne, trouve une base qui est de plus orthonormée.
\end{remark}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Loi d'inertie de Sylvester :} Un théorème fondamental dont la preuve (montrant l'invariance de la signature) est très instructive et élégante.
        \item \textbf{Décomposition de Gauss d'une forme quadratique :} Un développement algorithmique qui montre la maîtrise du concept. Il faut choisir un exemple non trivial.
        \item \textbf{Classification des coniques (partie quadratique) :} Un développement qui illustre la puissance géométrique de la classification par la signature.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre congruence ($P^T S P$) et similitude ($P^{-1}SP$) :} C'est l'erreur la plus grave, qui montre une confusion entre la réduction des formes quadratiques et celle des endomorphismes.
        \item \textbf{Penser que la base orthogonale de Gauss est orthonormée :} Elle ne l'est pas en général. L'orthonormalité requiert le cadre euclidien et le Théorème spectral.
        \item \textbf{Mal calculer la signature :} Il faut bien maîtriser l'algorithme de Gauss ou la diagonalisation pour la déterminer sans erreur.
        \item \textbf{Omettre la classification sur $\mathbb{C}$ :} Elle est plus simple mais fait partie du paysage.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 171 : Formes quadratiques réelles. Coniques. Exemples et applications.}

\begin{philosophie}
    C'est la leçon qui applique la théorie générale des formes quadratiques au cas réel, beaucoup plus riche que le cas complexe. L'invariant fondamental est la signature (loi de Sylvester). La leçon doit être très géométrique, en montrant comment cet invariant algébrique permet de classifier complètement les coniques et les quadriques, et comment il apparaît en analyse pour l'étude des points critiques.
\end{philosophie}

\section{Théorie Générale des Formes Quadratiques sur $\mathbb{R}$}
\subsection{Réduction et Invariants}
\begin{itemize}
    \item \textbf{Rappel :} Réduction de Gauss, existence d'une base orthogonale.
    \item \textbf{Loi d'Inertie de Sylvester :} Toute forme quadratique réelle $q$ est équivalente à une unique forme du type $\sum_{i=1}^s x_i^2 - \sum_{j=s+1}^{s+t} x_j^2$. Le couple $(s,t)$ est la \textbf{signature} de $q$.
    \item \textbf{Formes définies :} Caractérisation des formes définies positives/négatives par leur signature ($(n,0)$ ou $(0,n)$).
\end{itemize}

\section{Application à la Classification des Coniques}
\subsection{Cadre Affine}
\begin{itemize}
    \item \textbf{Définition :} Une conique est le lieu des points $(x,y)$ vérifiant une équation de degré 2 : $q(x,y) + l(x,y) + c = 0$, où $q$ est la partie quadratique.
    \item \textbf{Classification Affine :} Après un changement de repère affine, l'équation peut être réduite à l'une des 9 formes canoniques. La nature de la conique ne dépend que du rang et de la signature de $q$.
        \begin{itemize}
            \item \textbf{Genre Ellipse :} $x^2+y^2=1$, $x^2+y^2=-1$ (vide), $x^2+y^2=0$ (un point). Signature $(2,0)$.
            \item \textbf{Genre Hyperbole :} $x^2-y^2=1$. Signature $(1,1)$.
            \item \textbf{Genre Parabole :} $y=x^2$. Rang de $q$ est 1.
        \end{itemize}
\end{itemize}
\subsection{Cadre Euclidien}
\begin{itemize}
    \item \textbf{Réduction de l'équation :} On utilise le Théorème Spectral pour diagonaliser la matrice de $q$ dans une base orthonormée. Cela correspond à trouver les "axes principaux" de la conique.
    \item \textbf{Classification Euclidienne :} Fait intervenir les invariants euclidiens (excentricité, etc.)
\end{itemize}

\section{Application à la Classification des Quadriques}
\subsection{Principe de la Réduction}
\begin{itemize}
    \item La méthode est la même qu'en dimension 2 : on réduit la forme quadratique en 3 variables pour trouver la nature de la surface.
\end{itemize}
\subsection{Exemples de Quadriques}
\begin{itemize}
    \item \textbf{Ellipsoïde :} $x^2/a^2+y^2/b^2+z^2/c^2=1$. Signature $(3,0)$.
    \item \textbf{Hyperboloïde à une nappe :} $x^2/a^2+y^2/b^2-z^2/c^2=1$. Signature $(2,1)$. C'est une surface réglée.
    \item \textbf{Hyperboloïde à deux nappes :} $x^2/a^2+y^2/b^2-z^2/c^2=-1$. Signature $(2,1)$.
    \item \textbf{Paraboloïde elliptique et hyperbolique.}
\end{itemize}

\section{Autres Applications}
\subsection{Analyse : Étude des Points Critiques}
\begin{itemize}
    \item \textbf{Forme Hessienne :} La nature d'un point critique d'une fonction $f: \mathbb{R}^n \to \mathbb{R}$ est déterminée par la signature de sa forme quadratique Hessienne $d^2f(a)$.
    \item \textbf{Lemme de Morse :} Au voisinage d'un point critique non dégénéré, une fonction se comporte (à un changement de coordonnées près) comme sa partie quadratique.
\end{itemize}
\subsection{Géométrie Différentielle}
\begin{itemize}
    \item La première et la seconde forme fondamentale d'une surface sont des formes quadratiques sur le plan tangent, dont les signatures encodent la géométrie de la surface.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Classification affine des coniques :} Un développement très complet qui montre la maîtrise de la réduction des formes quadratiques.
        \item \textbf{Loi d'inertie de Sylvester :} Un théorème fondamental dont la preuve (montrant l'invariance de la signature) est très instructive.
        \item \textbf{Lien entre la signature de la Hessienne et la nature d'un point critique (Lemme de Morse) :} Un beau développement qui connecte la leçon à l'analyse.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre classification affine et euclidienne :} La classification affine ne "voit" pas les angles ni les distances. Une ellipse et un cercle sont affinement équivalents.
        \item \textbf{Faire une erreur dans l'algorithme de Gauss :} Il faut le maîtriser parfaitement sur un exemple.
        \item \textbf{Oublier les cas dégénérés} dans la classification (e.g., deux droites sécantes pour le genre hyperbole).
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 181 : Barycentres. Convexité. Applications.}

\begin{philosophie}
    C'est une leçon de géométrie à l'interface avec l'algèbre et l'analyse. Les barycentres sont un outil algébrique (combinaisons linéaires pondérées) pour traiter des problèmes de géométrie affine. La convexité est une propriété géométrique fondamentale avec des conséquences analytiques profondes (optimisation, séparation). Le plan doit montrer comment ces deux notions s'enrichissent mutuellement.
\end{philosophie}

\section{Barycentres dans un Espace Affine}
\subsection{Définition et Propriétés Fondamentales}
\begin{itemize}
    \item \textbf{Définition :} Soient $(A_i, \alpha_i)$ des points pondérés avec $\sum \alpha_i \neq 0$. Le barycentre $G$ est l'unique point vérifiant $\sum \alpha_i \vec{GA_i} = \vec{0}$.
    \item \textbf{Associativité du barycentre :} Permet de regrouper des sous-systèmes.
    \item \textbf{Coordonnées barycentriques :} Dans un repère affine, les coordonnées du barycentre sont les moyennes pondérées des coordonnées des points.
\end{itemize}

\section{Convexité : la Géométrie des Segments}
\subsection{Ensembles Convexes}
\begin{itemize}
    \item \textbf{Définition :} Un ensemble $C$ est convexe si pour tous $x,y \in C$, le segment $[x,y]$ est inclus dans $C$.
    \item \textbf{Propriétés de Stabilité :} L'intersection de convexes est convexe. L'image d'un convexe par une application affine est convexe.
    \item \textbf{Enveloppe Convexe $\mathrm{conv}(A)$ :} Le plus petit convexe contenant $A$. C'est l'ensemble des barycentres à coefficients positifs des points de $A$.
    \item \textbf{Théorème de Carathéodory :} Dans $\mathbb{R}^n$, tout point de $\mathrm{conv}(A)$ est un barycentre d'au plus $n+1$ points de $A$.
\end{itemize}
\subsection{Points Extrémaux et Théorème de Krein-Milman}
\begin{itemize}
    \item \textbf{Définition :} Un point $x$ d'un convexe $C$ est extrémal s'il n'est pas le milieu d'un segment non trivial de $C$.
    \item \textbf{Théorème de Krein-Milman :} Un convexe compact est l'enveloppe convexe fermée de ses points extrémaux.
\end{itemize}

\section{Théorèmes de Séparation et Applications}
\subsection{Séparation par des Hyperplans}
\begin{itemize}
    \item \textbf{Théorème de Hahn-Banach (forme géométrique) :} Deux convexes disjoints, dont l'un est ouvert, peuvent être séparés par un hyperplan affine. Si les deux sont compacts, la séparation peut être stricte.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Optimisation Convexe :} Un minimum local d'une fonction convexe sur un convexe est un minimum global. L'ensemble des minima est convexe.
    \item \textbf{Théorème du point fixe de Brouwer :} Toute application continue d'un convexe compact de $\mathbb{R}^n$ dans lui-même admet un point fixe.
\end{itemize}

\section{Fonctions Convexes}
\subsection{Définition et Propriétés}
\begin{itemize}
    \item \textbf{Définition :} $f$ est convexe si son épigraphe est un ensemble convexe.
    \item \textbf{Caractérisations différentielles} (tangentes, hessienne).
    \item \textbf{Inégalité de Jensen :} $f(E[X]) \le E[f(X)]$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de séparation de deux convexes par un hyperplan (Hahn-Banach géométrique) :} Un développement fondamental à la frontière de l'analyse et de la géométrie.
        \item \textbf{Théorème de Carathéodory :} Un beau résultat de géométrie combinatoire.
        \item \textbf{Le centre de gravité d'un triangle est le point de concours des médianes :} Preuve par associativité du barycentre.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre espace affine et vectoriel :} La notion de barycentre n'a de sens que dans un espace affine.
        \item \textbf{Mal énoncer les théorèmes de séparation :} Les hypothèses (ouvert, compact) sont cruciales.
        \item \textbf{Faire une leçon trop analytique ou trop géométrique :} L'intérêt est de montrer l'interaction entre les deux points de vue.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 190 : Méthodes combinatoires, problèmes de dénombrement.}

\begin{philosophie}
    C'est une leçon qui doit montrer une maîtrise des techniques de comptage, des plus élémentaires aux plus sophistiquées. L'objectif n'est pas de faire un catalogue de formules, mais de structurer le plan par \textbf{type de méthode}. Il faut montrer que face à un problème de dénombrement, on a une boîte à outils stratégique : le principe de bijection, les outils algébriques (séries génératrices), les outils d'inclusion-exclusion, et les outils géométriques (actions de groupes).
\end{philosophie}

\section{Principes Fondamentaux et Dénombrement Direct}
\subsection{Principes de Base}
\begin{itemize}
    \item \textbf{Principe de Bijection :} Pour compter un ensemble $A$, on le met en bijection avec un ensemble $B$ dont on connaît le cardinal. C'est le principe le plus fondamental.
    \item \textbf{Principes Additif et Multiplicatif.}
\end{itemize}
\subsection{Arrangements, Permutations, Combinaisons}
\begin{itemize}
    \item \textbf{Coefficients binomiaux $\binom{n}{k}$ :} Dénombrement des parties à $k$ éléments.
    \item \textbf{Formule du binôme de Newton et triangle de Pascal.}
\end{itemize}

\section{Dénombrement par l'Inclusion-Exclusion}
\subsection{Formule du Crible de Poincaré}
\begin{itemize}
    \item \textbf{Formule :} $|\cup A_i| = \sum |A_i| - \sum |A_i \cap A_j| + \dots$
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Nombre de dérangements :} Nombre de permutations sans point fixe.
    \item \textbf{Nombre de surjections} d'un ensemble de cardinal $n$ dans un ensemble de cardinal $p$.
\end{itemize}

\section{Dénombrement par les Séries Génératrices}
\subsection{Le Dictionnaire Combinatoire-Analyse}
\begin{itemize}
    \item \textbf{Série génératrice ordinaire $A(x) = \sum a_n x^n$ :} Encode une suite $(a_n)$.
    \item \textbf{Opérations :} Le produit des séries correspond au produit de convolution des suites, qui modélise la construction d'objets en deux étapes.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Relations de Récurrence Linéaires :} Les nombres de Fibonacci. La relation de récurrence donne une équation sur la série génératrice, dont on déduit une fraction rationnelle. La décomposition en éléments simples donne la formule explicite.
    \item \textbf{Problèmes de Partitions d'Entiers :} La série génératrice pour le nombre de partitions de $n$ est $\prod_{k=1}^\infty \frac{1}{1-x^k}$.
    \item \textbf{Nombres de Catalan :} La relation de récurrence $C_{n+1} = \sum_{k=0}^n C_k C_{n-k}$ se traduit par une équation quadratique sur la série génératrice, ce qui permet de trouver la formule pour $C_n$.
\end{itemize}

\section{Dénombrement par les Actions de Groupes}
\subsection{La Formule de Burnside}
\begin{itemize}
    \item \textbf{Principe :} Quand on dénombre des objets "à isomorphisme près" (des classes d'équivalence), on dénombre en fait des orbites.
    \item \textbf{Formule :} Le nombre d'orbites est la moyenne du nombre de points fixes.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Dénombrement de colliers :} On compte les colliers à $n$ perles avec $k$ couleurs, à rotation près (action de $\mathbb{Z}/n\mathbb{Z}$) ou à rotation et réflexion près (action de $D_n$).
    \item \textbf{Dénombrement des graphes non-isomorphes.}
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Dénombrement des dérangements par la formule du crible :} Un développement classique et très clair.
        \item \textbf{Nombres de Catalan via les séries génératrices :} Un superbe développement qui montre la puissance de l'outil analytique en combinatoire.
        \item \textbf{Application de la formule de Burnside au dénombrement des colliers :} Un exemple très visuel et convaincant.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Se limiter aux dénombrements de base :} Le jury attend de voir des méthodes plus sophistiquées.
        \item \textbf{Ne pas justifier la convergence des séries génératrices :} Souvent, on peut les traiter comme des séries formelles, mais il faut le préciser.
        \item \textbf{Faire une erreur de comptage dans l'application de Burnside :} Il faut être très systématique.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 191 : Exemples d'utilisation des techniques d'algèbre en géométrie.}

\begin{philosophie}
    C'est la leçon de "culture" et de "vision unificatrice" par excellence. L'objectif est de montrer que l'on a compris que l'algèbre n'est pas un but en soi, mais un langage et un outil pour résoudre des problèmes dans d'autres domaines, en particulier la géométrie. Le plan doit être une galerie de trois ou quatre exemples spectaculaires, où un problème d'origine purement géométrique est résolu de manière élégante par sa traduction en termes algébriques.
\end{philosophie}

\section{L'Algèbre Linéaire au Service de la Géométrie Affine et Projective}
\subsection{Le Point de Vue Vectoriel}
\begin{itemize}
    \item \textbf{Principe :} On "vectorialise" l'espace affine pour utiliser la puissance de l'algèbre linéaire.
    \item \textbf{Application : Théorème de Ménélaüs et de Céva.} Ces théorèmes de géométrie du triangle peuvent être démontrés de manière très efficace en utilisant des barycentres ou des coordonnées.
\end{itemize}
\subsection{La Géométrie Projective}
\begin{itemize}
    \item \textbf{Principe :} Le plan projectif est l'ensemble des droites vectorielles d'un espace de dimension 3. Les transformations projectives sont induites par $GL_3(K)$.
    \item \textbf{Application : Théorème de Pappus.} Ce théorème sur l'alignement de points d'intersection de droites, complexe en géométrie euclidienne, devient une simple vérification algébrique en coordonnées homogènes.
\end{itemize}

\section{La Théorie des Groupes pour Classifier les Géométries}
\subsection{Le Programme d'Erlangen de Felix Klein}
\begin{itemize}
    \item \textbf{Philosophie :} Une géométrie est définie par un espace et un groupe de transformations. Les "théorèmes" de cette géométrie sont les propriétés qui sont invariantes sous l'action de ce groupe.
    \item \textbf{Exemples :}
        \begin{itemize}
            \item Géométrie euclidienne $\leftrightarrow$ Groupe des isométries.
            \item Géométrie affine $\leftrightarrow$ Groupe affine.
            \item Topologie $\leftrightarrow$ Groupe des homéomorphismes.
        \end{itemize}
\end{itemize}
\subsection{Application à la Classification des Isométries}
\begin{itemize}
    \item \textbf{Problème Géométrique :} Classifier les "déplacements rigides" de l'espace.
    \item \textbf{Traduction Algébrique :} Classifier les éléments du groupe orthogonal $O(3)$.
    \item \textbf{Résultat :} On montre par des arguments d'algèbre linéaire (valeurs propres, etc.) que les isométries directes sont des rotations.
\end{itemize}

\section{La Théorie des Corps pour les Constructions à la Règle et au Compas}
\subsection{La Traduction Algébrique}
\begin{itemize}
    \item \textbf{Problème Géométrique :} Un point est-il constructible à la règle et au compas ?
    \item \textbf{Traduction Algébrique :} Un nombre est constructible si et seulement si son corps de rupture est contenu dans une tour d'extensions quadratiques.
\end{itemize}
\subsection{Résolution des Problèmes Antiques}
\begin{itemize}
    \item \textbf{Impossibilité de la duplication du cube :} Le problème se traduit par la constructibilité de $\sqrt[3]{2}$, dont l'extension associée est de degré 3.
\end{itemize}

\section{L'Algèbre Commutative pour la Géométrie Algébrique}
\subsection{Le Dictionnaire Idéaux-Variétés}
\begin{itemize}
    \item \textbf{Principe :} Il y a une correspondance entre les ensembles de solutions d'équations polynomiales (les "variétés affines") et les idéaux de l'anneau de polynômes.
    \item \textbf{Théorème des Zéros de Hilbert (Nullstellensatz) :} Ce théorème fondamental établit le dictionnaire précis entre les objets algébriques (idéaux maximaux, premiers) et les objets géométriques (points, sous-variétés irréductibles).
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Impossibilité de la duplication du cube :} Un développement parfait pour cette leçon, qui illustre la traduction d'un problème géométrique en un problème d'extensions de corps.
        \item \textbf{Le groupe des isométries directes du cube est isomorphe à $\mathfrak{S}_4$ :} Un autre excellent choix, qui utilise la théorie des groupes pour résoudre un problème de géométrie.
        \item \textbf{Théorème de Pappus en géométrie projective :} Montre la puissance de la "vectorialisation" et des coordonnées homogènes.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un catalogue :} La leçon doit être une narration. Chaque partie doit présenter un problème géométrique et montrer comment l'algèbre le résout.
        \item \textbf{Choisir des exemples trop simples :} Il faut montrer des cas où l'algèbre apporte une simplification ou une solution qui serait très difficile à obtenir autrement.
        \item \textbf{Ne pas bien expliquer la traduction :} L'intérêt de la leçon est de montrer comment on passe du monde de la géométrie à celui de l'algèbre. Cette "traduction" doit être au cœur de l'exposé.
    \end{itemize}
\end{erreurs}

\part{Leçons sur l'Analyse et la Topologie des Fonctions}

\chapter{Leçon 201 : Espaces de fonctions. Exemples et applications.}

\begin{philosophie}
    Cette leçon est une invitation à un voyage dans le "zoo" des espaces fonctionnels. L'objectif n'est pas de faire un catalogue, mais de montrer que chaque espace est un "outil" forgé pour un but précis. Le plan doit s'organiser par type de structure ou de problème : la topologie (convergence uniforme), l'intégration (convergence $L^p$), la régularité (espaces de Sobolev). C'est une leçon transversale qui teste la culture mathématique et la capacité à choisir le bon cadre pour un problème donné.
\end{philosophie}

\section{Espaces de Fonctions pour la Topologie et l'Analyse}
\subsection{Le Cadre de la Convergence Uniforme}
\begin{itemize}
    \item \textbf{Espace $\mathcal{C}_b(X, E)$ :} Espace des fonctions continues et bornées sur un espace topologique $X$, à valeurs dans un Banach $E$. Muni de la norme sup $\|f\|_\infty = \sup_{x \in X} \|f(x)\|$, c'est un espace de Banach.
    \item \textbf{L'Outil Clé : Théorème d'Arzelà-Ascoli.}
        \begin{itemize}
            \item \textbf{Énoncé :} Une partie de $\mathcal{C}(K, E)$ (où $K$ est compact) est relativement compacte si et seulement si elle est bornée, équicontinue, et si pour tout $x$, l'ensemble $\{f(x) \mid f \in A\}$ est relativement compact dans $E$.
            \item \textbf{Philosophie :} En dimension infinie, "borné" ne suffit plus pour la compacité. L'équicontinuité est la condition additionnelle "miracle" qui permet de la retrouver.
        \end{itemize}
\end{itemize}
\subsection{Le Cadre de l'Approximation}
\begin{itemize}
    \item \textbf{Théorème de Stone-Weierstrass :} Une sous-algèbre de $\mathcal{C}(K, \mathbb{R})$ (K compact) qui sépare les points et contient les constantes est dense. C'est la généralisation ultime du théorème de Weierstrass polynomial.
    \item \textbf{Application :} Les polynômes trigonométriques sont denses dans l'espace des fonctions continues $2\pi$-périodiques.
\end{itemize}

\section{Espaces de Fonctions pour l'Intégration : les Espaces $L^p$}
\subsection{La Révolution de Lebesgue}
\begin{itemize}
    \item \textbf{Définition :} Espaces $L^p(\Omega, \mu)$ pour $p \in [1, \infty]$.
    \item \textbf{Théorème de Riesz-Fischer :} Les espaces $L^p$ sont des espaces de Banach. C'est la justification principale de la théorie de Lebesgue : on obtient des espaces complets, contrairement à l'intégration de Riemann.
    \item \textbf{Dualité :} Le dual de $L^p$ est $L^q$ (pour $p \in [1, \infty[$). $L^2$ est un Hilbert auto-dual.
\end{itemize}
\subsection{Relations d'Inclusion et de Densité}
\begin{itemize}
    \item \textbf{Inclusions :} Si la mesure est finie, on a les inclusions $L^q \subset L^p$ pour $p < q$.
    \item \textbf{Théorèmes de Densité :} L'espace des fonctions continues à support compact $\mathcal{C}_c(\Omega)$ est dense dans $L^p(\Omega)$ pour $p \in [1, \infty[$. Cela permet d'approcher des objets "bruts" de $L^p$ par des fonctions très régulières.
\end{itemize}

\section{Espaces de Fonctions pour la Régularité et les EDP}
\subsection{Les Espaces de Sobolev $H^k(\Omega)$}
\begin{itemize}
    \item \textbf{Motivation :} Comment mesurer la régularité d'une fonction tout en bénéficiant de la structure d'un espace de Hilbert ?
    \item \textbf{Définition :} $H^k(\Omega)$ est l'espace des fonctions de $L^2(\Omega)$ dont les dérivées (au sens des distributions) jusqu'à l'ordre $k$ sont aussi dans $L^2(\Omega)$. C'est un espace de Hilbert.
    \item \textbf{Application :} $H_0^1(\Omega)$ est le cadre naturel pour la formulation variationnelle des problèmes elliptiques (Laplacien).
\end{itemize}
\subsection{Les Espaces de Schwartz et les Distributions}
\begin{itemize}
    \item \textbf{Espace de Schwartz $\mathcal{S}(\mathbb{R}^n)$ :} L'espace des fonctions $\mathcal{C}^\infty$ à décroissance rapide. La transformée de Fourier est un automorphisme de cet espace.
    \item \textbf{Espace des distributions tempérées $\mathcal{S}'(\mathbb{R}^n)$ :} Le dual topologique de $\mathcal{S}$. C'est le cadre le plus général où la transformée de Fourier se comporte bien.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème d'Arzelà-Ascoli :} Un développement central de l'analyse, indispensable.
        \item \textbf{Démonstration que $\mathcal{C}([a,b])$ muni de la norme $L^1$ n'est pas complet :} Un contre-exemple essentiel pour motiver les espaces $L^p$.
        \item \textbf{Théorème de Stone-Weierstrass (version réelle) :} Un développement de haut niveau qui montre une grande maîtrise de la topologie et de l'analyse.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un catalogue sans fil directeur :} Le plan doit être structuré par les propriétés (topologie, intégration, régularité).
        \item \textbf{Confondre les différents modes de convergence :} Uniforme, $L^p$, simple... Il faut être très précis.
        \item \textbf{Oublier les théorèmes de densité :} Ils sont cruciaux car ils sont la base des arguments d'approximation.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 202 : Exemples de parties denses et applications.}

\begin{philosophie}
    C'est une leçon sur l'art de l'approximation. La densité est la formalisation de l'idée que l'on peut approcher un objet "compliqué" d'aussi près que l'on veut par des objets "simples". Une bonne leçon doit être une galerie d'exemples spectaculaires montrant comment cette idée est mise en œuvre dans tous les domaines des mathématiques : analyse, algèbre, topologie...
\end{philosophie}

\section{Densité dans les Espaces Numériques}

\subsection{Les Ensembles de Nombres Classiques}
\begin{itemize}
    \item \textbf{Densité de $\mathbb{Q}$ dans $\mathbb{R}$ :} Le résultat fondamental. Tout réel est limite d'une suite de rationnels.
    \item \textbf{Application :} Permet d'étendre par continuité des propriétés des rationnels aux réels.
    \item \textbf{Densité des nombres décimaux $\mathbb{D}$ dans $\mathbb{R}$.}
\end{itemize}

\subsection{Densité dans les Espaces de Matrices}
\begin{itemize}
    \item \textbf{Théorème :} L'ensemble des matrices diagonalisables à valeurs propres distinctes (et donc diagonalisables) est \textbf{dense} dans $\mathcal{M}_n(\mathbb{C})$.
    \item \textbf{Contre-exemple sur $\mathbb{R}$ :} L'ensemble des matrices $\mathbb{R}$-diagonalisables n'est pas dense dans $\mathcal{M}_n(\mathbb{R})$. Une rotation ne peut pas être approchée par des matrices diagonalisables.
    \item \textbf{Théorème :} Le groupe linéaire $GL_n(K)$ est un ouvert \textbf{dense} de $\mathcal{M}_n(K)$.
\end{itemize}
\begin{application}
    La densité des matrices inversibles est la clé pour démontrer le théorème de Cayley-Hamilton par un argument de densité.
\end{application}

\section{Approximation de Fonctions : les Théorèmes de Weierstrass}

\subsection{Approximation par des Polynômes}
\begin{itemize}
    \item \textbf{Théorème de Weierstrass (version polynomiale) :} Les polynômes sont denses dans l'espace $\mathcal{C}([a,b], \mathbb{R})$ pour la norme de la convergence uniforme.
    \item \textbf{Preuve constructive :} On peut le prouver en utilisant les polynômes de Bernstein, qui fournissent une suite explicite de polynômes convergeant uniformément vers la fonction.
\end{itemize}

\subsection{Approximation par des Polynômes Trigonométriques}
\begin{itemize}
    \item \textbf{Théorème de Weierstrass (version trigonométrique) :} Les polynômes trigonométriques sont denses dans l'espace des fonctions continues $2\pi$-périodiques.
    \item \textbf{Preuve constructive :} La preuve via les moyennes de Césaro (noyau de Fejér) est un grand classique de l'analyse de Fourier.
\end{itemize}

\subsection{Généralisation : le Théorème de Stone-Weierstrass}
\begin{itemize}
    \item \textbf{Énoncé :} C'est le cadre conceptuel qui unifie les deux théorèmes précédents.
\end{itemize}

\section{Densité dans les Espaces de l'Intégration et de l'Analyse Fonctionnelle}

\subsection{Densité dans les Espaces $L^p$}
\begin{itemize}
    \item \textbf{Hiérarchie de l'Approximation :} Dans $L^p(\mathbb{R}^n)$, on a la chaîne de densités :
    $$ \mathcal{C}_c^\infty(\mathbb{R}^n) \subset \mathcal{C}_c(\mathbb{R}^n) \subset L^p(\mathbb{R}^n) $$
    Les fonctions très régulières sont denses.
    \item \textbf{Application Fondamentale :} Pour prouver une propriété sur toutes les fonctions de $L^p$, il suffit souvent de la prouver sur les fonctions $\mathcal{C}_c^\infty$ (où l'on peut intégrer par parties, dériver...) puis de passer à la limite. C'est le "principe de densité".
\end{itemize}

\subsection{Densité dans les Espaces de Hilbert}
\begin{itemize}
    \item \textbf{Caractérisation des bases hilbertiennes :} Une famille orthonormale est une base hilbertienne si et seulement si le sous-espace qu'elle engendre est dense dans l'espace de Hilbert.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de Weierstrass trigonométrique (preuve par le noyau de Fejér) :} Un développement d'analyse très classique et élégant.
        \item \textbf{Densité des matrices diagonalisables sur $\mathbb{C}$ :} Un beau développement qui connecte réduction et topologie.
        \item \textbf{Démonstration que $\mathcal{C}_c(\mathbb{R}^n)$ est dense dans $L^p(\mathbb{R}^n)$ :} Un développement technique mais fondamental de la théorie de la mesure.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Ne pas préciser la topologie :} Dire qu'un ensemble est dense n'a de sens que si l'on précise pour quelle topologie (ou norme).
        \item \textbf{Extrapoler le cas réel au cas complexe (et vice-versa) :} La densité des matrices diagonalisables est un contre-exemple parfait.
        \item \textbf{Faire un catalogue sans montrer l'utilité :} Le titre est "et applications". Il faut montrer que la densité est un outil d'approximation.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 203 : Utilisation de la notion de compacité.}

\begin{philosophie}
    Le titre est "Utilisation...". Cette leçon ne doit absolument pas être un cours sur la compacité. C'est une leçon transversale qui doit montrer que la compacité est l'outil fondamental de l'analyse pour prouver des théorèmes d'**existence**. Le plan doit être une galerie d'exemples spectaculaires où la compacité permet de transformer un problème infini en un problème fini, ou de garantir l'existence de limites.
\end{philosophie}

\subsection*{Plan Détaillé}
\begin{itemize}
    \item[\textbf{I.}] \textbf{Compacité et Existence d'Extrema}
    \begin{enumerate}
        \item Théorème fondamental : une fonction continue sur un compact atteint ses bornes.
        \item Application en algèbre : Preuve de l'existence d'une valeur propre pour un endomorphisme symétrique via la compacité de la sphère unité.
        \item Application en topologie : Le groupe orthogonal $O(n)$ est un compact.
    \end{enumerate}
    \item[\textbf{II.}] \textbf{Compacité dans les Espaces de Fonctions}
    \begin{enumerate}
        \item Le Théorème d'Arzelà-Ascoli : la condition "miracle" d'équicontinuité.
        \item Application à l'existence de solutions d'EDO : Théorème de Peano (via la méthode d'Euler).
        \item La compacité faible : Théorème de Banach-Alaoglu et applications au calcul des variations.
    \end{enumerate}
    \item[\textbf{III.}] \textbf{Compacité et Points Fixes}
    \begin{enumerate}
        \item Théorème du point fixe de Brouwer en dimension finie.
        \item Théorème du point fixe de Schauder en dimension infinie.
        \item Application à l'existence de solutions d'équations intégrales.
    \end{enumerate}
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème d'Arzelà-Ascoli (Couteau-suisse) :} Un développement central de l'analyse. Casable dans les leçons 201, 205, 209...
        \item \textbf{Le groupe orthogonal $O(n)$ est un compact de $\mathcal{M}_n(\mathbb{R})$ :} Un développement élégant qui mêle topologie, algèbre linéaire et géométrie.
        \item \textbf{Démonstration du théorème spectral pour les opérateurs compacts auto-adjoints :} Un développement de haut niveau qui montre une grande maîtrise de l'analyse fonctionnelle.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Réciter un cours sur la compacité.} C'est un hors-sujet.
        \item \textbf{Ne pas distinguer compacité en dimension finie (fermé borné) et infinie (où c'est faux).}
        \item \textbf{Oublier de mentionner la compacité faible,} qui est une notion clé en analyse moderne et montre une grande maturité.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 204 : Connexité. Exemples et applications.}

\begin{philosophie}
    La connexité est la formalisation topologique de l'idée "d'être d'un seul tenant". C'est une notion globale, dont la principale puissance est de fournir une version très générale du Théorème des Valeurs Intermédiaires. La leçon doit illustrer cette idée à travers des exemples variés, de l'analyse (TVI) à l'algèbre (connexité de groupes topologiques).
\end{philosophie}

\section{Définitions et Propriétés Fondamentales}

\subsection{Connexité}
\begin{itemize}
    \item \textbf{Définition :} Un espace topologique est connexe s'il n'est pas la réunion de deux ouverts non vides disjoints.
    \item \textbf{Propriétés de Stabilité :} L'image continue d'un connexe est connexe. Un produit de connexes est connexe. L'adhérence d'un connexe est connexe.
\end{itemize}

\subsection{Connexité par Arcs}
\begin{itemize}
    \item \textbf{Définition :} Un espace est connexe par arcs si tout couple de points peut être relié par un chemin continu.
    \item \textbf{Théorème :} La connexité par arcs implique la connexité.
    \item \textbf{Le Contre-Exemple Classique : le Sinus du Topologue.} La courbe $G = \{ (x, \sin(1/x)) \mid x \in ]0,1] \}$ est connexe par arcs. Son adhérence $\bar{G}$ (en ajoutant le segment $\{0\} \times [-1,1]$) est connexe mais pas connexe par arcs.
\end{itemize}

\section{Applications Analytiques}

\subsection{Les Connexes de $\mathbb{R}$}
\begin{itemize}
    \item \textbf{Théorème :} Une partie de $\mathbb{R}$ est connexe si et seulement si c'est un intervalle.
\end{itemize}

\subsection{Le Théorème des Valeurs Intermédiaires}
\begin{itemize}
    \item \textbf{Énoncé Général :} L'image d'un espace connexe par une application continue est connexe.
    \item \textbf{Corollaire (TVI classique) :} Si $f: [a,b] \to \mathbb{R}$ est continue, son image est un intervalle.
    \item \textbf{Application au Point Fixe :} Théorème du point fixe de Brouwer en dimension 1.
\end{itemize}

\section{Applications Algébriques et Géométriques}

\subsection{Connexité dans les Groupes Topologiques}
\begin{itemize}
    \item \textbf{Composante Connexe de l'Identité :} La composante connexe de l'élément neutre $G_0$ est un sous-groupe distingué de $G$.
    \item \textbf{Exemple : $GL_n(\mathbb{R})$.} Le déterminant est une application continue de $GL_n(\mathbb{R})$ sur $\mathbb{R}^*$. L'image n'est pas connexe. $GL_n(\mathbb{R})$ a deux composantes connexes : les matrices à déterminant positif ($GL_n^+(\mathbb{R})$) et négatif.
    \item \textbf{Théorème : $O(n), SO(n), U(n), SU(n)$.} Les groupes $SO(n), U(n), SU(n)$ sont connexes. $O(n)$ a deux composantes connexes.
\end{itemize}
\subsection{Connexité et Convexité}
\begin{itemize}
    \item \textbf{Proposition :} Tout ensemble convexe dans un e.v.n. est connexe par arcs (et donc connexe).
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration que les connexes de $\mathbb{R}$ sont les intervalles :} Un résultat de base mais dont la preuve doit être parfaitement maîtrisée.
        \item \textbf{Connexité de $SO(n)$ :} Un superbe développement qui mêle algèbre, géométrie et topologie, en montrant que tout voisinage de l'identité engendre le groupe.
        \item \textbf{Le sinus du topologue est connexe mais pas connexe par arcs :} Un contre-exemple très formateur.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre connexité et connexité par arcs :} C'est l'erreur la plus grave. Il faut connaître le contre-exemple classique.
        \item \textbf{Ne pas savoir prouver le résultat de base sur les connexes de $\mathbb{R}$.}
        \item \textbf{Faire un plan trop théorique :} Le titre mentionne les applications. Le TVI et la connexité des groupes classiques doivent être des points centraux.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 205 : Espaces complets. Exemples et applications.}

\begin{philosophie}
    C'est la leçon sur la "puissance de l'analyse". La complétude est la propriété qui permet de passer à la limite et de garantir que les processus d'approximation convergent. Le plan doit mettre en évidence ce rôle fondamental, en présentant d'abord le théorème du point fixe comme l'outil constructif par excellence, puis en dévoilant les grands théorèmes d'analyse fonctionnelle qui découlent de la complétude via le lemme de Baire.
\end{philosophie}

\section{Définitions, Exemples et Contre-exemples}
\subsection{Cadre Métrique}
\begin{itemize}
    \item \textbf{Définition :} Espace métrique complet.
    \item \textbf{Exemple Fondamental :} $\mathbb{R}$ est complet, $\mathbb{Q}$ ne l'est pas.
\end{itemize}
\subsection{Cadre des Espaces Vectoriels Normés}
\begin{itemize}
    \item \textbf{Définition (Espace de Banach) :} Un e.v.n. complet.
    \item \textbf{Exemples :}
        \begin{itemize}
            \item Tout e.v.n. de dimension finie est un espace de Banach.
            \item L'espace $\mathcal{C}([a,b])$ muni de la norme sup est un Banach.
            \item Les espaces $L^p$ (pour $p \ge 1$) sont des Banach (Théorème de Riesz-Fischer).
        \end{itemize}
    \item \textbf{Contre-exemple crucial :} L'espace des fonctions continues sur $[a,b]$ muni de la norme $L^1$ n'est pas complet. C'est une motivation pour l'introduction de l'intégrale de Lebesgue.
\end{itemize}
\subsection{Théorème de Complétion}
\begin{itemize}
    \item Tout espace métrique peut être "plongé" de manière isométrique dans un unique espace complet (son complété). Exemple : $\mathbb{R}$ est le complété de $\mathbb{Q}$.
\end{itemize}

\section{Le Théorème du Point Fixe : une Conséquence Constructive}
\subsection{Théorème de Banach-Picard}
\begin{itemize}
    \item \textbf{Énoncé :} Dans un espace métrique complet, une application strictement contractante admet un unique point fixe.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Théorème de Cauchy-Lipschitz :} Preuve de l'existence et de l'unicité de la solution d'une EDO.
    \item \textbf{Théorème d'inversion locale :} La preuve utilise une itération de point fixe.
    \item \textbf{Résolution de systèmes linéaires par des méthodes itératives.}
\end{itemize}

\section{Les Grands Théorèmes : Conséquences de la Complétude et de Baire}
\subsection{Le Théorème de Baire}
\begin{itemize}
    \item \textbf{Énoncé :} Dans un espace métrique complet, une intersection dénombrable d'ouverts denses est dense.
\end{itemize}
\subsection{Les Piliers de l'Analyse Fonctionnelle}
\begin{itemize}
    \item \textbf{Théorème de Banach-Steinhaus (Principe de la borne uniforme) :} Une famille d'opérateurs simplement bornée est uniformément bornée.
    \item \textbf{Théorème de l'Application Ouverte :} Une application linéaire continue surjective entre deux Banach est ouverte.
    \item \textbf{Corollaire (Isomorphisme de Banach) :} Toute application linéaire continue bijective entre deux Banach est un homéomorphisme.
    \item \textbf{Théorème du Graphe Fermé :} Une application linéaire entre deux Banach est continue si et seulement si son graphe est fermé.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème du point fixe de Banach et application à Cauchy-Lipschitz :} Un développement magnifique qui montre la puissance de l'analyse fonctionnelle pour résoudre des problèmes d'analyse "classique".
        \item \textbf{Théorème de Baire :} Un développement de topologie de base, mais dont les applications sont spectaculaires.
        \item \textbf{Démonstration du théorème de l'isomorphisme de Banach à partir du théorème de l'application ouverte :} Un développement très formateur en analyse fonctionnelle.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Ne pas citer le théorème de Baire :} C'est la clé cachée derrière les grands théorèmes d'analyse fonctionnelle.
        \item \textbf{Oublier les contre-exemples :} Montrer que l'espace des fonctions continues avec la norme $L^1$ n'est pas complet est essentiel pour justifier l'intérêt de la leçon.
        \item \textbf{Faire une leçon qui ne parle que de topologie générale :} Le titre est "Espaces complets", il faut rapidement arriver aux espaces de Banach et à leurs applications.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 207 : Prolongement de fonctions. Exemples et applications.}

\begin{philosophie}
    C'est une leçon transversale qui illustre un problème fondamental en mathématiques : connaissant un objet sur une petite partie de son univers, peut-on l'étendre à l'univers entier en conservant ses bonnes propriétés ? Le plan doit s'organiser par type de propriété à conserver : la continuité, la linéarité, l'analyticité. Chaque partie révèle un grand théorème d'analyse.
\end{philosophie}

\section{Prolongement par Continuité}
\subsection{Le Cadre des Espaces Métriques}
\begin{itemize}
    \item \textbf{Théorème :} Soit $f: A \to Y$ une application continue, où $A$ est une partie dense d'un espace métrique $X$ et $Y$ est un espace métrique complet. Alors $f$ se prolonge de manière unique en une application continue $\tilde{f}: X \to Y$ si et seulement si $f$ est uniformément continue sur $A$.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Construction de l'intégrale :} On définit l'intégrale sur les fonctions en escalier, puis on la prolonge par densité à un espace plus grand.
    \item \textbf{Construction des espaces $L^p$ :} Vus comme le complété de l'espace des fonctions continues pour la norme $L^p$.
\end{itemize}

\section{Prolongement des Applications Linéaires : Hahn-Banach}
\subsection{La Puissance de la Dualité}
\begin{itemize}
    \item \textbf{Motivation :} Peut-on prolonger une forme linéaire définie sur un sous-espace à l'espace entier, sans augmenter sa "taille" (sa norme) ?
    \item \textbf{Théorème de Hahn-Banach (forme analytique) :} Soit $F$ un s.e.v. d'un e.v.n. $E$. Toute forme linéaire continue $\phi$ sur $F$ peut être prolongée en une forme linéaire continue $\tilde{\phi}$ sur $E$ telle que $\|\tilde{\phi}\|_{E^*} = \|\phi\|_{F^*}$.
\end{itemize}
\subsection{Interprétation Géométrique : Théorèmes de Séparation}
\begin{itemize}
    \item \textbf{Théorème (Hahn-Banach géométrique) :} Deux convexes disjoints, dont l'un est ouvert, peuvent être séparés par un hyperplan affine fermé.
\end{itemize}

\section{Prolongement de Fonctions Analytiques}
\subsection{La Rigidité des Fonctions Holomorphes}
\begin{itemize}
    \item \textbf{Théorème (Principe des zéros isolés) :} Les zéros d'une fonction holomorphe non nulle sur un domaine sont isolés.
    \item \textbf{Théorème du Prolongement Analytique :} Soient $f,g$ deux fonctions holomorphes sur un domaine connexe $\Omega$. Si $f$ et $g$ coïncident sur un ensemble de points ayant un point d'accumulation dans $\Omega$, alors $f=g$ sur tout $\Omega$.
\end{itemize}
\begin{remark}
    C'est la forme la plus spectaculaire de prolongement. La connaissance d'une fonction sur un ensemble même très petit (par exemple, un segment de droite) détermine entièrement et de manière unique la fonction sur le plus grand domaine connexe possible.
\end{remark}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Principe de symétrie de Schwarz :} Permet de prolonger une fonction holomorphe à travers un segment de l'axe réel.
    \item \textbf{Définition de fonctions :} La fonction Gamma d'Euler, définie par une intégrale pour $\mathrm{Re}(z)>0$, peut être prolongée en une fonction méromorphe sur tout $\mathbb{C}$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de Hahn-Banach (forme analytique pour un espace normé réel) :} Un développement de base de l'analyse fonctionnelle.
        \item \textbf{Démonstration de l'unicité du prolongement analytique :} Un développement élégant qui utilise le principe des zéros isolés.
        \item \textbf{Théorème de Tietze-Urysohn (culture) :} Un résultat de topologie générale qui affirme que toute fonction continue définie sur un fermé d'un espace normal peut être prolongée à l'espace entier.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les différents types de prolongement :} Le prolongement par continuité uniforme est très différent du prolongement analytique.
        \item \textbf{Mal énoncer Hahn-Banach :} Oublier la conservation de la norme.
        \item \textbf{Oublier l'hypothèse de connexité} pour le prolongement analytique.
    \end{itemize}
\end{erreurs}

\newpage
\chapter{Leçon 208 : Espaces vectoriels normés, applications linéaires continues.}

\begin{philosophie}
    C'est la leçon fondatrice de l'analyse fonctionnelle. Elle introduit les objets de base (e.v.n., Banach) et leurs morphismes (applications linéaires continues). L'objectif est de montrer comment la combinaison d'une structure algébrique et d'une structure topologique permet de développer une théorie riche. Le plan doit être très structuré : d'abord les objets, puis les morphismes, et enfin les grands théorèmes qui montrent la puissance de ce cadre.
\end{philosophie}

\section{Structure des Espaces Vectoriels Normés}
\subsection{Normes et Topologie}
\begin{itemize}
    \item \textbf{Définition :} Norme, distance associée, topologie.
    \item \textbf{Le Cas de la Dimension Finie :}
        \begin{itemize}
            \item \textbf{Théorème d'équivalence des normes :} Toutes les normes sur un espace de dimension finie sont équivalentes.
            \item \textbf{Conséquence :} La topologie est unique. Un e.v.n. de dimension finie est toujours un espace de Banach. Les compacts sont les fermés bornés.
        \end{itemize}
\end{itemize}
\subsection{Le Cas de la Dimension Infinie}
\begin{itemize}
    \item \textbf{Non-équivalence des normes :} Sur $\mathcal{C}([0,1])$, les normes $\|\cdot\|_1, \|\cdot\|_2, \|\cdot\|_\infty$ ne sont pas équivalentes.
    \item \textbf{Le Lemme de Riesz et la non-compacité :} La boule unité fermée n'est jamais compacte en dimension infinie.
\end{itemize}

\section{Applications Linéaires Continues}
\subsection{Caractérisation de la Continuité}
\begin{itemize}
    \item \textbf{Théorème :} Pour une application linéaire $u: E \to F$, les propositions suivantes sont équivalentes :
        \begin{enumerate}
            \item $u$ est continue.
            \item $u$ est continue en 0.
            \item $u$ est lipschitzienne.
            \item $u$ est bornée (i.e., transforme la boule unité en un ensemble borné).
        \end{enumerate}
\end{itemize}
\subsection{L'Espace de Banach $\mathcal{L}(E,F)$}
\begin{itemize}
    \item \textbf{Norme d'opérateur :} Définition de $\|u\| = \sup_{\|x\|=1} \|u(x)\|$.
    \item \textbf{Théorème :} Si $F$ est un espace de Banach, alors $\mathcal{L}(E,F)$ est aussi un espace de Banach.
    \item \textbf{Le dual topologique $E^* = \mathcal{L}(E, K)$ est donc toujours un Banach.}
\end{itemize}

\section{Les Grands Théorèmes de l'Analyse Fonctionnelle}
\begin{objectif}
    Montrer que l'hypothèse de complétude (le fait de travailler avec des espaces de Banach) est la clé qui débloque une série de théorèmes d'une puissance spectaculaire, qui n'ont pas d'équivalent en dimension finie.
\end{objectif}
\begin{itemize}
    \item \textbf{Théorème de Hahn-Banach :} Existence de prolongements pour les formes linéaires.
    \item \textbf{Théorème de Banach-Steinhaus :} Une famille d'opérateurs simplement bornée est uniformément bornée.
    \item \textbf{Théorème de l'Application Ouverte :} Une application linéaire continue surjective entre deux Banach est ouverte.
    \item \textbf{Théorème du Graphe Fermé :} Une application linéaire entre deux Banach est continue si et seulement si son graphe est fermé.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème d'équivalence des normes en dimension finie :} Un développement de base essentiel, qui utilise la compacité.
        \item \textbf{Démonstration que $\mathcal{L}(E,F)$ est un Banach si $F$ l'est :} Un développement très formateur sur la complétude.
        \item \textbf{Un des grands théorèmes (e.g., Banach-Steinhaus ou Graphe Fermé) :} Un choix de haut niveau qui montre une grande maîtrise du sujet.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Passer trop de temps sur la dimension finie :} La leçon prend tout son sens en dimension infinie.
        \item \textbf{Oublier la complétude de $\mathcal{L}(E,F)$ :} C'est un résultat central.
        \item \textbf{Ne pas connaître les énoncés des grands théorèmes :} Ils sont le point culminant de la théorie.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 209 : Approximation d'une fonction par des fonctions régulières. Exemples et applications.}

\begin{philosophie}
    C'est la leçon sur la densité. L'idée centrale de l'analyse est souvent de prouver un résultat sur une classe de fonctions "simples" et "régulières", puis de l'étendre par densité à un espace plus grand et plus "compliqué". Cette leçon doit être une galerie de ces théorèmes de densité fondamentaux, en organisant le plan par type d'approximation (uniforme, $L^p$, ...).
\end{philosophie}

\section{Approximation Uniforme sur un Compact}
\subsection{Le Théorème de Weierstrass}
\begin{itemize}
    \item \textbf{Version Polynomiale :} L'ensemble des polynômes est dense dans $\mathcal{C}([a,b], \mathbb{R})$ pour la norme de la convergence uniforme.
    \item \textbf{Version Trigonométrique :} L'ensemble des polynômes trigonométriques est dense dans l'espace des fonctions continues $2\pi$-périodiques.
\end{itemize}
\subsection{Preuves Constructives}
\begin{itemize}
    \item \textbf{Polynômes de Bernstein :} Fournissent une suite explicite de polynômes qui converge uniformément vers la fonction. La preuve est probabiliste.
    \item \textbf{Noyau de Fejér :} La convergence des moyennes de Césaro des sommes partielles de la série de Fourier est uniforme, ce qui prouve la version trigonométrique.
\end{itemize}
\subsection{Généralisation : le Théorème de Stone-Weierstrass}
\begin{itemize}
    \item \textbf{Énoncé (version réelle) :} Une sous-algèbre de $\mathcal{C}(K, \mathbb{R})$ (K compact) qui sépare les points et contient les constantes est dense.
\end{itemize}

\section{Approximation dans les Espaces $L^p$}
\subsection{La Hiérarchie de la Densité}
\begin{itemize}
    \item \textbf{Théorème :} Dans $L^p(\mathbb{R}^n)$ (pour $p \in [1, \infty[$), on a la chaîne de densités :
    $$ \mathcal{C}_c^\infty(\mathbb{R}^n) \text{ est dense dans } \mathcal{C}_c(\mathbb{R}^n) \text{ qui est dense dans } L^p(\mathbb{R}^n) $$
\end{itemize}
\begin{remark}
    C'est le résultat le plus important pour la théorie des EDP et des distributions. Il signifie que toute fonction $L^p$, aussi "sauvage" soit-elle, peut être approchée par une fonction infiniment régulière à support compact. C'est le "principe de densité".
\end{remark}

\section{Régularisation par Convolution}
\subsection{Approximation de l'Unité}
\begin{itemize}
    \item \textbf{Définition :} Une suite régularisante (ou approximation de l'unité) est une suite de fonctions $(\rho_n)$ positives, d'intégrale 1, et dont le support se concentre en 0.
    \item \textbf{Théorème de Régularisation :} Si $f \in L^p$, alors la suite des convolées $f*\rho_n$ converge vers $f$ en norme $L^p$. De plus, si les $\rho_n$ sont $\mathcal{C}^\infty$, alors les $f*\rho_n$ le sont aussi.
\end{itemize}
\begin{application}
    C'est la méthode constructive qui prouve la densité de $\mathcal{C}^\infty$ dans $L^p$.
\end{application}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de Weierstrass trigonométrique par le noyau de Fejér :} Un développement d'analyse très classique, élégant et qui montre une bonne maîtrise de l'analyse de Fourier.
        \item \textbf{Démonstration du théorème de Weierstrass polynomial par les polynômes de Bernstein :} Un développement qui connecte analyse et probabilités.
        \item \textbf{Densité de $\mathcal{C}_c^\infty(\Omega)$ dans $L^p(\Omega)$ par régularisation :} Un développement technique mais fondamental, qui montre une grande maturité en analyse.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Ne pas distinguer les modes de convergence :} C'est une leçon où la précision (convergence uniforme, $L^p$, etc.) est capitale.
        \item \textbf{Oublier la version trigonométrique de Weierstrass :} Elle est aussi importante que la version polynomiale.
        \item \textbf{Ne pas mentionner la convolution :} C'est l'outil moderne et le plus puissant pour l'approximation par des fonctions régulières.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 213 : Espaces de Hilbert. Bases hilbertiennes.}

\begin{philosophie}
    Les espaces de Hilbert sont le "paradis" de l'analyse fonctionnelle. Ils fusionnent la complétude analytique des espaces de Banach avec la richesse géométrique des espaces euclidiens (produit scalaire, orthogonalité). Cette leçon doit célébrer cette synthèse. L'objectif est de montrer comment l'intuition géométrique (projections, Pythagore) se généralise en dimension infinie, et comment le concept de "base hilbertienne" est la clé de voûte de l'analyse de Fourier et de la théorie du signal.
\end{philosophie}

\section{La Structure Géométrique des Espaces de Hilbert}

\subsection{Produit Scalaire et Orthogonalité}
\begin{itemize}
    \item \textbf{Définition :} Espace préhilbertien (muni d'un produit scalaire), espace de Hilbert (préhilbertien complet).
    \item \textbf{Identité du Parallélogramme :} Caractérise les normes qui dérivent d'un produit scalaire.
    \item \textbf{Théorème de Pythagore :} $\|x+y\|^2 = \|x\|^2 + \|y\|^2$ si $\langle x,y \rangle = 0$.
\end{itemize}

\subsection{Le Théorème de Projection}
\begin{itemize}
    \item \textbf{Théorème :} Dans un Hilbert, tout sous-espace vectoriel \textbf{complet} (donc fermé) $F$ admet un supplémentaire orthogonal unique, $F^\perp$. On a la décomposition en somme directe orthogonale $H = F \oplus F^\perp$.
    \item \textbf{Corollaire (Projection) :} Pour tout $x \in H$, il existe un unique $p_F(x) \in F$ qui est le point de $F$ le plus proche de $x$.
\end{itemize}

\subsection{Le Théorème de Représentation de Riesz}
\begin{itemize}
    \item \textbf{Théorème :} Tout espace de Hilbert s'identifie canoniquement à son dual topologique. Pour toute forme linéaire continue $\phi \in H^*$, il existe un unique $y \in H$ tel que $\phi(x) = \langle x, y \rangle$ pour tout $x$.
\end{itemize}
\begin{remark}
    Ce théorème est extraordinaire. Il nous dit qu'en géométrie hilbertienne, les "fonctions" (formes linéaires) sont en fait des "points" (vecteurs). C'est le prototype d'un espace réflexif.
\end{remark}

\section{Bases Hilbertiennes : L'Analyse de Fourier Généralisée}

\subsection{Définitions et Caractérisations}
\begin{itemize}
    \item \textbf{Définition (Famille orthonormale) :} Une famille $(e_i)_{i \in I}$ telle que $\langle e_i, e_j \rangle = \delta_{ij}$.
    \item \textbf{Inégalité de Bessel :} Pour toute famille orthonormale, $\sum |\langle x, e_i \rangle|^2 \le \|x\|^2$.
    \item \textbf{Définition (Base hilbertienne ou famille orthonormale totale) :} Une famille orthonormale qui est "maximale".
    \item \textbf{Théorème (Caractérisations équivalentes) :} Soit $(e_i)$ une famille orthonormale. Les assertions suivantes sont équivalentes :
        \begin{enumerate}
            \item C'est une base hilbertienne.
            \item Le sous-espace vectoriel engendré est dense dans $H$ ($\overline{\mathrm{Vect}(e_i)} = H$).
            \item \textbf{Identité de Parseval :} Pour tout $x \in H$, $\|x\|^2 = \sum |\langle x, e_i \rangle|^2$.
            \item Pour tout $x \in H$, $x = \sum \langle x, e_i \rangle e_i$.
            \item Le seul vecteur orthogonal à tous les $e_i$ est le vecteur nul.
        \end{enumerate}
\end{itemize}

\subsection{Existence et Isomorphismes}
\begin{itemize}
    \item \textbf{Théorème d'Existence :} Tout espace de Hilbert admet une base hilbertienne. Si l'espace est séparable, la base est dénombrable (le procédé de Gram-Schmidt à partir d'une suite dense le prouve).
    \item \textbf{Théorème d'Isomorphisme :} Tout espace de Hilbert séparable de dimension infinie est isométrique et isomorphe à l'espace de suites $\ell^2(\mathbb{N})$.
\end{itemize}
\begin{remark}
    Ce résultat est une classification spectaculaire : il n'y a, à isomorphisme près, qu'un seul "type" d'espace de Hilbert séparable de dimension infinie. Tous les autres (espaces de Sobolev, $L^2$, etc.) sont des "déguisements" de $\ell^2$.
\end{remark}

\section{Exemples et Applications Fondamentales}
\subsection{L'Espace $\ell^2(\mathbb{N})$}
\begin{itemize}
    \item La base canonique $(e_k)$ (la suite avec un 1 en k-ième position) est la base hilbertienne prototypique.
\end{itemize}
\subsection{L'Espace $L^2([0, 2\pi])$ et les Séries de Fourier}
\begin{itemize}
    \item La famille des exponentielles complexes $\{e^{int}\}_{n \in \mathbb{Z}}$ (normalisée) forme une base hilbertienne.
    \item L'identité de Parseval devient la célèbre relation entre l'énergie d'un signal et la somme des carrés de ses coefficients de Fourier. La décomposition sur la base est le développement en série de Fourier.
\end{itemize}
\subsection{Les Polynômes Orthogonaux}
\begin{itemize}
    \item En appliquant le procédé de Gram-Schmidt à la base canonique $\{1, x, x^2, \dots\}$ dans l'espace $L^2([-1,1], dx)$, on obtient les \textbf{polynômes de Legendre}. D'autres poids donnent d'autres familles célèbres (Hermite, Laguerre, Tchebychev).
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Existence d'une base hilbertienne dans un Hilbert séparable (par Gram-Schmidt) :} Un développement constructif et fondamental.
        \item \textbf{Inégalité de Bessel et Identité de Parseval :} Un développement au cœur de la théorie.
        \item \textbf{L'isomorphisme entre $\ell^2(\mathbb{N})$ et $L^2([0,2\pi])$ via les coefficients de Fourier :} Un développement qui montre une grande hauteur de vue.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre base hilbertienne (topologique) et base de Hamel (algébrique).} En dimension infinie, une base hilbertienne n'engendre l'espace que par adhérence.
        \item \textbf{Appliquer le théorème de projection sur un sous-espace non fermé.}
        \item \textbf{Ne pas connaître l'exemple des séries de Fourier.} C'est l'application historique et la plus importante de la théorie.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 214 : Théorème de projection sur un convexe fermé.}

\begin{philosophie}
    C'est une leçon focalisée sur un unique et puissant théorème. Le plan doit en explorer toutes les facettes : sa preuve, sa signification géométrique ("meilleure approximation"), et ses applications très larges qui vont de l'optimisation à la statistique, en passant par l'analyse de Fourier et les EDP. La leçon doit montrer comment un résultat de géométrie hilbertienne devient un outil de calcul et d'existence dans de nombreux domaines.
\end{philosophie}

\section{Le Théorème de Projection et sa Démonstration}
\subsection{Cadre : Géométrie des Espaces de Hilbert}
\begin{itemize}
    \item \textbf{Rappel :} Identité du parallélogramme, définition d'un convexe fermé.
\end{itemize}
\subsection{Le Théorème de la Meilleure Approximation Convexe}
\begin{itemize}
    \item \textbf{Théorème :} Soit $C$ un convexe fermé non vide d'un espace de Hilbert $H$. Pour tout $x \in H$, il existe un \textbf{unique} point $p_C(x) \in C$ tel que $\|x - p_C(x)\| = \inf_{y \in C} \|x - y\|$. Ce point est appelé la projection de $x$ sur $C$.
    \item \textbf{Démonstration :}
        \begin{itemize}
            \item \textbf{Existence :} On prend une suite minimisante $(y_n)$ dans $C$. On utilise l'identité du parallélogramme pour montrer qu'elle est de Cauchy. Par complétude de $H$ et fermeture de $C$, elle converge dans $C$.
            \item \textbf{Unicité :} On suppose deux solutions et on utilise à nouveau l'identité du parallélogramme et la convexité.
        \end{itemize}
\end{itemize}
\subsection{Caractérisation du Projeté}
\begin{itemize}
    \item \textbf{Théorème :} Soit $z = p_C(x)$. Le projeté est caractérisé par l'inégalité variationnelle :
    $$ \forall y \in C, \quad \mathrm{Re}\langle x-z, y-z \rangle \le 0 $$
    \item \textbf{Interprétation géométrique :} Le vecteur $x-z$ (la "direction de projection") fait un angle obtus avec tous les vecteurs "pointant vers l'intérieur" du convexe à partir du projeté $z$.
\end{itemize}

\section{Le Cas Particulier Fondamental : la Projection sur un Sous-Espace Vectoriel}
\subsection{Caractérisation et Linéarité}
\begin{itemize}
    \item \textbf{Caractérisation :} Si $F$ est un sous-espace vectoriel fermé, la caractérisation devient $x-p_F(x) \in F^\perp$. Le vecteur "erreur" est orthogonal au sous-espace.
    \item \textbf{Linéarité :} L'application de projection $p_F: H \to F$ est un endomorphisme linéaire, continu, de norme 1, et c'est un projecteur ($p_F \circ p_F = p_F$).
\end{itemize}
\subsection{Décomposition en Somme Directe Orthogonale}
\begin{itemize}
    \item \textbf{Théorème :} Si $F$ est un s.e.v. fermé, on a $H = F \oplus F^\perp$.
\end{itemize}
\subsection{Lien avec le Théorème de Riesz}
\begin{itemize}
    \item La preuve du théorème de représentation de Riesz utilise une projection. Pour une forme linéaire $\phi$, on projette sur son noyau $F = \ker \phi$ (qui est un s.e.v. fermé), ce qui nous donne le vecteur orthogonal qui représente $\phi$.
\end{itemize}

\section{Applications}
\subsection{Statistiques : Méthode des Moindres Carrés}
\begin{itemize}
    \item \textbf{Problème :} Trouver la meilleure approximation d'un vecteur de données $Y$ par un modèle linéaire, i.e., trouver $X\beta$ qui minimise $\|Y-X\beta\|^2$.
    \item \textbf{Solution :} C'est la projection orthogonale de $Y$ sur le sous-espace vectoriel engendré par les colonnes de $X$. La condition d'orthogonalité $Y - X\beta \in (\mathrm{Im} X)^\perp$ donne les équations normales $X^T(Y-X\beta)=0$.
\end{itemize}
\subsection{Analyse de Fourier}
\begin{itemize}
    \item \textbf{Coefficients de Fourier :} Les sommes partielles de la série de Fourier d'une fonction $f$ sont la projection orthogonale de $f$ sur le sous-espace des polynômes trigonométriques de degré au plus $N$. Le théorème de Parseval est une conséquence de Pythagore.
\end{itemize}
\subsection{Analyse Numérique : Méthode de Galerkin pour les EDP}
\begin{itemize}
    \item \textbf{Principe :} On cherche une solution approchée d'une EDP (via sa formulation faible) non pas dans l'espace de Hilbert de dimension infinie $H$, mais dans un sous-espace de dimension finie $V_n \subset H$. La meilleure solution est la projection de la solution exacte sur $V_n$. La condition d'orthogonalité est la condition de Galerkin. C'est la base de la méthode des éléments finis.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration du théorème de projection sur un convexe fermé :} Un grand classique, très formateur, qui montre une bonne maîtrise des outils hilbertiens.
        \item \textbf{Application aux moindres carrés et régression linéaire :} Un développement qui montre une application très concrète du théorème en statistiques.
        \item \textbf{Lien entre la projection et le théorème de Riesz :} Un développement plus théorique qui montre une vision unifiée de la géométrie hilbertienne.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier les hypothèses :} Complétude de l'espace, fermeture ET convexité de l'ensemble sont cruciales.
        \item \textbf{Ne pas savoir la caractérisation du projeté :} C'est ce qui est utilisé dans toutes les applications.
        \item \textbf{Confondre projection sur un convexe (non-linéaire en général) et sur un sous-espace vectoriel (linéaire).}
    \end{itemize}
\end{erreurs}
\chapter{Leçon 215 : Applications différentiables définies sur un ouvert de $\mathbb{R}^n$.}

\begin{philosophie}
    C'est la leçon sur le calcul différentiel en plusieurs variables. L'idée clé à transmettre est que la dérivée n'est plus un nombre, mais une application linéaire (la différentielle), qui représente la meilleure approximation locale de notre fonction. Le plan doit logiquement partir de cette définition, en déduire les grands théorèmes structurels (inversion locale, fonctions implicites), puis aboutir aux différentielles d'ordre supérieur et à l'optimisation.
\end{philosophie}

\section{La Différentielle : la Meilleure Approximation Linéaire Locale}
\subsection{Définitions de la Différentiabilité}
\begin{itemize}
    \item \textbf{Différentiabilité au sens de Fréchet :} $f(a+h) = f(a) + df(a)(h) + o(\|h\|)$, où $df(a)$ est une application linéaire.
    \item \textbf{Dérivées partielles et Matrice Jacobienne :} Lien entre la différentielle et les dérivées partielles.
    \item \textbf{Classe $\mathcal{C}^1$ :} $f$ est $\mathcal{C}^1$ si l'application $x \mapsto df(x)$ est continue. Le théorème fondamental est que si les dérivées partielles existent et sont continues, alors $f$ est de classe $\mathcal{C}^1$.
\end{itemize}

\subsection{Règles de Calcul}
\begin{itemize}
    \item \textbf{Règle de la chaîne (composition) :} $d(g \circ f)(a) = dg(f(a)) \circ df(a)$. Matriciellement, c'est le produit des jacobiennes.
\end{itemize}

\section{Théorèmes Fondamentaux de l'Analyse Locale}
\subsection{Inégalité des Accroissements Finis}
\begin{itemize}
    \item \textbf{Énoncé :} $\|f(b)-f(a)\| \le \sup_{t \in [0,1]} \|df(a+t(b-a))\| \cdot \|b-a\|$.
\end{itemize}
\subsection{Le Théorème d'Inversion Locale}
\begin{itemize}
    \item \textbf{Théorème :} Soit $f: U \to F$ de classe $\mathcal{C}^1$ (avec $E,F$ Banach, ici $\mathbb{R}^n$). Si $df(a)$ est un isomorphisme (i.e. le déterminant jacobien est non nul), alors $f$ est un difféomorphisme local de classe $\mathcal{C}^1$ au voisinage de $a$.
\end{itemize}
\subsection{Le Théorème des Fonctions Implicites}
\begin{itemize}
    \item \textbf{Énoncé :} Soit $f(x,y)$ une fonction $\mathcal{C}^1$. Si la différentielle partielle par rapport à $y$ est inversible en un point $(a,b)$ où $f(a,b)=c$, alors on peut localement exprimer $y$ comme une fonction de $x$ sur la ligne de niveau $f(x,y)=c$.
    \item \textbf{Application :} Définition des sous-variétés comme graphes locaux.
\end{itemize}

\section{Différentielles d'Ordre Supérieur et Optimisation}
\subsection{Définitions}
\begin{itemize}
    \item \textbf{Différentielle Seconde $d^2f(a)$ :} Une application bilinéaire symétrique.
    \item \textbf{Matrice Hessienne :} La matrice de $d^2f(a)$ dans la base canonique.
    \item \textbf{Théorème de Schwarz :} Si $f$ est $\mathcal{C}^2$, la Hessienne est symétrique.
\end{itemize}
\subsection{Formules de Taylor}
\begin{itemize}
    \item \textbf{Taylor-Young à l'ordre 2 :} $f(a+h) = f(a) + df(a)(h) + \frac{1}{2}d^2f(a)(h,h) + o(\|h\|^2)$.
    \item \textbf{Taylor-Lagrange (reste intégral).}
\end{itemize}
\subsection{Application à la Recherche d'Extrema}
\begin{itemize}
    \item \textbf{Condition nécessaire du premier ordre :} $\nabla f(a) = 0$.
    \item \textbf{Condition du second ordre :} La signature de la matrice Hessienne en un point critique détermine la nature de l'extremum (minimum si définie positive, maximum si définie négative, point selle sinon).
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème d'inversion locale :} La preuve est un grand classique, basée sur le théorème du point fixe de Banach dans un bon espace.
        \item \textbf{Le lemme de Morse :} Un résultat fin qui montre que localement, une fonction se comporte comme sa partie quadratique.
        \item \textbf{Conditions du second ordre pour les extrema locaux :} Un développement qui connecte la leçon à la réduction des formes quadratiques.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre différentiable et $\mathcal{C}^1$.} L'existence des dérivées partielles ne suffit pas à la différentiabilité.
        \item \textbf{Mal énoncer le théorème d'inversion locale :} L'hypothèse $\mathcal{C}^1$ est cruciale.
        \item \textbf{Ne pas voir la différentielle seconde comme une forme bilinéaire symétrique.}
        \item \textbf{Appliquer l'égalité des accroissements finis} (qui est fausse en général).
    \end{itemize}
\end{erreurs}
\chapter{Leçon 217 : Sous-variétés de $\mathbb{R}^n$.}

\begin{philosophie}
    Cette leçon formalise la notion intuitive d'objet "courbe" (un cercle, une sphère, une surface) vivant dans l'espace euclidien. La clé est de comprendre qu'un tel objet est "localement" un espace plat $\mathbb{R}^p$. Le plan doit donc s'articuler autour des différentes manières (équivalentes) de définir rigoureusement cette idée de "ressembler localement à $\mathbb{R}^p$". L'espace tangent est alors l'outil fondamental qui permet de faire du calcul différentiel sur ces objets courbes.
\end{philosophie}

\section{Définitions Équivalentes d'une Sous-Variété}
\subsection{Le Point de Vue "Graphe Local"}
\begin{itemize}
    \item \textbf{Définition :} Une partie $M \subset \mathbb{R}^n$ est une sous-variété de dimension $p$ si, localement, elle est le graphe d'une fonction $\mathcal{C}^k$ de $\mathbb{R}^p$ dans $\mathbb{R}^{n-p}$ (après un changement de coordonnées).
\end{itemize}

\subsection{Le Point de Vue "Lieu d'Annulation" (Submersion)}
\begin{itemize}
    \item \textbf{Définition :} $M$ est une sous-variété de dimension $p$ si, localement, elle est l'ensemble de niveau d'une submersion, i.e., $M = f^{-1}(0)$ où $f: U \to \mathbb{R}^{n-p}$ est une application $\mathcal{C}^k$ dont la différentielle est surjective en tout point de $M$.
\end{itemize}
\begin{remark}
    C'est la définition la plus pratique pour montrer qu'un ensemble est une sous-variété.
\end{remark}

\subsection{Le Point de Vue "Paramétrisation Locale" (Immersion)}
\begin{itemize}
    \item \textbf{Définition :} $M$ est une sous-variété de dimension $p$ si, localement, elle est l'image d'une immersion, i.e., une application $\mathcal{C}^k$ injective $\phi: V \subset \mathbb{R}^p \to \mathbb{R}^n$ dont la différentielle est injective en tout point.
\end{itemize}
\subsection{Théorème d'Équivalence}
\begin{itemize}
    \item Ces trois définitions sont équivalentes. Le passage de l'une à l'autre se fait via le théorème d'inversion locale ou des fonctions implicites.
\end{itemize}

\section{Espace Tangent et Géométrie Locale}
\subsection{Définition de l'Espace Tangent}
\begin{itemize}
    \item \textbf{Définition "cinématique" :} L'espace tangent $T_x M$ en $x \in M$ est l'ensemble des vecteurs vitesse $\gamma'(0)$ des courbes $\gamma: ]-\epsilon, \epsilon[ \to M$ tracées sur la variété et passant par $x$.
    \item \textbf{Caractérisation pratique :} Si $M=f^{-1}(0)$, alors $T_x M = \ker(df(x))$. C'est un sous-espace vectoriel de dimension $p$.
    \item \textbf{Espace Normal :} C'est le supplémentaire orthogonal de l'espace tangent, $N_x M = (T_x M)^\perp$.
\end{itemize}

\section{Exemples et Applications}
\subsection{Exemples Fondamentaux}
\begin{itemize}
    \item \textbf{La sphère $\mathbb{S}^{n-1}$ :} C'est l'ensemble de niveau $f^{-1}(1)$ pour $f(x)=\|x\|^2$. C'est une sous-variété de dimension $n-1$.
    \item \textbf{Le groupe orthogonal $O(n)$ :} C'est l'ensemble de niveau $f^{-1}(I_n)$ pour $f(A)=A^T A$. C'est une sous-variété de dimension $n(n-1)/2$.
    \item Les graphes de fonctions $\mathcal{C}^k$ sont des sous-variétés.
\end{itemize}
\subsection{Application à l'Optimisation sous Contraintes : Extrema Liés}
\begin{itemize}
    \item \textbf{Problème :} Trouver les extrema d'une fonction $F$ restreinte à une sous-variété $M = \{x \mid g(x)=0\}$.
    \item \textbf{Théorème des Extrema Liés (Multiplicateurs de Lagrange) :} Si $a$ est un extremum local de $F|_M$, alors le gradient de $F$ en $a$ est orthogonal à l'espace tangent de $M$ en $a$. Cela se traduit par l'existence de multiplicateurs de Lagrange $\lambda$ tels que $\nabla F(a) = \lambda \nabla g(a)$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Le groupe orthogonal $O(n)$ est une sous-variété de $\mathcal{M}_n(\mathbb{R})$ :} Un développement classique qui utilise la définition par les submersions.
        \item \textbf{Théorème des extrema liés (Multiplicateurs de Lagrange) :} Un développement majeur qui est l'application la plus importante de la notion d'espace tangent.
        \item \textbf{Calcul de l'espace tangent à un groupe de matrices (e.g. $SL_n(\mathbb{R})$) :} Un bel exercice d'application des définitions.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les différentes définitions :} Il faut savoir naviguer entre elles.
        \item \textbf{Ne pas savoir calculer l'espace tangent d'un exemple simple} comme la sphère.
        \item \textbf{Mal énoncer le théorème des extrema liés :} Oublier la condition de régularité sur la contrainte (le fait que $g$ soit une submersion).
        \item \textbf{Confondre immersion et plongement.} Un plongement est une immersion qui est aussi un homéomorphisme sur son image.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 219 : Extrema : existence, caractérisation, recherche.}

\begin{philosophie}
    C'est une leçon de synthèse au cœur de l'optimisation. Le plan doit suivre scrupuleusement le triptyque du titre, qui est une feuille de route parfaite pour la résolution d'un problème d'optimisation. Il faut montrer que l'on maîtrise à la fois les outils théoriques d'existence (compacité, convexité), les outils de caractérisation (calcul différentiel) et les outils algorithmiques de recherche (méthodes de descente).
\end{philosophie}

\section{Existence d'Extrema}
\subsection{Le Cadre de la Compacité}
\begin{itemize}
    \item \textbf{Théorème Fondamental :} Toute fonction continue sur un espace compact non vide à valeurs dans $\mathbb{R}$ est bornée et atteint ses bornes.
    \item \textbf{Application en dimension finie :} En dimension finie, les compacts sont les fermés bornés.
\end{itemize}
\subsection{Le Cadre de la Dimension Infinie : Coercivité}
\begin{itemize}
    \item \textbf{Motivation :} En dimension infinie, les boules fermées ne sont pas compactes. On a besoin d'une autre condition.
    \item \textbf{Définition (Fonction coercive) :} Une fonction $f: E \to \mathbb{R}$ est coercive si $f(x) \to +\infty$ quand $\|x\| \to \infty$.
    \item \textbf{Théorème d'existence :} Toute fonction continue, coercive et semi-continue inférieurement sur un espace de Hilbert admet un minimum global. La convexité est une condition qui assure la semi-continuité inférieure.
\end{itemize}

\section{Caractérisation des Extrema (Optimisation sans Contraintes)}
\subsection{Conditions du Premier Ordre}
\begin{itemize}
    \item \textbf{Théorème (Condition Nécessaire) :} Si $f$ est différentiable et admet un extremum local en $a$ (point intérieur), alors $df(a)=0$ (le gradient est nul).
\end{itemize}
\subsection{Conditions du Second Ordre}
\begin{itemize}
    \item \textbf{Théorème (Condition Nécessaire) :} Si $f$ est deux fois différentiable et admet un minimum local en $a$, sa Hessienne $H_f(a)$ est une forme quadratique positive.
    \item \textbf{Théorème (Condition Suffisante) :} Si $df(a)=0$ et si la Hessienne $H_f(a)$ est définie positive, alors $a$ est un minimum local strict.
\end{itemize}

\section{Caractérisation des Extrema (Optimisation sous Contraintes)}
\subsection{Contraintes d'Égalité : Multiplicateurs de Lagrange}
\begin{itemize}
    \item \textbf{Problème :} Optimiser $f(x)$ sous la contrainte $g(x)=0$, où $g$ définit une sous-variété.
    \item \textbf{Théorème des Extrema Liés :} Si $a$ est un extremum local sous la contrainte et que $a$ est un point régulier de la contrainte, alors il existe un multiplicateur de Lagrange $\lambda$ tel que $\nabla f(a) = \lambda \nabla g(a)$.
    \item \textbf{Interprétation Géométrique :} En un point d'extremum, les lignes de niveau de la fonction sont tangentes à la contrainte.
\end{itemize}
\subsection{Contraintes d'Inégalité : Conditions de Karush-Kuhn-Tucker (KKT)}
\begin{itemize}
    \item \textbf{Problème :} Optimiser $f(x)$ sous les contraintes $g_i(x) \le 0$.
    \item \textbf{Conditions KKT :} Généralisent la condition de Lagrange, en introduisant des conditions de "complémentarité" qui distinguent les contraintes actives (saturées) des contraintes inactives.
\end{itemize}

\section{Méthodes Numériques de Recherche d'Extrema}
\subsection{Méthodes de Descente}
\begin{itemize}
    \item \textbf{Principe :} Construire une suite $x_{k+1} = x_k + \alpha_k d_k$ où $d_k$ est une direction de descente.
    \item \textbf{Méthode du Gradient à pas optimal :} $d_k = -\nabla f(x_k)$. Simple mais potentiellement lente.
    \item \textbf{Méthode de Newton :} $x_{k+1} = x_k - [H_f(x_k)]^{-1} \nabla f(x_k)$. Convergence quadratique mais coûteuse.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème des extrema liés (Multiplicateurs de Lagrange) :} Un développement majeur qui connecte le calcul différentiel et la géométrie des sous-variétés.
        \item \textbf{Étude d'un exemple d'optimisation :} Trouver le rectangle d'aire maximale pour un périmètre donné, ou l'inégalité arithmético-géométrique comme un problème d'optimisation sous contrainte.
        \item \textbf{Lemme de Morse :} Un résultat fin qui montre que localement, une fonction se comporte comme sa partie quadratique.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier la partie "existence".} La caractérisation n'a de sens que si l'on sait qu'un extremum existe.
        \item \textbf{Mal énoncer le théorème des extrema liés :} Oublier la condition de régularité sur la contrainte (le fait que $g$ soit une submersion).
        \item \textbf{Ne pas savoir interpréter géométriquement les multiplicateurs de Lagrange.}
        \item \textbf{Penser qu'une condition nécessaire est suffisante.}
    \end{itemize}
\end{erreurs}

\part{Leçons sur les Équations Différentielles, Suites et Fonctions}

\chapter{Leçon 220 : Équations différentielles $X'=f(t,X)$.}

\begin{philosophie}
    C'est la leçon qui pose les fondations théoriques de l'étude des EDO. L'objectif est de montrer que l'on a compris le dogme du déterminisme en physique : sous de "bonnes" conditions, l'état initial d'un système détermine entièrement son futur. Le plan doit s'articuler autour du théorème central de Cauchy-Lipschitz, en explorant sa portée (existence et unicité locale), ses limites (explosion en temps fini, non-unicité sans l'hypothèse de Lipschitz) et ses conséquences (dépendance continue des solutions par rapport aux données).
\end{philosophie}

\section{Le Théorème de Cauchy-Lipschitz : Existence et Unicité Locale}
\subsection{Position du Problème}
\begin{itemize}
    \item \textbf{Cadre :} On cherche une solution à l'équation $X'=f(t,X)$ vérifiant la condition initiale $X(t_0)=X_0$, où $f$ est une fonction d'un ouvert $U \subset \mathbb{R} \times E$ dans un espace de Banach $E$.
    \item \textbf{Définition (Fonction localement lipschitzienne) :} $f$ est localement lipschitzienne par rapport à sa seconde variable si pour tout point de $U$, il existe un voisinage où $f$ est lipschitzienne.
\end{itemize}

\subsection{Le Théorème Fondamental}
\begin{itemize}
    \item \textbf{Théorème de Cauchy-Lipschitz :} Si $f$ est continue et localement lipschitzienne en $X$, alors il existe une solution \textbf{locale unique} au problème de Cauchy.
    \item \textbf{Idée de la preuve (Méthode de Picard) :} On réécrit le problème différentiel comme un problème de point fixe pour l'opérateur intégral $T(u)(t) = X_0 + \int_{t_0}^t f(s, u(s))ds$. Sur un bon espace de fonctions (un Banach), on montre que cet opérateur est une contraction. Le théorème du point fixe de Banach garantit alors l'existence et l'unicité.
\end{itemize}

\subsection{Le Rôle de l'Hypothèse de Lipschitz}
\begin{itemize}
    \item \textbf{Théorème d'existence de Peano-Arzelà :} Si $f$ est seulement continue (et $E$ de dimension finie), il y a \textbf{existence} d'au moins une solution locale, mais pas nécessairement unicité.
    \item \textbf{Contre-exemple classique :} Le problème $y' = \sqrt{|y|}$ avec $y(0)=0$ admet une infinité de solutions (la solution nulle et les solutions recollées).
\end{itemize}

\section{Solutions Maximales et Comportement Global}
\subsection{Prolongement des Solutions}
\begin{itemize}
    \item \textbf{Lemme de Zorn :} Permet de montrer que toute solution peut être prolongée en une solution maximale.
    \item \textbf{Définition :} Une solution $(J, \phi)$ est maximale si elle n'admet aucun prolongement strict. L'intervalle de définition $J$ est ouvert.
\end{itemize}
\subsection{Théorème de Sortie des Compacts}
\begin{itemize}
    \item \textbf{Théorème :} Soit $(J, \phi)$ une solution maximale avec $J=]a,b[$. Si $b < \infty$, alors la solution "explose" : pour tout compact $K \subset U$, le graphe de $\phi$ finit par sortir de $K$.
\end{itemize}
\subsection{Conditions d'Existence Globale}
\begin{itemize}
    \item \textbf{Corollaire :} Si $f$ est globalement lipschitzienne et définie sur $\mathbb{R} \times E$, les solutions sont globales. C'est le cas des équations différentielles linéaires.
    \item \textbf{Corollaire :} Si la solution reste confinée dans un compact, elle est globale.
\end{itemize}

\section{Dépendance par Rapport aux Données}
\subsection{Le Lemme de Grönwall}
\begin{itemize}
    \item \textbf{Énoncé (forme intégrale) :} C'est l'outil technique fondamental pour obtenir des majorations sur les solutions.
\end{itemize}
\subsection{Continuité par Rapport aux Conditions Initiales}
\begin{itemize}
    \item \textbf{Théorème :} La solution dépend continûment de sa condition initiale sur tout intervalle compact de son domaine de définition.
\end{itemize}
\subsection{Différentiabilité par Rapport aux Conditions Initiales}
\begin{itemize}
    \item \textbf{Théorème :} Si $f$ est de classe $\mathcal{C}^k$, la solution est de classe $\mathcal{C}^k$ par rapport à sa condition initiale. L'écart entre deux solutions proches est gouverné en premier ordre par une EDO linéaire (l'équation aux variations).
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration du théorème de Cauchy-Lipschitz par le point fixe de Picard :} Un grand classique qui connecte les EDO à l'analyse fonctionnelle.
        \item \textbf{Lemme de Grönwall et application à l'unicité :} Un développement technique mais très formateur.
        \item \textbf{Théorème de sortie des compacts :} Un résultat fin sur le comportement global des solutions.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre existence locale et globale :} Cauchy-Lipschitz ne garantit que l'existence locale.
        \item \textbf{Mal énoncer les hypothèses :} La continuité seule ne suffit pas pour l'unicité.
        \item \textbf{Ne pas connaître le contre-exemple $y'=\sqrt{|y|}$} : il est essentiel pour illustrer l'importance de la condition de Lipschitz.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 221 : Équations différentielles linéaires.}

\begin{philosophie}
    C'est la leçon "reine" sur les EDO. L'objectif est de montrer la puissance de l'algèbre linéaire pour comprendre la structure globale des solutions. Le plan doit mettre en évidence cette dualité constante : l'analyse garantit l'existence et l'unicité (qui deviennent globales dans le cas linéaire), tandis que l'algèbre linéaire décrit la structure de l'espace des solutions et prédit le comportement asymptotique.
\end{philosophie}

\section{Le Cadre Analytique : Existence Globale et Structure}
\subsection{Théorie Générale}
\begin{itemize}
    \item \textbf{Cadre :} $Y'=A(t)Y+B(t)$ sur $I \times E$ où $E$ est un Banach.
    \item \textbf{Théorème de Cauchy-Lipschitz Linéaire :} L'application $f(t,Y)=A(t)Y+B(t)$ est globalement lipschitzienne en $Y$ (si $A$ est continue).
    \item \textbf{Conséquence Fondamentale (Solutions Globales) :} Les solutions maximales sont définies sur l'intervalle entier $I$ de continuité de $A$ et $B$. Il n'y a jamais d'explosion en temps fini pour une EDO linéaire.
\end{itemize}
\subsection{Structure de l'Espace des Solutions}
\begin{itemize}
    \item \textbf{Superposition des Solutions :} L'ensemble des solutions de l'équation homogène $Y'=A(t)Y$ est un $K$-espace vectoriel.
    \item \textbf{Théorème :} Cet espace est de dimension $n = \dim(E)$.
    \item \textbf{Structure Affine :} L'ensemble des solutions de l'équation complète est un espace affine $Y_p + \ker(L)$, où $L(y) = y' - A(t)y$.
    \item \textbf{Wronskien :} Le wronskien d'une famille de $n$ solutions caractérise si elles forment une base de l'espace des solutions.
\end{itemize}

\section{Le Cas Fondamental : Coefficients Constants $Y'=AY$}
\subsection{L'Exponentielle de Matrice : L'Outil Central}
\begin{itemize}
    \item \textbf{Définition et Propriétés :} $e^{tA}$ comme somme de série. C'est l'unique solution de $X'=AX, X(0)=I_n$.
    \item \textbf{Solution du Problème de Cauchy :} $Y(t) = e^{tA}Y_0$.
\end{itemize}
\subsection{Calcul de l'Exponentielle par la Réduction}
\begin{itemize}
    \item \textbf{Cas Diagonalisable :} $e^{tA}=Pe^{tD}P^{-1}$.
    \item \textbf{Cas Général (Dunford) :} $A=D+N$, $e^{tA}=e^{tD}e^{tN}$. Le calcul de $e^{tN}$ est un polynôme en $t$.
\end{itemize}
\subsection{Interprétation Géométrique : Portraits de Phase}
\begin{itemize}
    \item Le comportement qualitatif des solutions est entièrement dicté par le spectre de $A$.
    \item Classification des portraits de phase en dimension 2 (nœuds, cols, foyers, centres).
\end{itemize}

\section{Stabilité et Comportement Asymptotique}
\subsection{Stabilité des Solutions d'Équilibre}
\begin{itemize}
    \item \textbf{Théorème :} Pour $Y'=AY$, l'équilibre 0 est asymptotiquement stable si et seulement si toutes les valeurs propres de $A$ ont une partie réelle strictement négative.
\end{itemize}
\subsection{Équations d'Ordre Supérieur}
\begin{itemize}
    \item Une équation scalaire d'ordre $n$ se ramène à un système linéaire de taille $n$ via la matrice compagnon.
    \item Les racines du polynôme caractéristique de l'équation gouvernent la stabilité.
\end{itemize}
\subsection{Solutions Périodiques et Résonance}
\begin{itemize}
    \item \textbf{Théorème :} Si $A$ n'a pas de valeur propre de la forme $ik\omega$ et que $B(t)$ est $T$-périodique ($T=2\pi/\omega$), il existe une unique solution $T$-périodique.
    \item \textbf{Phénomène de résonance :} Si une fréquence propre du système est excitée par le second membre, l'amplitude des solutions peut croître indéfiniment.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Calcul de l'exponentielle d'une matrice par Dunford et résolution d'un système (Couteau-suisse) :} Un bijou qui connecte la réduction et les EDO.
        \item \textbf{Étude de l'oscillateur harmonique forcé et discussion de la résonance :} Un exemple physique très parlant.
        \item \textbf{Stabilité de l'équilibre pour un système linéaire :} Preuve basée sur la réduction de Jordan.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Ne pas mentionner que les solutions sont globales :} C'est la différence majeure avec le cas non-linéaire.
        \item \textbf{Ne pas faire le lien entre la stabilité et la partie réelle des valeurs propres.}
        \item \textbf{Confondre la résolution de $y'=ay$ (primitive) et $Y'=AY$ (exponentielle). L'un est un calcul, l'autre est une théorie.}
    \end{itemize}
\end{erreurs}
\chapter{Leçon 222 : Exemples d'études qualitatives d'équations différentielles.}

\begin{philosophie}
    Le titre est "Exemples...". Cette leçon est la géométrie des EDO. L'objectif n'est pas de résoudre explicitement des équations, mais de comprendre le comportement global des solutions en étudiant le champ de vecteurs. C'est l'art de dessiner le "portrait de phase". La leçon doit être une galerie d'exemples illustrant les outils de l'étude qualitative : stabilité des équilibres, fonctions de Liapounov, orbites périodiques.
\end{philosophie}

\section{Stabilité des Points d'Équilibre}
\subsection{Systèmes Autonomes et Linéarisation}
\begin{itemize}
    \item \textbf{Définition :} Système autonome $X'=f(X)$, point d'équilibre $f(X_0)=0$.
    \item \textbf{Stabilité au sens de Liapounov, Stabilité Asymptotique.}
    \item \textbf{Théorème de Stabilité par Linéarisation :} La stabilité de l'équilibre $X_0$ est gouvernée par le spectre de la matrice jacobienne $J_f(X_0)$. Si toutes les valeurs propres sont à partie réelle $<0$, il est asymptotiquement stable. Si au moins une est à partie réelle $>0$, il est instable.
    \item \textbf{Le cas critique :} Si des valeurs propres sont à partie réelle nulle, le linéarisé ne permet pas de conclure.
\end{itemize}
\subsection{Fonctions de Liapounov : une Approche Énergétique}
\begin{itemize}
    \item \textbf{Principe :} Trouver une fonction "énergie" $V(X)$ qui décroît le long des trajectoires.
    \item \textbf{Théorème :} S'il existe une fonction de Liapounov (définie positive, à dérivée le long des trajectoires négative), alors l'équilibre est stable. S'il est de plus strictement négative, il est asymptotiquement stable.
    \item \textbf{Exemple :} L'énergie mécanique pour un système conservatif (pendule non amorti).
\end{itemize}

\section{Portraits de Phase en Dimension 2}
\subsection{Classification des Équilibres Linéaires}
\begin{itemize}
    \item Rappel de la classification (nœuds, cols, foyers, centres) en fonction des valeurs propres.
\end{itemize}
\subsection{Le Théorème de Hartman-Grobman}
\begin{itemize}
    \item \textbf{Théorème :} Au voisinage d'un point d'équilibre hyperbolique (pas de valeur propre à partie réelle nulle), le système non-linéaire est topologiquement équivalent à son linéarisé.
\end{itemize}
\subsection{Exemple : Le Pendule Simple}
\begin{itemize}
    \item Équation $\theta'' + \sin\theta = 0$.
    \item Équilibre stable en $(0,0)$ (un centre), instable en $(\pi,0)$ (un col).
    \item Le portrait de phase montre les orbites périodiques et les lignes de séparatrices.
\end{itemize}

\section{Comportement Périodique et Limite}
\subsection{Le Théorème de Poincaré-Bendixson}
\begin{itemize}
    \item \textbf{Théorème :} Dans le plan, toute trajectoire qui reste dans un compact et ne tend pas vers un point d'équilibre doit s'enrouler sur une orbite périodique.
\end{itemize}
\begin{remark}
    C'est un résultat d'existence très puissant pour les orbites périodiques, mais il est spécifique à la dimension 2. En dimension 3 et plus, des comportements chaotiques peuvent apparaître.
\end{remark}
\subsection{Exemples Classiques de la Modélisation}
\begin{itemize}
    \item \textbf{Modèle de Lotka-Volterra (proies-prédateurs) :} Présente des orbites périodiques, illustrant des cycles écologiques.
    \item \textbf{Oscillateur de Van der Pol :} Un modèle avec un cycle limite, c'est-à-dire une orbite périodique isolée qui attire toutes les trajectoires voisines.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Étude complète du portrait de phase du pendule simple :} Un grand classique qui permet de montrer sa maîtrise de la linéarisation et de l'analyse qualitative.
        \item \textbf{Utilisation d'une fonction de Liapounov pour prouver la stabilité d'un équilibre :} Par exemple pour le pendule amorti.
        \item \textbf{Le modèle de Lotka-Volterra :} Étude des points d'équilibre et preuve de l'existence d'un invariant (quantité conservée) qui montre que les orbites sont fermées.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Essayer de résoudre les équations explicitement :} Le titre est "étude qualitative".
        \item \textbf{Confondre stabilité et stabilité asymptotique :} Un centre est stable, mais pas asymptotiquement stable.
        \item \textbf{Croire que la linéarisation permet toujours de conclure :} Le cas des centres est le contre-exemple fondamental.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 223 : Suites numériques. Convergence, valeurs d'adhérence.}

\begin{philosophie}
    C'est une leçon sur les fondements de l'analyse réelle. Loin d'être élémentaire, elle exige une maîtrise parfaite de la structure topologique de $\mathbb{R}$. Le plan doit progresser logiquement : de la notion de base de convergence à l'étude fine des oscillations (valeurs d'adhérence), pour aboutir à l'étude des types de suites les plus importants (monotones, adjacentes, récurrentes).
\end{philosophie}

\section{Convergence et Structure de $\mathbb{R}$}
\subsection{Définitions et Propriétés}
\begin{itemize}
    \item \textbf{Définition de la limite.} Unicité dans $\mathbb{R}$ (qui est séparé).
    \item \textbf{Propriétés :} Opérations sur les limites, passage à la limite dans les inégalités.
\end{itemize}
\subsection{Le Critère de Cauchy}
\begin{itemize}
    \item \textbf{Définition (Suite de Cauchy) :} Une notion de "convergence intrinsèque".
    \item \textbf{Théorème :} Une suite réelle converge si et seulement si elle est de Cauchy. C'est la propriété de \textbf{complétude} de $\mathbb{R}$.
\end{itemize}
\subsection{Suites Monotones}
\begin{itemize}
    \item \textbf{Théorème de la limite monotone :} Toute suite monotone et bornée converge. C'est une autre formulation de la complétude (propriété de la borne supérieure).
    \item \textbf{Exemple :} La suite $(1+1/n)^n$ est croissante et majorée, donc elle converge vers $e$.
    \item \textbf{Théorème des suites adjacentes :} Un outil puissant pour encadrer une limite.
\end{itemize}

\section{Valeurs d'Adhérence et Théorème de Bolzano-Weierstrass}
\subsection{Définition et Caractérisation}
\begin{itemize}
    \item \textbf{Définition :} Une valeur d'adhérence est la limite d'une sous-suite.
    \item \textbf{L'ensemble des valeurs d'adhérence $A$ :} C'est un fermé de $\mathbb{R} \cup \{-\infty, +\infty\}$.
    \item \textbf{Caractérisation de la convergence :} Une suite converge si et seulement si elle est bornée et admet une unique valeur d'adhérence.
\end{itemize}
\subsection{Limite Supérieure et Inférieure}
\begin{itemize}
    \item \textbf{Définition :} $\limsup u_n = \sup A$ et $\liminf u_n = \inf A$.
    \item \textbf{Propriétés :} Une suite converge si et seulement si $\limsup u_n = \liminf u_n$.
\end{itemize}
\subsection{Le Théorème Fondamental}
\begin{itemize}
    \item \textbf{Théorème de Bolzano-Weierstrass :} De toute suite réelle bornée, on peut extraire une sous-suite convergente. C'est la propriété de compacité séquentielle des segments de $\mathbb{R}$.
\end{itemize}

\section{Exemples et Applications}
\subsection{Suites Arithmético-Géométriques}
\begin{itemize}
    \item Étude de la convergence en fonction de la raison.
\end{itemize}
\subsection{Suites Récurrentes $u_{n+1}=f(u_n)$}
\begin{itemize}
    \item \textbf{Méthode d'étude :} Stabilité des intervalles, monotonie, points fixes.
    \item \textbf{Théorème du point fixe :} Si $f$ est contractante sur un intervalle fermé stable, la suite converge vers l'unique point fixe.
\end{itemize}
\subsection{Moyenne de Césaro}
\begin{itemize}
    \item \textbf{Théorème :} Si une suite $(u_n)$ converge vers $\ell$, alors la suite de ses moyennes $(v_n = \frac{1}{n}\sum u_k)$ converge aussi vers $\ell$.
    \item \textbf{La réciproque est fausse :} $u_n = (-1)^n$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de Césaro :} Un développement d'analyse classique.
        \item \textbf{Étude de la suite $u_{n+1} = \sin(u_n)$ ou $u_{n+1} = u_n - u_n^2$ :} Un bon exemple d'étude de suite récurrente avec recherche d'un équivalent.
        \item \textbf{Théorème de Bolzano-Weierstrass :} La preuve par dichotomie est très formatrice.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre limite et valeur d'adhérence.}
        \item \textbf{Penser qu'une suite bornée converge.} C'est le piège classique.
        \item \textbf{Mal étudier une suite récurrente :} Il faut absolument commencer par justifier l'existence et la stabilité d'un intervalle.
        \item \textbf{Croire que la réciproque du théorème de Césaro est vraie.}
    \end{itemize}
\end{erreurs}
\chapter{Leçon 224 : Exemples de développements asymptotiques de suites et de fonctions.}

\begin{philosophie}
    Cette leçon teste la virtuosité technique en analyse. Le but n'est plus seulement de savoir si une suite ou une fonction converge, mais de décrire \textit{comment} elle converge, avec quelle vitesse, et quel est le "terme correctif" suivant. C'est l'art de la comparaison et de l'approximation. Le plan doit s'organiser par type d'objet (suites, fonctions) et par type de méthode (Taylor, intégrales, etc.).
\end{philosophie}

\section{Développements Asymptotiques de Suites}
\subsection{Méthodes Basées sur les Développements Limités}
\begin{itemize}
    \item \textbf{Suites récurrentes $u_{n+1}=f(u_n)$ convergeant vers un point fixe $L$ :}
        \begin{itemize}
            \item Principe : On pose $v_n = u_n - L$ et on utilise un développement de Taylor de $f$ en $L$.
            \item Exemple : $u_{n+1} = \sin(u_n)$. On trouve $u_n \sim \sqrt{3/n}$.
        \end{itemize}
    \item \textbf{Suites définies implicitement :}
        \begin{itemize}
            \item Exemple : $x_n = \tan(x_n)$ sur $]n\pi - \pi/2, n\pi + \pi/2[$. On pose $x_n = n\pi + \pi/2 - \epsilon_n$ et on injecte.
        \end{itemize}
\end{itemize}
\subsection{Méthodes Basées sur la Comparaison Série-Intégrale}
\begin{itemize}
    \item \textbf{Principe :} Pour une suite $u_n = f(n)$, on peut souvent obtenir un développement de la somme $S_N = \sum_{n=1}^N u_n$ en comparant avec $\int_1^N f(t)dt$.
    \item \textbf{Application : Série Harmonique :} $\sum_{k=1}^n \frac{1}{k} = \ln(n) + \gamma + \frac{1}{2n} + o(\frac{1}{n})$.
    \item \textbf{Application : Formule de Stirling :} $\ln(n!) = \sum \ln k \approx \int \ln t dt$. On obtient $n! \sim \sqrt{2\pi n} (n/e)^n$.
\end{itemize}
\subsection{Autres Méthodes}
\begin{itemize}
    \item \textbf{Lemme de Césaro :} Permet parfois de trouver des équivalents.
\end{itemize}

\section{Développements Asymptotiques de Fonctions}
\subsection{Outils de Base : Les Formules de Taylor}
\begin{itemize}
    \item \textbf{Taylor-Young (local) :} $f(x) = \sum_{k=0}^n \frac{f^{(k)}(a)}{k!}(x-a)^k + o((x-a)^n)$.
    \item \textbf{Taylor-Lagrange et Taylor avec reste intégral (global).}
\end{itemize}
\subsection{Développements Asymptotiques aux Bornes}
\begin{itemize}
    \item \textbf{Principe :} Obtenus par des changements de variables qui ramènent au voisinage de 0.
    \item \textbf{Exemple :} Développement de $(1+1/x)^\alpha$ en $+\infty$.
\end{itemize}
\subsection{Intégrales à Paramètres}
\begin{itemize}
    \item \textbf{Lemme de Laplace :} Donne un équivalent d'intégrales du type $\int_a^b e^{t\phi(x)}f(x)dx$ quand $t \to \infty$. L'équivalent est dominé par le comportement au voisinage du maximum de $\phi$.
    \item \textbf{Intégration par parties itérée :} Permet d'obtenir des développements asymptotiques de transformées de Fourier ou de Laplace.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Formule de Stirling (par comparaison série-intégrale et intégrales de Wallis) :} Un grand classique, très complet.
        \item \textbf{Développement asymptotique d'une suite récurrente ($u_{n+1}=\sin u_n$) :} Un excellent exemple qui montre la maîtrise des développements limités.
        \item \textbf{Lemme de Laplace :} Un développement de plus haut niveau, qui montre une grande culture en analyse.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Intégrer ou sommer des équivalents ou des $o(\cdot)$ :} C'est l'erreur la plus grave et la plus fréquente. Il faut revenir aux définitions ou utiliser des théorèmes spécifiques.
        \item \textbf{Faire des erreurs dans les développements limités usuels.}
        \item \textbf{Ne pas justifier les hypothèses des théorèmes (e.g., monotonie pour la comparaison série-intégrale).}
    \end{itemize}
\end{erreurs}
\chapter{Leçon 226 : Suites vectorielles et matricielles.}

\begin{philosophie}
    Cette leçon applique les outils de l'algèbre linéaire (normes, réduction) à l'étude de l'analyse (convergence de suites). Le résultat central est que, en dimension finie, toutes les normes sont équivalentes, ce qui rend la topologie unique et simple. La leçon doit exploiter cette simplicité pour étudier en profondeur les suites linéaires (où la réduction est la clé) et les suites itératives (où les méthodes de point fixe dominent).
\end{philosophie}

\section{Topologie et Convergence en Dimension Finie}
\subsection{Le Théorème Fondamental : Équivalence des Normes}
\begin{itemize}
    \item \textbf{Théorème :} Sur un espace vectoriel de dimension finie, toutes les normes sont équivalentes.
    \item \textbf{Conséquences :}
        \begin{itemize}
            \item La topologie est unique. La convergence d'une suite ne dépend pas de la norme choisie.
            \item Une suite converge si et seulement si ses composantes dans une base convergent.
            \item Les e.v.n. de dimension finie sont toujours des espaces de Banach.
            \item Les compacts sont les fermés bornés.
        \end{itemize}
\end{itemize}
\subsection{Normes Matricielles}
\begin{itemize}
    \item \textbf{Définition :} Normes subordonnées. $\|A\| = \sup_{\|x\|=1} \|Ax\|$.
    \item \textbf{Rayon Spectral $\rho(A)$ :} $\rho(A) = \max\{|\lambda| \mid \lambda \in \mathrm{Sp}(A)\}$.
    \item \textbf{Théorème :} $\rho(A) = \lim_{k \to \infty} \|A^k\|^{1/k}$. De plus, $\rho(A) \le \|A\|$ pour toute norme subordonnée.
\end{itemize}

\section{Suites Linéaires $U_{n+1} = A U_n + B$}
\subsection{Le Cas Homogène $U_{n+1}=AU_n$}
\begin{itemize}
    \item \textbf{Solution Explicite :} $U_n = A^n U_0$.
    \item \textbf{Théorème de Convergence :} La suite $(A^n)$ tend vers 0 si et seulement si $\rho(A)<1$.
    \item \textbf{Comportement Asymptotique :} Le comportement de la suite est entièrement gouverné par le spectre de $A$. La réduction de Jordan permet une étude fine.
\end{itemize}
\subsection{Le Cas Affine}
\begin{itemize}
    \item \textbf{Recherche de Points Fixes :} La limite éventuelle est un point fixe de l'application $X \mapsto AX+B$.
    \item \textbf{Théorème de Convergence :} Si $\rho(A)<1$, la suite converge vers l'unique point fixe $X = (I-A)^{-1}B$ pour tout point de départ.
\end{itemize}

\section{Applications aux Méthodes Itératives}
\subsection{Résolution de Systèmes Linéaires $Ax=b$}
\begin{itemize}
    \item \textbf{Principe :} On décompose $A=M-N$ et on itère $x_{k+1} = M^{-1}(Nx_k + b)$.
    \item \textbf{Convergence :} La méthode converge si et seulement si le rayon spectral de la matrice d'itération $J=M^{-1}N$ est strictement inférieur à 1.
    \item \textbf{Exemples :} Jacobi, Gauss-Seidel. Convergence si $A$ est à diagonale strictement dominante.
\end{itemize}
\subsection{Résolution de Systèmes Non-Linéaires : Méthode de Newton}
\begin{itemize}
    \item \textbf{Itération :} $X_{k+1} = X_k - [J_F(X_k)]^{-1} F(X_k)$.
    \item \textbf{Convergence :} Locale et quadratique.
\end{itemize}
\subsection{Calcul de Valeurs Propres : Méthode de la Puissance}
\begin{itemize}
    \item \textbf{Principe :} Pour trouver la valeur propre de plus grand module, on itère $x_{k+1} = \frac{A x_k}{\|A x_k\|}$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème d'équivalence des normes en dimension finie :} Un développement de base essentiel, qui utilise la compacité.
        \item \textbf{Convergence de la méthode de Jacobi pour une matrice à diagonale strictement dominante :} Un développement classique d'analyse numérique matricielle.
        \item \textbf{Le rayon spectral gouverne la convergence de $A^n$ :} Un résultat central dont la preuve utilise la réduction de Jordan.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier que la dimension est finie :} L'équivalence des normes et la complétude sont fausses en général.
        \item \textbf{Confondre norme et rayon spectral} pour la condition de convergence de $A^n$.
        \item \textbf{Mal maîtriser la définition du rayon spectral.}
    \end{itemize}
\end{erreurs}
\chapter{Leçon 228 : Continuité et dérivabilité des fonctions réelles d'une variable réelle.}

\begin{philosophie}
    C'est la leçon sur les fondations de l'analyse, mais elle ne doit pas être une récitation de cours de L1. Le jury attend une maîtrise parfaite des subtilités et une vision claire de l'articulation entre les concepts. Le plan doit montrer comment les propriétés \textbf{locales} (continuité/dérivabilité en un point) engendrent des propriétés \textbf{globales} puissantes sur les intervalles, grâce à la topologie de $\mathbb{R}$ (complétude, connexité).
\end{philosophie}

\section{Continuité : Propriétés Locales et Globales}
\subsection{Définitions et Premières Propriétés}
\begin{itemize}
    \item \textbf{Définitions équivalentes :} En termes de limites, "epsilon-delta", séquentielle, topologique.
    \item \textbf{Stabilité :} Opérations algébriques, composition.
\end{itemize}
\subsection{Les Grands Théorèmes Globaux}
\begin{itemize}
    \item \textbf{Théorème des Valeurs Intermédiaires (TVI) :} L'image d'un intervalle (un connexe) est un intervalle.
    \item \textbf{Théorème de la Borne Atteinte (Heine) :} L'image d'un segment (un compact) est un segment.
    \item \textbf{Théorème de la Continuité Uniforme (Heine-Cantor) :} Une fonction continue sur un segment est uniformément continue.
\end{itemize}
\begin{remark}
    Ces trois théorèmes sont des conséquences directes des propriétés topologiques de $\mathbb{R}$ (connexité et compacité des segments). Ils sont le cœur de l'analyse réelle.
\end{remark}

\section{Dérivabilité : L'Approximation Linéaire Locale}
\subsection{Définition et Propriétés}
\begin{itemize}
    \item \textbf{Définition :} Existence d'un développement limité d'ordre 1.
    \item \textbf{Lien avec la continuité :} Dérivable $\implies$ Continue. La réciproque est fausse (ex: $x \mapsto |x|$).
    \item \textbf{Opérations :} Dérivée d'une somme, produit, quotient, composée.
\end{itemize}
\subsection{Les Grands Théorèmes de la Dérivation}
\begin{itemize}
    \item \textbf{Théorème de Rolle :} Le point de départ. Si $f(a)=f(b)$, il existe un point où la tangente est horizontale.
    \item \textbf{Théorème des Accroissements Finis (TAF) :} Existence d'un point $c$ tel que $f(b)-f(a) = f'(c)(b-a)$. Interprétation géométrique.
    \item \textbf{Inégalité des Accroissements Finis :} Outil de majoration fondamental.
\end{itemize}

\section{Régularité Supérieure et Formules de Taylor}
\subsection{Fonctions de Classe $\mathcal{C}^k$}
\begin{itemize}
    \item \textbf{Théorème de la Limite de la Dérivée :} Si $f$ est continue sur $[a,b]$, dérivable sur $]a,b[$ et que $f'$ a une limite en $a$, alors $f$ est dérivable en $a$ et $f'(a)$ est cette limite.
\end{itemize}
\subsection{Formules de Taylor}
\begin{itemize}
    \item \textbf{Taylor-Young (local) :} Existence d'un développement limité.
    \item \textbf{Taylor avec Reste Intégral (global) :} Expression exacte du reste.
    \item \textbf{Taylor-Lagrange (global) :} Généralisation du TAF.
\end{itemize}
\subsection{Contre-exemples et Pathologies}
\begin{itemize}
    \item \textbf{Fonction continue partout, dérivable nulle part :} Fonction de Weierstrass.
    \item \textbf{Fonction dérivable à dérivée non-continue :} $f(x)=x^2\sin(1/x)$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration du Théorème de Rolle et du TAF :} Un classique indispensable.
        \item \textbf{Caractérisation de la continuité uniforme par le théorème de Heine-Cantor :} Montre une bonne compréhension des notions topologiques.
        \item \textbf{Formule de Taylor avec reste intégral et application :} Par exemple, pour prouver que $e$ est irrationnel.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre continuité et continuité uniforme.}
        \item \textbf{Mal appliquer les théorèmes :} Oublier une hypothèse (continuité sur le fermé, dérivabilité sur l'ouvert pour Rolle).
        \item \textbf{Penser que la dérivée d'une fonction dérivable est continue.} Il faut connaître le contre-exemple classique.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 229 : Fonctions monotones. Fonctions convexes.}

\begin{philosophie}
    Cette leçon se concentre sur deux classes de fonctions qui, même sans être supposées régulières, possèdent des propriétés de régularité remarquables. L'objectif est de montrer comment une contrainte \textbf{géométrique} (la croissance pour la monotonie, la position par rapport aux cordes pour la convexité) induit des conséquences \textbf{analytiques} très fortes.
\end{philosophie}

\section{Fonctions Monotones}
\subsection{Propriétés de Continuité}
\begin{itemize}
    \item \textbf{Théorème de la Limite Monotone :} Une fonction monotone sur un intervalle admet en tout point une limite à gauche et une limite à droite.
    \item \textbf{Conséquence (Ensemble des discontinuités) :} L'ensemble des points de discontinuité d'une fonction monotone est au plus dénombrable.
\end{itemize}
\subsection{Propriétés de Dérivabilité}
\begin{itemize}
    \item \textbf{Théorème de Dérivabilité de Lebesgue :} Une fonction monotone sur un intervalle est dérivable presque partout.
    \item \textbf{Lien avec l'intégrale :} Une fonction monotone est Riemann-intégrable.
\end{itemize}

\section{Fonctions Convexes}
\subsection{Définitions et Propriétés Géométriques}
\begin{itemize}
    \item \textbf{Définition Géométrique :} L'épigraphe est un ensemble convexe.
    \item \textbf{Définition Algébrique :} Inégalité de convexité.
    \item \textbf{Inégalité des Trois Pentes :} Une caractérisation géométrique très utile.
\end{itemize}
\subsection{Propriétés de Régularité}
\begin{itemize}
    \item \textbf{Continuité :} Une fonction convexe sur un intervalle \textbf{ouvert} est continue.
    \item \textbf{Dérivées à Gauche et à Droite :} En tout point d'un intervalle ouvert, une fonction convexe admet une dérivée à gauche et une dérivée à droite, qui sont des fonctions croissantes.
    \item \textbf{Dérivabilité Presque Partout :} Une fonction convexe est dérivable presque partout.
\end{itemize}
\subsection{Caractérisations Différentielles}
\begin{itemize}
    \item \textbf{Ordre 1 :} Si $f$ est dérivable, $f$ est convexe $\iff f'$ est croissante $\iff$ la courbe est au-dessus de ses tangentes.
    \item \textbf{Ordre 2 :} Si $f$ est deux fois dérivable, $f$ est convexe $\iff f'' \ge 0$.
\end{itemize}

\section{Inégalités de Convexité et Applications}
\subsection{L'Inégalité de Jensen}
\begin{itemize}
    \item \textbf{Énoncé (cas discret et intégral) :} Pour une fonction convexe $\phi$, $\phi(E[X]) \le E[\phi(X)]$.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Inégalités Classiques :} L'inégalité arithmético-géométrique se déduit de la convexité de la fonction $-\ln(x)$.
    \item \textbf{Inégalités de Hölder et Minkowski :} Peuvent être démontrées en utilisant la convexité de l'exponentielle.
    \item \textbf{Optimisation :} Pour une fonction convexe, tout minimum local est un minimum global.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{L'ensemble des points de discontinuité d'une fonction monotone est au plus dénombrable :} Un beau résultat de topologie.
        \item \textbf{Une fonction convexe sur un ouvert est continue :} Un développement classique qui utilise l'inégalité des trois pentes.
        \item \textbf{Démonstration de l'inégalité arithmético-géométrique par l'inégalité de Jensen :} Un exemple parfait d'application de la théorie.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Penser qu'une fonction convexe est toujours continue :} C'est faux sur un intervalle fermé.
        \item \textbf{Penser qu'une fonction convexe est toujours dérivable :} Il faut connaître le contre-exemple $x \mapsto |x|$.
        \item \textbf{Mal énoncer l'inégalité de Jensen} (oublier les poids ou la mesure de probabilité).
    \end{itemize}
\end{erreurs}

\chapter{Leçon 230 : Séries de nombres réels ou complexes.}

\begin{philosophie}
    C'est la leçon qui fonde l'art de sommer l'infini. Loin d'être élémentaire, elle exige une maîtrise des subtilités de la convergence. Le plan doit s'articuler autour de la dichotomie fondamentale : la convergence absolue, qui autorise toutes les manipulations algébriques comme si les sommes étaient finies, et la convergence simple (ou semi-convergence), un monde plus fragile où l'ordre des termes devient crucial. La leçon doit être une galerie d'outils et de contre-exemples pour naviguer dans ces deux mondes.
\end{philosophie}

\section{Nature d'une Série : Le Sens d'une Somme Infinie}
\subsection{Définitions et Critère Fondamental}
\begin{itemize}
    \item \textbf{Définition :} Convergence via la suite des sommes partielles $(S_N)$.
    \item \textbf{Condition Nécessaire :} Si $\sum u_n$ converge, alors $u_n \to 0$. La réciproque est fausse (série harmonique).
    \item \textbf{Critère de Cauchy pour les Séries :} Une série converge si et seulement si pour tout $\epsilon > 0$, il existe $N$ tel que pour tout $q > p \ge N$, $|\sum_{k=p+1}^q u_k| < \epsilon$. C'est l'outil théorique le plus puissant.
\end{itemize}
\subsection{La Dichotomie : Convergence Absolue vs. Semi-Convergence}
\begin{itemize}
    \item \textbf{Définition (Convergence Absolue) :} La série $\sum |u_n|$ converge.
    \item \textbf{Théorème :} La convergence absolue implique la convergence.
    \item \textbf{Semi-Convergence :} Une série qui converge mais pas absolument.
    \item \textbf{Exemple Canonique :} La série harmonique alternée $\sum (-1)^{n+1}/n$ est semi-convergente.
    \item \textbf{Théorème de Réarrangement de Riemann :} Dans une série semi-convergente, on peut réarranger les termes pour la faire converger vers n'importe quelle valeur réelle, ou la faire diverger. C'est la pathologie fondamentale de la semi-convergence.
\end{itemize}

\section{Critères de Convergence}
\subsection{Séries à Termes Positifs}
\begin{itemize}
    \item \textbf{Le Cas de Base :} Une série à termes positifs converge si et seulement si ses sommes partielles sont majorées.
    \item \textbf{Critères de Comparaison :} Comparaison directe, par équivalents, par domination.
    \item \textbf{Séries de Référence :} Séries géométriques, séries de Riemann $\sum 1/n^\alpha$.
    \item \textbf{Critères de d'Alembert et de Cauchy :} Basés sur le comportement asymptotique du rapport $|u_{n+1}/u_n|$ ou de la racine $|u_n|^{1/n}$.
    \item \textbf{Comparaison Série-Intégrale :} Permet de retrouver la nature des séries de Riemann et d'obtenir des équivalents des restes ou des sommes partielles.
\end{itemize}
\subsection{Séries à Termes Quelconques}
\begin{itemize}
    \item \textbf{Critère Spécial des Séries Alternées :} Si $(a_n)$ est une suite positive, décroissante et tendant vers 0, alors $\sum (-1)^n a_n$ converge.
    \item \textbf{Transformation d'Abel :} L'analogue de l'intégration par parties pour les séries. C'est un outil plus puissant qui permet de prouver le critère précédent.
\end{itemize}

\section{Opérations sur les Séries et Familles Sommables}
\subsection{Produit de Cauchy}
\begin{itemize}
    \item \textbf{Définition :} $( \sum a_n ) ( \sum b_n ) = \sum c_n$ avec $c_n = \sum_{k=0}^n a_k b_{n-k}$.
    \item \textbf{Théorème de Mertens :} Si $\sum a_n$ converge absolument et $\sum b_n$ converge, alors leur produit de Cauchy converge vers le produit des sommes.
\end{itemize}
\subsection{Familles Sommables}
\begin{itemize}
    \item \textbf{Motivation :} Comment sommer une famille de nombres indexée par un ensemble non dénombrable, ou sans ordre naturel ?
    \item \textbf{Définition :} Une famille $(a_i)_{i \in I}$ est sommable si la famille des modules $(|a_i|)$ est "sommable" (i.e. le sup des sommes finies est fini). C'est une notion de convergence absolue non ordonnée.
    \item \textbf{Théorème (Fubini pour les séries) :} Si une famille doublement indexée $(a_{i,j})$ est sommable, on peut intervertir les sommes : $\sum_i (\sum_j a_{i,j}) = \sum_j (\sum_i a_{i,j})$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de réarrangement de Riemann :} Un développement spectaculaire qui montre la subtilité de la semi-convergence.
        \item \textbf{Comparaison série-intégrale et application à la constante d'Euler-Mascheroni :} Un grand classique de l'analyse.
        \item \textbf{Produit de Cauchy de $\sum (-1)^n/\sqrt{n+1}$ par elle-même :} Un exemple célèbre de produit de deux séries convergentes qui diverge.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Appliquer les critères pour termes positifs à des séries quelconques.}
        \item \textbf{Croire que $u_n \to 0$ est une condition suffisante.}
        \item \textbf{Manipuler une série semi-convergente comme une somme finie (changer l'ordre des termes).}
        \item \textbf{Oublier les hypothèses du théorème de Mertens.}
    \end{itemize}
\end{erreurs}
\chapter{Leçon 234 : Espaces $L^p$.}

\begin{philosophie}
    C'est une leçon au cœur de l'analyse moderne. Elle construit les espaces fonctionnels fondamentaux pour l'analyse, les EDP et les probabilités. Le plan doit mettre en évidence pourquoi la théorie de Lebesgue est nécessaire (pour obtenir la complétude), exposer la géométrie de ces espaces (Banach, Hilbert pour p=2), et détailler les relations essentielles entre eux (dualité, densité, inclusions).
\end{philosophie}

\section{Construction et Propriétés Fondamentales}
\subsection{Motivation : Les Limites de l'Intégrale de Riemann}
\begin{itemize}
    \item L'espace des fonctions Riemann-intégrables n'est pas complet pour les normes usuelles. Il "manque" des limites. L'intégrale de Lebesgue est construite pour "boucher les trous".
\end{itemize}
\subsection{Définition des Espaces $L^p$}
\begin{itemize}
    \item \textbf{Cadre :} Un espace mesuré $(\Omega, \mathcal{A}, \mu)$.
    \item \textbf{Norme $L^p$} pour $p \in [1, \infty[$ et \textbf{norme essentielle sup} pour $p=\infty$.
    \item \textbf{Espace $L^p(\Omega)$ :} Ensemble des classes d'équivalence de fonctions mesurables de norme finie.
\end{itemize}
\subsection{Les Théorèmes Fondamentaux}
\begin{itemize}
    \item \textbf{Inégalités de Hölder et Minkowski :} La première est fondamentale, la seconde prouve que $\|\cdot\|_p$ est une norme.
    \item \textbf{Théorème de Riesz-Fischer :} Les espaces $L^p(\Omega)$ sont des espaces de Banach. C'est le résultat central qui justifie la construction.
\end{itemize}

\section{Structure Topologique et Géométrique}
\subsection{Relations d'Inclusion}
\begin{itemize}
    \item \textbf{Théorème :} Si la mesure de l'espace est \textbf{finie}, alors pour $1 \le p \le q \le \infty$, on a l'inclusion continue $L^q(\Omega) \subset L^p(\Omega)$.
    \item \textbf{Contre-exemple :} Sur $\mathbb{R}$ (mesure infinie), $x \mapsto 1/(1+|x|)$ est dans $L^2$ mais pas dans $L^1$.
\end{itemize}
\subsection{Dualité}
\begin{itemize}
    \item \textbf{Théorème :} Pour $p \in [1, \infty[$, le dual topologique de $L^p(\Omega)$ s'identifie isométriquement à $L^q(\Omega)$ où $\frac{1}{p}+\frac{1}{q}=1$.
    \item \textbf{Le cas $p=2$ :} $L^2$ est un espace de Hilbert auto-dual.
    \item \textbf{Les cas pathologiques :} Le dual de $L^\infty$ est plus grand que $L^1$.
    \item \textbf{Réflexivité :} Les espaces $L^p$ pour $p \in ]1,\infty[$ sont réflexifs.
\end{itemize}
\subsection{Séparabilité}
\begin{itemize}
    \item \textbf{Théorème :} Si l'espace de base est "gentil" (e.g. $\mathbb{R}^n$), les espaces $L^p$ sont séparables pour $p \in [1, \infty[$.
    \item \textbf{Contre-exemple :} $L^\infty$ n'est jamais séparable.
\end{itemize}

\section{Théorèmes de Densité et Applications}
\subsection{Hiérarchie de l'Approximation}
\begin{itemize}
    \item \textbf{Théorème :} Dans $L^p(\mathbb{R}^n)$ (pour $p \in [1, \infty[$), les fonctions simples, les fonctions en escalier, les fonctions continues à support compact, et les fonctions $\mathcal{C}^\infty$ à support compact sont toutes des sous-espaces denses.
\end{itemize}
\begin{remark}
    C'est le "principe de densité" de l'analyse. Pour prouver un résultat sur une fonction $L^p$ quelconque, on le prouve sur une fonction très régulière (où l'on peut dériver, intégrer par parties...), puis on conclut par passage à la limite.
\end{remark}
\subsection{Régularisation par Convolution}
\begin{itemize}
    \item \textbf{Théorème :} Si $f \in L^p$ et $(\rho_n)$ est une suite régularisante, alors $f * \rho_n \to f$ en norme $L^p$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration des inégalités de Hölder et Minkowski :} Un développement technique mais fondamental.
        \item \textbf{Démonstration que $\mathcal{C}([a,b])$ muni de la norme $L^1$ n'est pas complet :} Un contre-exemple essentiel pour motiver les espaces $L^p$.
        \item \textbf{Densité des fonctions continues à support compact dans $L^p$ :} Un développement important qui montre la maîtrise des outils de la mesure.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier de quotienter par l'égalité presque partout :} C'est ce qui fait que $\|\cdot\|_p$ est une norme.
        \item \textbf{Appliquer l'inclusion $L^q \subset L^p$ sur un espace de mesure infinie.}
        \item \textbf{Confondre les duals de $L^1$ et $L^\infty$.}
        \item \textbf{Ne pas connaître le théorème de Riesz-Fischer :} C'est le résultat central.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 235 : Problèmes d'interversion de limites et d'intégrales.}

\begin{philosophie}
    Cette leçon est au cœur de la machinerie de l'analyse. Intervertir des opérateurs limites ($\lim$, $\sum$, $\int$, $\frac{d}{dx}$) est une opération puissante mais périlleuse. Le plan doit être une progression, partant des contre-exemples qui montrent le danger, passant par les conditions suffisantes simples (convergence uniforme), pour aboutir aux outils ultimes et les plus puissants de la théorie de Lebesgue (convergence monotone et dominée).
\end{philosophie}

\section{Le Danger de l'Interversion : Contre-exemples Fondamentaux}
\subsection{Le Cas des Suites de Fonctions}
\begin{itemize}
    \item \textbf{Contre-exemple "chapeau" :} Une suite de fonctions continues qui converge simplement vers une fonction discontinue. $\lim \int f_n \neq \int \lim f_n$.
    \item \textbf{Contre-exemple "bosse glissante" :} Une suite de fonctions $f_n$ continues et intégrables sur $\mathbb{R}$ telles que $\int f_n = 1$ pour tout $n$, mais $f_n \to 0$ simplement. $\lim \int f_n = 1 \neq \int \lim f_n = 0$.
\end{itemize}
\subsection{Le Cas des Séries de Fonctions}
\begin{itemize}
    \item L'interversion $\sum \int f_n = \int \sum f_n$ est un cas particulier de l'interversion limite-intégrale, appliqué à la suite des sommes partielles.
\end{itemize}

\section{Conditions Suffisantes dans le Cadre de Riemann : la Convergence Uniforme}
\subsection{Le Rôle de la Convergence Uniforme}
\begin{itemize}
    \item \textbf{Théorème (Continuité) :} La limite uniforme d'une suite de fonctions continues est continue.
    \item \textbf{Théorème (Intégration sur un segment) :} Si $(f_n)$ converge uniformément vers $f$ sur $[a,b]$, alors $\int_a^b f_n \to \int_a^b f$.
    \item \textbf{Théorème (Dérivation) :} Si $(f_n)$ converge simplement vers $f$, et si $(f'_n)$ converge uniformément vers $g$, alors $f$ est dérivable et $f'=g$.
\end{itemize}
\begin{remark}
    La convergence uniforme est une condition très forte et souvent trop restrictive en pratique. C'est une des limitations du cadre de Riemann.
\end{remark}

\section{La Solution de Lebesgue : la Domination}
\subsection{Les Grands Théorèmes de l'Intégration}
\begin{itemize}
    \item \textbf{Théorème de Convergence Monotone (Beppo-Levi) :} Pour une suite croissante de fonctions mesurables positives, l'interversion $\int \lim = \lim \int$ est toujours valide.
    \item \textbf{Lemme de Fatou :} Fournit une inégalité $\int \liminf \le \liminf \int$.
    \item \textbf{Théorème de Convergence Dominée de Lebesgue :} Si $(f_n)$ converge presque partout vers $f$ et si la suite est dominée en module par une fonction intégrable $g$, alors on peut intervertir limite et intégrale.
\end{itemize}
\begin{remark}
    Le théorème de convergence dominée est le résultat le plus important et le plus utile de la théorie de l'intégration. L'hypothèse de domination est la clé.
\end{remark}
\subsection{Application à la Dérivation sous le Signe Intégrale}
\begin{itemize}
    \item \textbf{Théorème de Dérivation des Intégrales à Paramètres :} Pour une fonction $F(x) = \int_I f(x,t)dt$, on peut dériver sous le signe $\int$ si la dérivée partielle $\frac{\partial f}{\partial x}$ est dominée par une fonction intégrable indépendante de $x$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Démonstration du théorème de convergence monotone et du lemme de Fatou :} Un développement qui montre une bonne maîtrise des fondements de la théorie de la mesure.
        \item \textbf{Calcul de l'intégrale de Dirichlet $\int_0^\infty \frac{\sin t}{t} dt$ :} Un grand classique qui utilise à la fois l'intégration par parties et la convergence dominée pour une intégrale à paramètre.
        \item \textbf{Continuité et dérivabilité de la fonction Gamma d'Euler :} Un excellent exemple d'application des théorèmes de domination.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Intervertir les limites sans justification :} C'est la faute la plus grave.
        \item \textbf{Mal énoncer le théorème de convergence dominée :} Oublier que la fonction dominante doit être intégrable, ou que la domination doit valoir pour tout $n$.
        \item \textbf{Confondre les hypothèses des différents théorèmes.}
        \item \textbf{Ne pas connaître les contre-exemples classiques :} Ils sont essentiels pour motiver la nécessité des théorèmes.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 236 : Illustrer par des exemples la théorie des séries de Fourier.}

\begin{philosophie}
    Le titre est "Illustrer par des exemples". Cette leçon ne doit pas être un cours théorique sur les séries de Fourier, mais une galerie vivante d'exemples montrant la puissance et les subtilités de l'outil. Chaque exemple doit être choisi pour mettre en lumière un concept clé : la convergence ponctuelle, le phénomène de Gibbs, la convergence en norme $L^2$, le calcul de sommes, la résolution d'EDP.
\end{philosophie}

\section{Convergence et Comportement aux Discontinuités}
\subsection{Exemple 1 : Le Signal Carré (Créneau)}
\begin{itemize}
    \item \textbf{Calcul des coefficients :} On calcule les coefficients de Fourier de la fonction $2\pi$-périodique valant 1 sur $[0,\pi[$ et -1 sur $[\pi, 2\pi[$. On trouve une série de sinus.
    \item \textbf{Illustration du Théorème de Dirichlet :} On observe que la série converge vers la fonction là où elle est continue, et vers la demi-somme des limites à gauche et à droite aux points de discontinuité.
    \item \textbf{Illustration du Phénomène de Gibbs :} On montre (graphiquement ou par le calcul) que près des sauts, les sommes partielles dépassent la valeur de la fonction d'environ 9\%, même pour un grand nombre d'harmoniques. C'est la manifestation de la non-convergence uniforme.
\end{itemize}
\subsection{Exemple 2 : Le Signal "Chapeau" (Triangulaire)}
\begin{itemize}
    \item \textbf{Calcul des coefficients :} On calcule les coefficients de la fonction continue, $\mathcal{C}^1$ par morceaux, valant $x$ sur $[0, \pi/2]$, etc.
    \item \textbf{Illustration de la Convergence Uniforme :} Comme la fonction est continue, la convergence est uniforme. Il n'y a pas de phénomène de Gibbs.
    \item \textbf{Lien Régularité-Décroissance :} On observe que les coefficients de Fourier décroissent beaucoup plus vite (en $1/n^2$) que pour le signal carré (en $1/n$), illustrant le principe général "plus la fonction est régulière, plus ses coefficients de Fourier décroissent vite".
\end{itemize}

\section{Utilisation pour le Calcul Explicite de Sommes}
\subsection{Exemple 1 : Le Problème de Bâle ($\sum 1/n^2 = \pi^2/6$)}
\begin{itemize}
    \item \textbf{Méthode :} On applique l'identité de Parseval à la série de Fourier de la fonction $2\pi$-périodique $f(x)=x$ sur $[-\pi, \pi]$.
    \item \textbf{Le calcul :} $\frac{1}{2\pi}\int_{-\pi}^{\pi} x^2 dx = \sum_{n \in \mathbb{Z}^*} |c_n|^2$. Le calcul mène directement au résultat.
\end{itemize}
\subsection{Exemple 2 : Calcul de $\sum 1/n^4 = \pi^4/90$}
\begin{itemize}
    \item \textbf{Méthode :} On applique l'identité de Parseval à la série de Fourier de la fonction $f(x)=x^2$ sur $[-\pi, \pi]$.
\end{itemize}

\section{Applications à la Résolution d'Équations aux Dérivées Partielles}
\subsection{Exemple 1 : L'Équation de la Chaleur sur un Segment}
\begin{itemize}
    \item \textbf{Le problème :} $u_t = u_{xx}$ avec conditions aux limites nulles et $u(0,x)=f(x)$.
    \item \textbf{La méthode (Séparation des variables) :} On cherche des solutions de la forme $T(t)X(x)$, ce qui mène à décomposer la solution sur la base des sinus.
    \item \textbf{La solution :} La solution s'exprime comme une série de Fourier où les coefficients sont exponentiellement amortis : $u(t,x) = \sum c_n e^{-n^2 t} \sin(nx)$.
    \item \textbf{Illustration :} L'effet régularisant de l'équation de la chaleur est visible sur la formule : les hautes fréquences sont tuées très rapidement.
\end{itemize}
\subsection{Exemple 2 : L'Équation des Ondes (Corde Vibrante)}
\begin{itemize}
    \item \textbf{Le problème :} $u_{tt} = u_{xx}$.
    \item \textbf{La méthode :} La même approche mène à une solution où les coefficients sont des fonctions oscillantes du temps : $u(t,x) = \sum (a_n \cos(nt) + b_n \sin(nt))\sin(nx)$.
    \item \textbf{Illustration :} Les modes propres de la corde vibrante sont les harmoniques de la série de Fourier.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Étude du phénomène de Gibbs pour le signal carré :} Un développement calculatoire mais très parlant.
        \item \textbf{Calcul de $\zeta(2)$ et $\zeta(4)$ par l'identité de Parseval :} Un grand classique qui montre la puissance de l'outil pour des problèmes d'analyse a priori non liés.
        \item \textbf{Résolution de l'équation de la chaleur par séries de Fourier :} Un développement parfait qui connecte la leçon aux EDP.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un cours théorique :} Le titre exige des exemples.
        \item \textbf{Se tromper dans le calcul des coefficients :} Il faut en avoir calculé plusieurs à l'avance.
        \item \textbf{Confondre les différents types de convergence} et leurs conditions.
        \item \textbf{Ne pas mentionner le phénomène de Gibbs :} C'est une subtilité essentielle de la théorie.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 237 : Méthodes de calcul d'intégrales.}

\begin{philosophie}
    C'est une leçon qui teste la virtuosité technique, mais pas seulement. Un bon plan ne doit pas être un catalogue de recettes, mais une démonstration stratégique d'une "boîte à outils" organisée. L'objectif est de montrer que face à une intégrale, on a une hiérarchie de méthodes : les outils de base du calcul différentiel, les techniques plus fines de l'analyse réelle (intégrales à paramètres), et l'arme ultime de l'analyse complexe (théorème des résidus). La leçon doit être riche en exemples techniques maîtrisés.
\end{philosophie}

\section{Méthodes Basées sur le Théorème Fondamental de l'Analyse}
\subsection{Recherche de Primitives}
\begin{itemize}
    \item \textbf{Principe :} Le calcul d'intégrale est ramené à la recherche d'une primitive.
    \item \textbf{Intégration par Parties :} L'outil fondamental pour "transférer" la difficulté d'une fonction à l'autre.
        \begin{itemize}
            \item \textbf{Application :} Calcul des intégrales de Wallis $W_n = \int_0^{\pi/2} \sin^n(t) dt$.
        \end{itemize}
    \item \textbf{Changement de Variables :} Permet de se ramener à des formes connues.
        \begin{itemize}
            \item \textbf{Application :} Règles de Bioche pour l'intégration des fractions rationnelles en $\sin$ et $\cos$.
        \end{itemize}
    \item \textbf{Le Cas des Fractions Rationnelles :} La décomposition en éléments simples est la méthode systématique.
\end{itemize}

\section{Méthodes Analytiques Avancées}
\subsection{Intégrales à Paramètres}
\begin{itemize}
    \item \textbf{Principe :} On plonge l'intégrale à calculer dans une famille d'intégrales $F(x) = \int f(x,t)dt$. On étudie la fonction $F$ (continuité, dérivabilité) pour en déduire la valeur en un point.
    \item \textbf{Théorèmes Clés :} Continuité et dérivation sous le signe intégrale (avec domination).
    \item \textbf{Application Célèbre : L'intégrale de Gauss $\int_0^\infty e^{-t^2} dt$.}
        \begin{itemize}
            \item Une méthode est de la calculer via une intégrale double et un passage en polaires (Fubini).
            \item Une autre méthode est d'étudier la fonction $F(x) = \int_0^\infty \frac{e^{-x^2(1+t^2)}}{1+t^2}dt$.
        \end{itemize}
\end{itemize}
\subsection{Fonctions Spéciales}
\begin{itemize}
    \item \textbf{La Fonction Gamma d'Euler :} $\Gamma(x) = \int_0^\infty t^{x-1}e^{-t}dt$.
    \item \textbf{La Fonction Bêta d'Euler :} $B(x,y) = \int_0^1 t^{x-1}(1-t)^{y-1}dt$. Lien avec Gamma : $B(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}$.
\end{itemize}

\section{La Puissance de l'Analyse Complexe}
\subsection{Le Théorème des Résidus}
\begin{itemize}
    \item \textbf{Principe :} L'intégrale d'une fonction holomorphe sur un lacet est $2i\pi$ fois la somme des résidus des singularités qu'il entoure. C'est un outil de calcul d'une puissance phénoménale.
\end{itemize}
\subsection{Galerie de Calculs}
\begin{itemize}
    \item \textbf{Intégrales de fractions rationnelles sur $\mathbb{R}$} : $\int_{-\infty}^\infty \frac{dx}{1+x^4}$. On utilise un contour en demi-cercle.
    \item \textbf{Intégrales trigonométriques sur $[0, 2\pi]$} : $\int_0^{2\pi} \frac{d\theta}{2+\cos\theta}$. On pose $z=e^{i\theta}$ pour se ramener à une intégrale sur le cercle unité.
    \item \textbf{Intégrales de Fourier} : $\int_{-\infty}^\infty \frac{\cos(x)}{1+x^2} dx$. On intègre $\frac{e^{iz}}{1+z^2}$ et on utilise le lemme de Jordan.
    \item \textbf{Intégrales avec des coupures} : Calcul de $\int_0^\infty \frac{\ln x}{x^2+1}dx$. On utilise un contour en "pac-man" pour éviter la singularité du logarithme.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Calcul de l'intégrale de Dirichlet $\int_0^\infty \frac{\sin t}{t} dt$ :} Un grand classique qui peut se faire par intégrales à paramètres ou par analyse complexe.
        \item \textbf{Formule des compléments et calcul de l'intégrale de Gauss via la fonction Gamma :} Montrer que $\Gamma(1/2)=\sqrt{\pi}$.
        \item \textbf{Calcul de $\int_{-\infty}^\infty \frac{dx}{1+x^n}$ par la méthode des résidus :} Un développement technique mais très impressionnant.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un catalogue de recettes sans structure :} Le plan doit être organisé par type de méthode.
        \item \textbf{Oublier de justifier la convergence des intégrales impropres.}
        \item \textbf{Mal appliquer les théorèmes :} Oublier la domination pour les intégrales à paramètres, se tromper de contour ou de calcul de résidus en analyse complexe.
        \item \textbf{Négliger l'analyse complexe :} C'est l'outil le plus puissant, il doit occuper une place de choix.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 239 : Fonctions définies par une intégrale dépendant d'un paramètre.}

\begin{philosophie}
    C'est une leçon centrale d'analyse, qui étudie l'outil principal de création de "fonctions spéciales" (Gamma, Bêta, transformées de Fourier/Laplace...). Le cœur de la leçon est la justification rigoureuse du transfert de propriétés de la fonction sous le signe intégrale à la fonction résultante. Le plan doit donc s'articuler logiquement autour de ces transferts de propriétés : continuité, dérivabilité, et intégrabilité. La domination est le concept clé qui gouverne toute la leçon.
\end{philosophie}

\section{Continuité des Intégrales à Paramètres}
\subsection{Le Théorème Fondamental de Continuité}
\begin{itemize}
    \item \textbf{Cadre :} $F(x) = \int_I f(x,t)dt$, où $x$ est dans un espace métrique (ou normé) $A$.
    \item \textbf{Théorème de Continuité sous le signe intégrale :} Si $t \mapsto f(x,t)$ est intégrable pour tout $x$, $x \mapsto f(x,t)$ est continue pour presque tout $t$, et s'il existe une fonction $g(t)$ intégrable qui domine $f$ (i.e. $|f(x,t)| \le g(t)$), alors $F$ est continue.
\end{itemize}
\begin{remark}
    L'hypothèse de domination est cruciale. Elle permet d'appliquer le théorème de convergence dominée pour intervertir la limite (sur $x$) et l'intégrale (sur $t$).
\end{remark}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Continuité de la transformée de Fourier/Laplace :} Pour $f \in L^1$, sa transformée de Fourier $\hat{f}(\xi) = \int e^{-2i\pi x \xi} f(x)dx$ est continue (la domination est simple : $|e^{-2i\pi x \xi} f(x)| = |f(x)|$).
    \item \textbf{Continuité de la fonction Gamma :} Sur tout segment de $]0, \infty[$.
\end{itemize}

\section{Dérivabilité des Intégrales à Paramètres}
\subsection{Le Théorème de Dérivation de Leibniz}
\begin{itemize}
    \item \textbf{Théorème :} Soit $F(x) = \int_I f(x,t)dt$. Si $t \mapsto f(x,t)$ est intégrable, si $\frac{\partial f}{\partial x}$ existe, et si la dérivée partielle est dominée par une fonction intégrable $g(t)$ indépendante de $x$, alors $F$ est de classe $\mathcal{C}^1$ et :
    $$ F'(x) = \int_I \frac{\partial f}{\partial x}(x,t) dt $$
    \item \textbf{Régularité $\mathcal{C}^k$ et $\mathcal{C}^\infty$ :} Le théorème s'itère. Si toutes les dérivées partielles sont dominées, on obtient une fonction $\mathcal{C}^\infty$.
\end{itemize}
\subsection{Le Cas des Bornes Variables}
\begin{itemize}
    \item \textbf{Formule de Leibniz complète :} Pour $F(x) = \int_{u(x)}^{v(x)} f(x,t)dt$, la formule de dérivation inclut des termes de bord.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Régularité de la fonction Gamma :} La fonction $\Gamma$ est $\mathcal{C}^\infty$ sur $]0, \infty[$ et est logarithmiquement convexe.
    \item \textbf{Résolution d'EDO :} On peut utiliser la dérivation sous le signe intégrale pour montrer que la transformée de Laplace d'une solution d'EDO linéaire à coefficients constants est une fraction rationnelle.
\end{itemize}

\section{Intégrabilité et Interversion : Fubini}
\subsection{Le Problème de l'Interversion}
\begin{itemize}
    \item Peut-on intervertir deux signes d'intégration ? $\int_A \left( \int_B f(x,t) dt \right) dx = \int_B \left( \int_A f(x,t) dx \right) dt$ ?
\end{itemize}
\subsection{Les Théorèmes de Fubini}
\begin{itemize}
    \item \textbf{Fubini-Tonelli (cas positif) :} Si $f(x,t)$ est mesurable et positive, l'interversion est toujours possible. Les trois intégrales (double et les deux itérées) sont égales (éventuellement à $+\infty$).
    \item \textbf{Fubini-Lebesgue (cas intégrable) :} Si $f$ est intégrable pour la mesure produit (i.e. $\iint |f| < \infty$), alors l'interversion est possible et les intégrales sont finies.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Calcul de l'intégrale de Gauss :} La preuve classique utilise Fubini-Tonelli.
    \item \textbf{Produit de Convolution :} Fubini est l'outil qui justifie les propriétés de la convolution (associativité, etc.) et qui montre que si $f,g \in L^1$, alors $f*g \in L^1$ et $\|f*g\|_1 \le \|f\|_1 \|g\|_1$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Continuité et dérivabilité de la fonction Gamma d'Euler :} Un développement très complet qui utilise les deux théorèmes principaux de la leçon.
        \item \textbf{Calcul de l'intégrale de Dirichlet $\int_0^\infty \frac{\sin t}{t} dt$ :} Le calcul via l'intégrale à paramètre $\int_0^\infty \frac{\sin t}{t} e^{-xt} dt$ est un grand classique.
        \item \textbf{Étude de la transformée de Fourier de la Gaussienne :} Montre que la Gaussienne est un point fixe (à un facteur près) de la transformée de Fourier.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier ou mal énoncer l'hypothèse de domination :} C'est le cœur de la leçon, l'erreur la plus grave.
        \item \textbf{Vérifier la domination sur tout l'ensemble de définition du paramètre :} La domination n'est souvent nécessaire que localement (sur un voisinage).
        \item \textbf{Confondre Fubini-Tonelli et Fubini-Lebesgue :} Le premier s'applique aux fonctions positives sans condition d'intégrabilité, le second aux fonctions intégrables.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 241 : Suites et séries de fonctions.}

\begin{philosophie}
    Cette leçon explore les subtilités de l'infini en analyse fonctionnelle. La question centrale est : quelles propriétés des fonctions d'une suite ou série sont conservées par le passage à la limite ? La leçon doit s'articuler autour de la hiérarchie des modes de convergence (simple, uniforme, normale), en montrant par des théorèmes et des contre-exemples que des modes de convergence plus forts permettent des transferts de propriétés plus puissants.
\end{philosophie}

\section{La Hiérarchie des Convergences}
\subsection{Convergence Simple : une Notion Faible}
\begin{itemize}
    \item \textbf{Définition :} Convergence point par point.
    \item \textbf{Pathologies :}
        \begin{itemize}
            \item La limite simple d'une suite de fonctions continues n'est pas forcément continue (ex: $f_n(x)=x^n$ sur $[0,1]$).
            \item L'interversion limite-intégrale est fausse en général (ex: "bosse glissante").
        \end{itemize}
\end{itemize}
\subsection{Convergence Uniforme : la "Bonne" Notion Topologique}
\begin{itemize}
    \item \textbf{Définition :} Convergence pour la norme du sup, $\|f_n - f\|_\infty \to 0$.
    \item \textbf{Interprétation Géométrique :} Le graphe de $f_n$ finit par être entièrement contenu dans un "tube" autour du graphe de $f$.
    \item \textbf{Critère de Cauchy Uniforme :} L'outil théorique pour prouver la convergence uniforme sans connaître la limite.
\end{itemize}
\subsection{Convergence Normale : un Critère Pratique pour les Séries}
\begin{itemize}
    \item \textbf{Définition :} Pour une série $\sum f_n$, la convergence normale est la convergence de la série numérique des normes sup $\sum \|f_n\|_\infty$.
    \item \textbf{Implications :} Convergence Normale $\implies$ Convergence Uniforme $\implies$ Convergence Simple. Les réciproques sont fausses.
\end{itemize}

\section{Théorèmes de Transfert de Propriétés}
\subsection{Continuité}
\begin{itemize}
    \item \textbf{Théorème (Double Limite) :} Si $f_n \to f$ uniformément et si chaque $f_n$ a une limite en $a$, alors on peut intervertir les limites : $\lim_{x \to a} \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \lim_{x \to a} f_n(x)$.
    \item \textbf{Corollaire :} La limite uniforme d'une suite de fonctions continues est continue.
\end{itemize}
\subsection{Intégration}
\begin{itemize}
    \item \textbf{Théorème :} Si $(f_n)$ converge uniformément vers $f$ sur un segment $[a,b]$, alors $\int_a^b f_n \to \int_a^b f$.
\end{itemize}
\subsection{Dérivation}
\begin{itemize}
    \item \textbf{Théorème (le plus subtil) :} Si $(f_n)$ converge simplement vers $f$, si les $f_n$ sont $\mathcal{C}^1$, et si la suite des dérivées $(f'_n)$ converge \textbf{uniformément} vers $g$, alors $f$ est $\mathcal{C}^1$ et $f'=g$.
\end{itemize}
\begin{remark}
    L'hypothèse cruciale porte sur la convergence uniforme des DÉRIVÉES. La convergence uniforme de $(f_n)$ ne suffit pas.
\end{remark}

\section{Applications et Exemples}
\subsection{Construction de Fonctions}
\begin{itemize}
    \item \textbf{L'exponentielle :} Définie par la série entière $\sum z^n/n!$. La convergence normale sur tout disque compact assure la continuité.
    \item \textbf{La fonction Zêta de Riemann :} Définie par $\zeta(s) = \sum 1/n^s$. La convergence normale de la série et de ses dérivées sur $\{s \mid \mathrm{Re}(s) > 1+\epsilon\}$ montre que $\zeta$ est holomorphe.
\end{itemize}
\subsection{Théorèmes d'Approximation}
\begin{itemize}
    \item \textbf{Théorèmes de Weierstrass :} Sont des résultats de convergence uniforme.
\end{itemize}
\subsection{Construction de "Monstres" : Fonctions Pathologiques}
\begin{itemize}
    \item \textbf{Fonction de Weierstrass :} La fonction $f(x) = \sum_{n=0}^\infty a^n \cos(b^n \pi x)$ (pour de bons choix de $a,b$) est une série normalement convergente de fonctions $\mathcal{C}^\infty$, donc sa somme est continue. Pourtant, on peut montrer qu'elle n'est dérivable en aucun point.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de Weierstrass trigonométrique (preuve par le noyau de Fejér) :} Un développement d'analyse très classique, élégant et qui montre une bonne maîtrise de l'analyse de Fourier.
        \item \textbf{Construction d'une fonction continue nulle part dérivable :} Un développement de haut niveau qui illustre les subtilités de la convergence.
        \item \textbf{Le théorème de la double limite et ses applications (continuité, dérivation) :} Un développement qui montre une compréhension profonde des fondements.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les différents modes de convergence :} C'est l'erreur la plus grave. Il faut connaître les contre-exemples qui les séparent.
        \item \textbf{Intervertir les limites sans justification :} Le cœur de la leçon est précisément de fournir ces justifications.
        \item \textbf{Mal énoncer le théorème de dérivation :} Oublier la convergence uniforme des dérivées.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 243 : Séries entières, propriétés de la somme.}

\begin{philosophie}
    Les séries entières sont des objets "magiques" au carrefour de l'algèbre et de l'analyse. L'objectif de la leçon est de montrer cette dualité : ce sont des séries de fonctions qui jouissent de propriétés de convergence et de régularité exceptionnelles, mais ce sont aussi des "polynômes de degré infini" avec des propriétés algébriques. Le plan doit mettre en valeur cette double nature, en montrant comment la structure rigide de la convergence mène à des propriétés analytiques spectaculaires.
\end{philosophie}

\section{Convergence : une Structure Rigide}
\subsection{Lemme d'Abel et Rayon de Convergence}
\begin{itemize}
    \item \textbf{Lemme d'Abel :} Le lemme fondamental qui structure le domaine de convergence.
    \item \textbf{Définition du Rayon de Convergence $R$} : Il existe un $R \in [0, \infty]$ tel que la série converge absolument pour $|z|<R$ et diverge grossièrement pour $|z|>R$.
    \item \textbf{Calcul du Rayon :} Règles de d'Alembert et de Cauchy (Hadamard).
\end{itemize}
\subsection{Comportement au Bord du Disque de Convergence}
\begin{itemize}
    \item \textbf{Le cercle d'incertitude :} Pour $|z|=R$, tout peut arriver (convergence absolue, semi-convergence, divergence).
    \item \textbf{Exemples canoniques :}
        \begin{itemize}
            \item $\sum z^n$ (diverge partout au bord).
            \item $\sum z^n/n$ (converge sauf en $z=1$).
            \item $\sum z^n/n^2$ (converge normalement, donc absolument et uniformément, sur le disque fermé).
        \end{itemize}
    \item \textbf{Théorème d'Abel Radia}l : Si la série converge en un point du bord, la convergence est uniforme sur le segment radial.
\end{itemize}

\section{Propriétés Analytiques de la Somme}
\subsection{Régularité de la Somme}
\begin{itemize}
    \item \textbf{Convergence Normale :} Une série entière converge normalement (donc uniformément) sur tout disque compact inclus dans son disque de convergence ouvert.
    \item \textbf{Continuité :} La somme est donc continue sur le disque ouvert.
    \item \textbf{Théorème (Analyticité) :} La somme d'une série entière est une fonction de classe $\mathcal{C}^\infty$ (holomorphe sur $\mathbb{C}$) à l'intérieur de son disque de convergence. On peut dériver et intégrer terme à terme, et le rayon de convergence reste le même.
\end{itemize}
\subsection{Propriétés d'Unicité et d'Identité}
\begin{itemize}
    \item \textbf{Principe des Zéros Isolés :} Les zéros d'une somme de série entière non nulle sont isolés.
    \item \textbf{Principe du Prolongement Analytique :} Si deux sommes de séries entières coïncident sur un ensemble ayant un point d'accumulation, elles sont égales.
\end{itemize}
\begin{remark}
    Ces propriétés traduisent la "rigidité" extrême des fonctions analytiques. Elles sont entièrement déterminées par leur comportement local.
\end{remark}

\section{Applications}
\subsection{Définition et Étude de Fonctions Spéciales}
\begin{itemize}
    \item \textbf{L'Exponentielle Complexe :} Définie par $e^z = \sum z^n/n!$. Le rayon est infini. Cette définition permet de prouver toutes les propriétés de l'exponentielle (morphisme, etc.).
    \item \textbf{Fonctions Trigonométriques et Hyperboliques :} Définies à partir de l'exponentielle complexe.
\end{itemize}
\subsection{Résolution d'Équations Différentielles}
\begin{itemize}
    \item \textbf{Méthode :} On cherche une solution sous la forme $y(x) = \sum a_n x^n$. L'équation différentielle se traduit par une relation de récurrence sur les coefficients $a_n$.
    \item \textbf{Exemple Fondamental : $y' = y, y(0)=1$} mène à l'exponentielle.
    \item \textbf{Exemple non trivial : Équation d'Airy $y'' - xy = 0$.}
\end{itemize}
\subsection{Combinatoire : les Séries Génératrices}
\begin{itemize}
    \item \textbf{Principe :} On encode une suite $(a_n)$ par sa série génératrice. Une relation de récurrence sur la suite devient une équation sur la fonction.
    \item \textbf{Exemple : Nombres de Fibonacci.} La série génératrice est une fraction rationnelle. Sa décomposition en éléments simples donne la formule de Binet.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Résolution de l'EDO $y''-xy=0$ par la méthode des séries entières :} Un exemple classique et non trivial qui montre la puissance de l'outil.
        \item \textbf{Dénombrement des arbres binaires (nombres de Catalan) via les séries génératrices :} Un développement très élégant qui connecte l'analyse et la combinatoire.
        \item \textbf{Le théorème d'Abel angulaire :} Un résultat fin et non trivial sur le comportement au bord du disque de convergence.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre rayon de convergence et domaine de convergence :} Le comportement au bord est crucial.
        \item \textbf{Appliquer le théorème de dérivation terme à terme sur le bord du disque sans précaution.}
        \item \textbf{Ne pas savoir définir l'exponentielle complexe et en déduire les propriétés de $\cos$ et $\sin$.}
        \item \textbf{Confondre série entière et série de Taylor :} Une fonction $\mathcal{C}^\infty$ n'est pas forcément égale à sa série de Taylor (ex: $e^{-1/x^2}$).
    \end{itemize}
\end{erreurs}

\chapter{Leçon 245 : Fonctions d'une variable complexe. Exemples et applications.}

\begin{philosophie}
    C'est la leçon qui ouvre la porte au monde "magique" de l'analyse complexe. L'objectif est de montrer qu'une condition locale d'apparence anodine (la $\mathbb{C}$-différentiabilité ou holomorphie) entraîne des conséquences globales d'une rigidité spectaculaire. Une fonction holomorphe n'est pas une fonction $\mathcal{C}^\infty$ ordinaire ; elle est contrainte par des principes qui n'ont pas d'équivalent en analyse réelle. Le plan doit suivre la progression logique de la théorie : de la condition locale (Cauchy-Riemann) au théorème intégral de Cauchy, puis à sa cascade de conséquences miraculeuses, pour aboutir à l'outil de calcul ultime, le théorème des résidus.
\end{philosophie}

\section{Holomorphie : une Condition de Rigidité Géométrique}
\subsection{Dérivabilité Complexe vs. Différentiabilité Réelle}
\begin{itemize}
    \item \textbf{Définition (Holomorphie) :} Existence de la limite du taux d'accroissement $\frac{f(z)-f(z_0)}{z-z_0}$.
    \item \textbf{Théorème (Équations de Cauchy-Riemann) :} Lien avec la différentiabilité réelle. $f=P+iQ$ est holomorphe ssi $P,Q$ sont $\mathcal{C}^1$ et vérifient $\partial_x P = \partial_y Q, \partial_y P = -\partial_x Q$.
    \item \textbf{Interprétation Géométrique :} Le Jacobien d'une fonction holomorphe est une matrice de similitude directe. L'holomorphie est équivalente à être une transformation conforme (préservant les angles) en tout point où la dérivée n'est pas nulle.
\end{itemize}

\section{Le Théorème de Cauchy et ses Conséquences Directes}
\subsection{Intégration Complexe}
\begin{itemize}
    \item \textbf{Théorème Intégral de Cauchy :} Pour une fonction holomorphe sur un ouvert étoilé (ou simplement connexe), son intégrale sur tout lacet est nulle.
\end{itemize}
\subsection{La Formule Intégrale de Cauchy : le Cœur de la Théorie}
\begin{itemize}
    \item \textbf{Formule :} $f(z_0) = \frac{1}{2i\pi} \oint_\gamma \frac{f(z)}{z-z_0} dz$.
    \item \textbf{Philosophie "Local-Global" :} La valeur d'une fonction en un point est entièrement déterminée par ses valeurs sur une courbe qui l'entoure. C'est la première manifestation de la rigidité holomorphe.
\end{itemize}
\subsection{Une Cascade de Conséquences Spectaculaires}
\begin{itemize}
    \item \textbf{Analyticité :} Toute fonction holomorphe est développable en série entière au voisinage de chaque point. (Holomorphe $\iff$ Analytique).
    \item \textbf{Régularité $\mathcal{C}^\infty$ :} Être dérivable une fois au sens complexe implique être infiniment dérivable.
    \item \textbf{Théorème de Liouville :} Toute fonction entière (holomorphe sur $\mathbb{C}$) et bornée est constante.
    \item \textbf{Théorème Fondamental de l'Algèbre (d'Alembert-Gauss) :} Preuve élégante via le théorème de Liouville.
\end{itemize}

\section{Principes Fondamentaux et Singularités}
\subsection{Principes issus de la Rigidité}
\begin{itemize}
    \item \textbf{Principe du Maximum :} Le module d'une fonction holomorphe non constante atteint son maximum sur la frontière du domaine.
    \item \textbf{Principe des Zéros Isolés et Prolongement Analytique.}
\end{itemize}
\subsection{Analyse des Singularités}
\begin{itemize}
    \item \textbf{Séries de Laurent :} Développement au voisinage d'une singularité isolée.
    \item \textbf{Classification des singularités :} Apparente, pôle, essentielle.
    \item \textbf{Théorème des Résidus :} $\oint_\gamma f(z) dz = 2i\pi \sum_k \mathrm{Res}(f, z_k)$.
\end{itemize}

\section{Applications}
\subsection{Calcul d'Intégrales Réelles}
\begin{itemize}
    \item Fractions rationnelles, intégrales trigonométriques, intégrales de Fourier...
\end{itemize}
\subsection{Principe de l'Argument et Théorème de Rouché}
\begin{itemize}
    \item \textbf{Principe de l'Argument :} $\frac{1}{2i\pi} \int_\gamma \frac{f'(z)}{f(z)}dz = Z-P$ (nombre de zéros moins nombre de pôles).
    \item \textbf{Théorème de Rouché :} Outil puissant pour localiser les zéros de fonctions complexes.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème des résidus et calcul d'une intégrale non triviale (e.g. $\int_0^\infty \frac{\ln x}{x^2+1}dx$)} : Un développement technique mais très impressionnant.
        \item \textbf{Démonstration du Théorème Fondamental de l'Algèbre par le théorème de Liouville :} Un grand classique, court et élégant.
        \item \textbf{Principe du maximum et application (Lemme de Schwarz)} : Un développement plus théorique qui montre une grande finesse.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre $\mathbb{R}$-différentiabilité et $\mathbb{C}$-différentiabilité :} Il faut connaître le contre-exemple $z \mapsto \bar{z}$.
        \item \textbf{Mal énoncer les hypothèses du théorème de Cauchy :} L'hypothèse sur le domaine (étoilé, simplement connexe) est cruciale.
        \item \textbf{Faire une erreur de calcul de résidu :} Il faut maîtriser les différentes techniques de calcul.
        \item \textbf{Faire une leçon catalogue sans narration :} Le plan doit raconter l'histoire de la rigidité holomorphe.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 246 : Séries de Fourier. Exemples et applications.}

\begin{philosophie}
    Le titre est "Exemples et applications". Cette leçon ne doit pas être un cours théorique sur les séries de Fourier, mais une galerie vivante d'exemples montrant la puissance et les subtilités de l'outil. Chaque exemple doit être choisi pour mettre en lumière un concept clé : la convergence ponctuelle, le phénomène de Gibbs, la convergence en norme $L^2$, le calcul de sommes, la résolution d'EDP. C'est une leçon où la connexion avec la physique et le traitement du signal est bienvenue.
\end{philosophie}

\section{Convergence et Comportement aux Discontinuités}
\subsection{Exemple 1 : Le Signal Carré (Créneau)}
\begin{itemize}
    \item \textbf{Calcul des coefficients :} On calcule les coefficients de Fourier de la fonction $2\pi$-périodique valant 1 sur $[0,\pi[$ et -1 sur $[\pi, 2\pi[$. On trouve une série de sinus.
    \item \textbf{Illustration du Théorème de Dirichlet :} On observe que la série converge vers la fonction là où elle est continue, et vers la demi-somme des limites à gauche et à droite aux points de discontinuité.
    \item \textbf{Illustration du Phénomène de Gibbs :} On montre (graphiquement ou par le calcul) que près des sauts, les sommes partielles dépassent la valeur de la fonction d'environ 9\%, même pour un grand nombre d'harmoniques. C'est la manifestation de la non-convergence uniforme.
\end{itemize}
\subsection{Exemple 2 : Le Signal "Chapeau" (Triangulaire)}
\begin{itemize}
    \item \textbf{Calcul des coefficients :} On calcule les coefficients de la fonction continue, $\mathcal{C}^1$ par morceaux.
    \item \textbf{Illustration de la Convergence Normale :} Comme la fonction est continue et $\mathcal{C}^1$ par morceaux, sa série de Fourier converge normalement. Il n'y a pas de phénomène de Gibbs.
    \item \textbf{Lien Régularité-Décroissance :} On observe que les coefficients de Fourier décroissent beaucoup plus vite (en $1/n^2$) que pour le signal carré (en $1/n$), illustrant le principe général "plus la fonction est régulière, plus ses coefficients de Fourier décroissent vite".
\end{itemize}

\section{Le Point de Vue Hilbertien : Convergence $L^2$ et Parseval}
\subsection{L'Espace $L^2([0,2\pi])$}
\begin{itemize}
    \item \textbf{Théorème :} La famille des exponentielles complexes $\{e^{int}\}_{n \in \mathbb{Z}}$ (normalisée) forme une base hilbertienne de l'espace de Hilbert $L^2([0, 2\pi])$.
\end{itemize}
\subsection{Exemple 3 : Calcul de $\sum 1/n^2 = \pi^2/6$ (Problème de Bâle)}
\begin{itemize}
    \item \textbf{Méthode :} On applique l'identité de Parseval à la série de Fourier de la fonction $2\pi$-périodique $f(x)=x$ sur $[-\pi, \pi]$.
    \item \textbf{Le calcul :} $\frac{1}{2\pi}\int_{-\pi}^{\pi} x^2 dx = \sum_{n \in \mathbb{Z}^*} |c_n|^2$. Le calcul mène directement au résultat.
\end{itemize}
\subsection{Exemple 4 : Calcul de $\sum 1/n^4 = \pi^4/90$}
\begin{itemize}
    \item \textbf{Méthode :} On applique l'identité de Parseval à la série de Fourier de la fonction $f(x)=x^2$ sur $[-\pi, \pi]$.
\end{itemize}

\section{Applications à la Résolution d'Équations aux Dérivées Partielles}
\subsection{Exemple 5 : L'Équation de la Chaleur sur un Segment}
\begin{itemize}
    \item \textbf{Le problème :} $u_t = u_{xx}$ avec conditions aux limites nulles et $u(0,x)=f(x)$.
    \item \textbf{La méthode (Séparation des variables) :} On cherche des solutions de la forme $T(t)X(x)$, ce qui mène à décomposer la solution sur la base des sinus.
    \item \textbf{La solution :} La solution s'exprime comme une série de Fourier où les coefficients sont exponentiellement amortis : $u(t,x) = \sum c_n e^{-n^2 t} \sin(nx)$.
    \item \textbf{Illustration :} L'effet régularisant de l'équation de la chaleur est visible sur la formule : les hautes fréquences sont tuées très rapidement.
\end{itemize}
\subsection{Exemple 6 : L'Équation des Ondes (Corde Vibrante)}
\begin{itemize}
    \item \textbf{Le problème :} $u_{tt} = u_{xx}$.
    \item \textbf{La méthode :} La même approche mène à une solution où les coefficients sont des fonctions oscillantes du temps : $u(t,x) = \sum (a_n \cos(nt) + b_n \sin(nt))\sin(nx)$.
    \item \textbf{Illustration :} Les modes propres de la corde vibrante sont les harmoniques de la série de Fourier.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Étude du phénomène de Gibbs pour le signal carré :} Un développement calculatoire mais très parlant.
        \item \textbf{Calcul de $\zeta(2)$ et $\zeta(4)$ par l'identité de Parseval :} Un grand classique qui montre la puissance de l'outil pour des problèmes d'analyse a priori non liés.
        \item \textbf{Résolution de l'équation de la chaleur par séries de Fourier :} Un développement parfait qui connecte la leçon aux EDP.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un cours théorique :} Le titre exige des exemples.
        \item \textbf{Se tromper dans le calcul des coefficients :} Il faut en avoir calculé plusieurs à l'avance.
        \item \textbf{Confondre les différents types de convergence} (simple, uniforme, $L^2$) et leurs conditions.
        \item \textbf{Ne pas mentionner le phénomène de Gibbs :} C'est une subtilité essentielle de la théorie, qui illustre la non-convergence uniforme.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 250 : Transformation de Fourier. Applications.}

\begin{philosophie}
    C'est la version "continue" de l'analyse harmonique, étendue à $\mathbb{R}$. La leçon doit montrer la dualité fondamentale entre le domaine "temporel/spatial" et le domaine "fréquentiel". Le plan doit être une progression logique : d'abord la définition et les propriétés sur l'espace naturel $L^1$, puis son extension isométrique à l'espace de Hilbert $L^2$ (Plancherel), et enfin sa généralisation ultime aux distributions tempérées, qui est le cadre le plus puissant. Les applications doivent montrer la transversalité de l'outil.
\end{philosophie}

\section{La Transformée de Fourier sur $L^1(\mathbb{R})$}
\subsection{Définition et Propriétés Fondamentales}
\begin{itemize}
    \item \textbf{Définition :} $\hat{f}(\xi) = \int_{-\infty}^\infty f(x) e^{-2i\pi x \xi} dx$.
    \item \textbf{Propriétés Élémentaires :} Effet des translations, modulations, dilatations.
    \item \textbf{Lemme de Riemann-Lebesgue :} Si $f \in L^1$, alors $\hat{f}$ est continue et tend vers 0 à l'infini.
\end{itemize}
\subsection{Le Théorème de Convolution}
\begin{itemize}
    \item \textbf{Définition du produit de convolution :} $(f*g)(x) = \int f(y)g(x-y)dy$.
    \item \textbf{Théorème :} $\widehat{f*g} = \hat{f} \cdot \hat{g}$. La transformée de Fourier transforme la convolution (une opération analytique) en un simple produit ponctuel (une opération algébrique).
\end{itemize}
\subsection{Régularité et Décroissance}
\begin{itemize}
    \item \textbf{Principe de dualité :} La régularité de $f$ se traduit par la décroissance de $\hat{f}$ à l'infini, et vice-versa.
    \item \textbf{Théorème :} Si $x^k f \in L^1$, alors $\hat{f}$ est $\mathcal{C}^k$. Si $f$ est $\mathcal{C}^k$ et ses dérivées sont dans $L^1$, alors $\widehat{f^{(k)}}(\xi) = (2i\pi\xi)^k \hat{f}(\xi)$.
\end{itemize}

\section{Extension à $L^2(\mathbb{R})$ : le Théorème de Plancherel}
\subsection{Le Problème de l'Extension}
\begin{itemize}
    \item Une fonction de $L^2$ n'est pas forcément dans $L^1$, donc la définition par l'intégrale n'a pas de sens a priori.
\end{itemize}
\subsection{Le Théorème de Plancherel}
\begin{itemize}
    \item \textbf{Théorème :} La transformation de Fourier, définie sur l'espace dense $L^1 \cap L^2$, se prolonge de manière unique en un isomorphisme d'espaces de Hilbert de $L^2(\mathbb{R})$ dans lui-même.
    \item \textbf{Identité de Plancherel :} Cette isométrie conserve le produit scalaire et la norme : $\int |f(x)|^2 dx = \int |\hat{f}(\xi)|^2 d\xi$.
\end{itemize}
\begin{remark}
    C'est la version continue de l'identité de Parseval. Elle exprime la conservation de l'énergie entre le domaine temporel et le domaine fréquentiel.
\end{remark}

\section{Le Cadre Ultime : les Distributions Tempérées}
\subsection{L'Espace de Schwartz et son Dual}
\begin{itemize}
    \item \textbf{Espace de Schwartz $\mathcal{S}(\mathbb{R})$ :} Fonctions $\mathcal{C}^\infty$ à décroissance rapide. C'est l'espace "idéal" pour la transformée de Fourier, qui est un automorphisme de $\mathcal{S}$.
    \item \textbf{Distributions Tempérées $\mathcal{S}'(\mathbb{R})$ :} Le dual topologique de $\mathcal{S}$. Contient $L^p$, les polynômes, la masse de Dirac, etc.
\end{itemize}
\subsection{Transformée de Fourier d'une Distribution}
\begin{itemize}
    \item \textbf{Définition (par dualité) :} $\langle \hat{T}, \phi \rangle = \langle T, \hat{\phi} \rangle$.
    \item \textbf{Exemples Fondamentaux :}
        \begin{itemize}
            \item $\mathcal{F}(1) = \delta_0$.
            \item $\mathcal{F}(\delta_0) = 1$.
            \item $\mathcal{F}(\text{peigne de Dirac}) = \text{peigne de Dirac}$ (Formule Sommatoire de Poisson).
        \end{itemize}
\end{itemize}

\section{Applications}
\subsection{Résolution d'EDP}
\begin{itemize}
    \item \textbf{Principe :} La TF transforme la dérivation en une multiplication par un polynôme. Une EDP est ainsi transformée en une équation algébrique pour la transformée de Fourier de la solution.
    \item \textbf{Application :} Résolution de l'équation de la chaleur et de l'équation de Schrödinger sur $\mathbb{R}$.
\end{itemize}
\subsection{Traitement du Signal et Physique}
\begin{itemize}
    \item \textbf{Principe d'incertitude de Heisenberg :} Une fonction et sa transformée de Fourier ne peuvent être simultanément très localisées.
    \item \textbf{Théorème d'échantillonnage de Shannon.}
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Résolution de l'équation de la chaleur sur $\mathbb{R}$ par la transformée de Fourier :} Un développement parfait qui montre toute la puissance de l'outil.
        \item \textbf{Formule sommatoire de Poisson :} Un résultat profond qui lie l'analyse de Fourier sur le cercle et sur la droite.
        \item \textbf{Théorème d'inversion de Fourier dans $L^1$ :} Un développement technique mais fondamental.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les cadres $L^1$ et $L^2$ :} Il faut être très clair sur les propriétés valables dans chaque espace.
        \item \textbf{Appliquer la formule d'inversion sans précaution :} Elle n'est pas toujours valide dans $L^1$.
        \item \textbf{Ignorer le cadre des distributions :} C'est le seul cadre qui permet de manipuler rigoureusement des objets comme la masse de Dirac.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 253 : Utilisation de la notion de convexité en analyse.}

\begin{philosophie}
    Le titre est "Utilisation...". C'est une leçon transversale qui doit montrer que la convexité est la propriété géométrique qui rend l'analyse "simple". La convexité garantit l'existence et l'unicité des solutions en optimisation, elle fournit les inégalités les plus puissantes, et elle assure des propriétés de régularité remarquables. Le plan doit être une galerie d'exemples montrant comment cette simple idée géométrique se déploie dans tous les champs de l'analyse.
\end{philosophie}

\section{Convexité et Topologie : Théorèmes de Séparation}
\subsection{Le Cadre Géométrique}
\begin{itemize}
    \item \textbf{Définition :} Ensembles convexes, enveloppe convexe, points extrémaux.
\end{itemize}
\subsection{Le Théorème de Hahn-Banach Géométrique}
\begin{itemize}
    \item \textbf{Énoncé :} Deux convexes disjoints, dont l'un est ouvert, peuvent être séparés par un hyperplan affine.
    \item \textbf{Conséquence :} Un convexe fermé est l'intersection des demi-espaces fermés qui le contiennent.
\end{itemize}
\subsection{Application à l'Optimisation}
\begin{itemize}
    \item \textbf{Théorème de Projection sur un Convexe Fermé} dans un espace de Hilbert. L'existence et l'unicité du projeté sont une conséquence directe de la structure hilbertienne et de la convexité.
\end{itemize}

\section{Convexité et Optimisation : l'Art de Trouver le Minimum}
\subsection{Fonctions Convexes}
\begin{itemize}
    \item \textbf{Définition et Caractérisations :} Épigraphe, inégalité des cordes, croissance de la dérivée, positivité de la Hessienne.
\end{itemize}
\subsection{Le Théorème Fondamental de l'Optimisation Convexe}
\begin{itemize}
    \item \textbf{Théorème :} Pour une fonction convexe, tout minimum local est un minimum global. De plus, l'ensemble des minima est un convexe.
    \item \textbf{Condition d'optimalité :} Pour une fonction convexe différentiable, $x^*$ est un minimum si et seulement si $\nabla f(x^*) = 0$.
\end{itemize}
\subsection{Existence de Minima}
\begin{itemize}
    \item \textbf{Théorème :} Une fonction convexe, semi-continue inférieurement et coercive sur un Hilbert admet un minimum global.
\end{itemize}

\section{Convexité et Inégalités Fondamentales}
\subsection{L'Inégalité de Jensen}
\begin{itemize}
    \item \textbf{Énoncé (probabiliste) :} Pour une fonction convexe $\phi$, $\phi(E[X]) \le E[\phi(X)]$.
\end{itemize}
\subsection{Une Cascade d'Inégalités Classiques}
\begin{itemize}
    \item \textbf{Inégalité Arithmético-Géométrique :} Se déduit de la convexité de $x \mapsto -\ln(x)$.
    \item \textbf{Inégalité de Hölder :} Se déduit de la convexité de $x \mapsto x^p$.
    \item \textbf{Inégalité de Young :} $ab \le \frac{a^p}{p} + \frac{b^q}{q}$, se déduit de la concavité du logarithme.
\end{itemize}

\section{Propriétés de Régularité des Fonctions Convexes}
\subsection{Continuité}
\begin{itemize}
    \item \textbf{Théorème :} Une fonction convexe sur un intervalle \textbf{ouvert} est continue.
\end{itemize}
\subsection{Dérivabilité}
\begin{itemize}
    \item \textbf{Théorème :} Une fonction convexe sur un intervalle est dérivable à gauche et à droite en tout point. Ces dérivées sont croissantes.
    \item \textbf{Corollaire :} Une fonction convexe est dérivable presque partout.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Théorème de projection sur un convexe fermé dans un Hilbert :} Un grand classique, à la frontière de l'analyse et de la géométrie.
        \item \textbf{Inégalité de Jensen et application à l'inégalité arithmético-géométrique :} Un développement élégant qui montre la puissance de la notion.
        \item \textbf{Une fonction convexe sur un ouvert est continue :} Un développement technique qui montre une bonne maîtrise des définitions.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire une leçon trop descriptive :} Le titre est "Utilisation". Il faut montrer à quoi sert la convexité.
        \item \textbf{Mal énoncer les théorèmes de séparation :} Les hypothèses (ouvert, compact) sont cruciales.
        \item \textbf{Penser qu'une fonction convexe est toujours continue} (contre-exemple sur un fermé) ou dérivable (contre-exemple $|x|$).
        \item \textbf{Ne pas faire le lien avec l'optimisation,} qui est la motivation principale de l'étude de la convexité.
    \end{itemize}
\end{erreurs}

\part{Probabilités et Statistiques}

\chapter{Leçon 260 : Espérance, variance et moments d'une variable aléatoire.}

\begin{philosophie}
    Cette leçon est le point de départ de la quantification en probabilités. L'objectif est de montrer comment on peut résumer une loi de probabilité, potentiellement très complexe, par quelques indicateurs numériques. L'espérance est le "centre de gravité" de la loi, sa position moyenne. La variance est sa "dispersion", son étalement. Le plan doit partir de la définition rigoureuse (l'intégrale de Lebesgue), explorer les propriétés et outils de calcul (théorème de transfert, fonctions génératrices), et aboutir aux inégalités fondamentales qui montrent la puissance de ces notions.
\end{philosophie}

\section{Espérance : la Valeur Moyenne}
\subsection{Définition via l'Intégrale de Lebesgue}
\begin{itemize}
    \item \textbf{Définition Formelle :} L'espérance d'une v.a. $X: (\Omega, \mathcal{F}, \mathbb{P}) \to \mathbb{R}$ est son intégrale par rapport à la mesure de probabilité : $E[X] = \int_\Omega X d\mathbb{P}$. Elle existe si $X \in L^1(\Omega, \mathcal{F}, \mathbb{P})$.
    \item \textbf{Le Théorème de Transfert (Fondamental) :} Il nous permet de calculer l'espérance sans connaître l'espace de départ $\Omega$. Si $X$ a pour loi $P_X$, alors $E[g(X)] = \int_\mathbb{R} g(x) dP_X(x)$.
        \begin{itemize}
            \item \textbf{Cas Discret :} $E[X] = \sum_k x_k \mathbb{P}(X=x_k)$.
            \item \textbf{Cas à Densité :} $E[X] = \int_\mathbb{R} x f_X(x)dx$.
        \end{itemize}
\end{itemize}
\subsection{Propriétés de l'Espérance}
\begin{itemize}
    \item \textbf{Linéarité :} $E[aX+bY] = aE[X]+bE[Y]$. C'est une propriété universelle, qui ne requiert pas l'indépendance.
    \item \textbf{Positivité et Croissance :} Si $X \ge 0$, $E[X] \ge 0$. Si $X \le Y$, $E[X] \le E[Y]$.
    \item \textbf{Espérance d'un produit :} Si $X,Y$ sont \textbf{indépendantes}, alors $E[XY]=E[X]E[Y]$. La réciproque est fausse.
\end{itemize}

\section{Variance et Moments d'Ordre Supérieur}
\subsection{Variance et Écart-Type : Mesurer la Dispersion}
\begin{itemize}
    \item \textbf{Définition :} $V(X) = E[(X-E[X])^2]$. L'écart-type $\sigma(X) = \sqrt{V(X)}$.
    \item \textbf{Formule de Huygens :} $V(X) = E[X^2] - (E[X])^2$.
    \item \textbf{Propriétés :} $V(aX+b) = a^2 V(X)$. Si $X,Y$ sont indépendantes, $V(X+Y)=V(X)+V(Y)$.
\end{itemize}
\subsection{Covariance et Corrélation : Mesurer le Lien}
\begin{itemize}
    \item \textbf{Définition :} $\mathrm{Cov}(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY]-E[X]E[Y]$.
    \item \textbf{Coefficient de Corrélation Linéaire :} $\rho(X,Y) = \frac{\mathrm{Cov}(X,Y)}{\sigma(X)\sigma(Y)} \in [-1,1]$.
    \item \textbf{Indépendance $\implies$ Corrélation Nulle.} La réciproque est fausse (ex: $X \sim \mathcal{U}([-1,1]), Y=X^2$).
\end{itemize}
\subsection{Moments et Fonctions Génératrices}
\begin{itemize}
    \item \textbf{Moments d'ordre $k$ :} $m_k=E[X^k]$.
    \item \textbf{Fonction Génératrice des Moments :} $M_X(t) = E[e^{tX}]$. Elle permet de retrouver les moments par dérivation en 0.
    \item \textbf{Fonction Caractéristique :} $\phi_X(t) = E[e^{itX}]$. Elle existe toujours, caractérise la loi, et transforme les convolutions en produits. C'est la transformée de Fourier de la loi de probabilité.
\end{itemize}

\section{Inégalités de Concentration et Applications}
\subsection{Inégalités Fondamentales}
\begin{itemize}
    \item \textbf{Inégalité de Markov :} Pour $X \ge 0$, $\mathbb{P}(X \ge a) \le \frac{E[X]}{a}$.
    \item \textbf{Inégalité de Bienaymé-Tchebychev :} $\mathbb{P}(|X-E[X]| \ge a) \le \frac{V(X)}{a^2}$.
\end{itemize}
\begin{remark}
    Ces inégalités sont universelles (elles ne dépendent pas de la loi de $X$) mais souvent très larges. Elles montrent néanmoins le lien qualitatif : une petite variance implique une forte concentration autour de la moyenne.
\end{remark}
\subsection{Inégalité de Jensen}
\begin{itemize}
    \item \textbf{Énoncé :} Pour une fonction convexe $\phi$, $\phi(E[X]) \le E[\phi(X)]$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Preuve de l'inégalité de Bienaymé-Tchebychev à partir de Markov :} Court, simple et élégant.
        \item \textbf{Démonstration que la corrélation nulle n'implique pas l'indépendance :} Il faut construire un contre-exemple explicite et maîtrisé.
        \item \textbf{Utilisation des fonctions génératrices pour calculer l'espérance et la variance de la loi binomiale ou de Poisson :} Un classique qui montre la puissance de l'outil.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier que la linéarité de l'espérance est toujours vraie,} alors que $V(X+Y)=V(X)+V(Y)$ et $E[XY]=E[X]E[Y]$ nécessitent l'indépendance (ou du moins une corrélation nulle pour la variance).
        \item \textbf{Confondre les différentes fonctions génératrices} et leurs domaines de définition.
        \item \textbf{Ne pas savoir définir l'espérance dans le cadre de la mesure.} C'est un attendu de l'agrégation.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 261 : Loi d'une variable aléatoire.}

\begin{philosophie}
    C'est la leçon sur la "carte d'identité" d'une variable aléatoire. L'idée clé est que la loi contient \textit{toute} l'information probabiliste sur la variable, indépendamment de l'espace $\Omega$ de départ, souvent inaccessible ou trop complexe. La leçon doit montrer les différentes manières de caractériser une loi (fonction de répartition, densité, fonction caractéristique) et les opérations fondamentales pour construire de nouvelles lois à partir d'anciennes (transformations, sommes).
\end{philosophie}

\section{Caractérisation d'une Loi de Probabilité}
\subsection{Définition via la Mesure Image}
\begin{itemize}
    \item \textbf{Définition :} La loi de la v.a. $X: \Omega \to \mathbb{R}$ est la mesure de probabilité $P_X$ sur $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ définie par $P_X(A) = \mathbb{P}(X \in A) = \mathbb{P}(X^{-1}(A))$.
\end{itemize}
\subsection{La Fonction de Répartition (CDF)}
\begin{itemize}
    \item \textbf{Définition :} $F_X(x) = \mathbb{P}(X \le x)$.
    \item \textbf{Théorème :} La fonction de répartition caractérise entièrement la loi. Elle est croissante, continue à droite, de limite 0 en $-\infty$ et 1 en $+\infty$.
    \item \textbf{Types de lois :} La nature de $F_X$ détermine la nature de la loi (discrète si en escalier, à densité si absolument continue, mixte...).
\end{itemize}
\subsection{La Fonction Caractéristique}
\begin{itemize}
    \item \textbf{Définition :} $\phi_X(t) = E[e^{itX}] = \int_\mathbb{R} e^{itx} dP_X(x)$. C'est la transformée de Fourier de la mesure de probabilité $P_X$.
    \item \textbf{Théorème (Unicité de Lévy) :} La fonction caractéristique caractérise entièrement la loi.
\end{itemize}

\section{Opérations sur les Lois}
\subsection{Transformation d'une Variable Aléatoire}
\begin{itemize}
    \item \textbf{Loi de $Y=g(X)$ :} Pour calculer la loi de $Y$, on calcule sa fonction de répartition : $F_Y(y) = \mathbb{P}(g(X) \le y)$.
    \item \textbf{Cas à densité :} Si $g$ est un difféomorphisme, on a une formule de changement de variable pour la densité.
\end{itemize}
\subsection{Somme de Variables Aléatoires Indépendantes}
\begin{itemize}
    \item \textbf{Théorème (Produit de Convolution) :} Si $X$ et $Y$ sont indépendantes, la loi de $S=X+Y$ est la convolée des lois de $X$ et $Y$ : $P_S = P_X * P_Y$.
        \begin{itemize}
            \item \textbf{Cas à densité :} $f_{X+Y}(s) = \int_\mathbb{R} f_X(x)f_Y(s-x)dx$.
            \item \textbf{Cas discret :} $\mathbb{P}(X+Y=k) = \sum_j \mathbb{P}(X=j)\mathbb{P}(Y=k-j)$.
        \end{itemize}
    \item \textbf{L'outil des fonctions caractéristiques :} La convolution étant compliquée, on utilise le fait que $\phi_{X+Y}(t) = \phi_X(t) \phi_Y(t)$.
\end{itemize}

\section{Convergence de Lois}
\subsection{Définition de la Convergence en Loi}
\begin{itemize}
    \item \textbf{Définition :} Une suite de v.a. $(X_n)$ converge en loi vers $X$ si $F_{X_n}(x) \to F_X(x)$ en tout point de continuité de $F_X$.
\end{itemize}
\subsection{Théorème de Continuité de Lévy}
\begin{itemize}
    \item \textbf{Théorème :} La convergence en loi est équivalente à la convergence simple des fonctions caractéristiques : $X_n \xrightarrow{\mathcal{L}} X \iff \phi_{X_n}(t) \to \phi_X(t)$ pour tout $t$.
\end{itemize}
\begin{remark}
    C'est un outil extraordinairement puissant. Pour prouver le Théorème Central Limite, il est bien plus simple de montrer la convergence des fonctions caractéristiques que celle des fonctions de répartition.
\end{remark}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Calcul de la loi de la somme de deux v.a. indépendantes par convolution :} Choisir un exemple comme la somme de deux lois uniformes sur $[0,1]$ (loi triangulaire).
        \item \textbf{Stabilité de la loi normale par addition :} Montrer que si $X,Y$ sont normales et indépendantes, $X+Y$ est normale, en utilisant les fonctions caractéristiques.
        \item \textbf{Le théorème de continuité de Lévy (partie $\Leftarrow$) :} Un développement de haut niveau.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre la loi et la variable aléatoire.}
        \item \textbf{Ne pas maîtriser la fonction de répartition} et ses propriétés, qui est l'outil de base.
        \item \textbf{Oublier l'hypothèse d'indépendance} en parlant de la loi d'une somme.
        \item \textbf{Penser que la convergence en loi implique la convergence des espérances} (faux sans domination).
    \end{itemize}
\end{erreurs}
\chapter{Leçon 262 : Exemples de lois de variables aléatoires.}

\begin{philosophie}
    Le titre est "Exemples". C'est une leçon de "culture probabiliste". L'objectif n'est pas de faire un catalogue exhaustif, mais de présenter un "zoo" structuré des lois les plus importantes. Le plan doit s'organiser logiquement (par exemple, en séparant discret et continu), mais surtout, pour chaque loi, il faut en donner l'histoire, le phénomène qu'elle modélise, ses propriétés clés, et les liens qu'elle entretient avec les autres lois.
\end{philosophie}

\section{Lois Discrètes Fondamentales}
\subsection{Les Briques de Base : Bernoulli et Uniforme}
\begin{itemize}
    \item \textbf{Loi de Bernoulli $\mathcal{B}(p)$ :} Modélise une expérience à deux issues (succès/échec). $E[X]=p, V(X)=p(1-p)$.
    \item \textbf{Loi Uniforme sur $\{1,..,n\}$ :} Modélise un tirage équiprobable.
\end{itemize}
\subsection{Schémas de Tirages et Temps d'Attente}
\begin{itemize}
    \item \textbf{Loi Binomiale $\mathcal{B}(n,p)$ :} Nombre de succès en $n$ épreuves de Bernoulli indépendantes. C'est une somme de v.a. de Bernoulli.
    \item \textbf{Loi Géométrique $\mathcal{G}(p)$ :} Rang du premier succès. Modélise un temps d'attente. Propriété d'absence de mémoire.
    \item \textbf{Loi Hypergéométrique :} Tirages sans remise dans une urne.
\end{itemize}
\subsection{La Loi des Événements Rares : la Loi de Poisson}
\begin{itemize}
    \item \textbf{Loi de Poisson $\mathcal{P}(\lambda)$ :} Modélise le nombre d'événements rares sur une période de temps ou un espace donné (appels téléphoniques, désintégrations radioactives...).
    \item \textbf{Théorème de Poisson :} C'est la limite de la loi binomiale $\mathcal{B}(n,p)$ quand $n \to \infty, p \to 0$ avec $np \to \lambda$.
\end{itemize}

\section{Lois à Densité Usuelles}
\subsection{Les Lois les plus Simples}
\begin{itemize}
    \item \textbf{Loi Uniforme sur $[a,b]$ :} Tirage au hasard d'un nombre dans un intervalle.
    \item \textbf{Loi Exponentielle $\mathcal{E}(\lambda)$ :} Modélise une durée de vie sans vieillissement. C'est l'analogue continu de la loi géométrique, elle est aussi sans mémoire.
\end{itemize}
\subsection{L'Universalité de la Loi Normale}
\begin{itemize}
    \item \textbf{Loi Normale (ou de Laplace-Gauss) $\mathcal{N}(\mu, \sigma^2)$ :} C'est la loi qui apparaît comme limite dans le Théorème Central Limite. Elle modélise des phénomènes qui sont la somme d'un grand nombre de petites causes indépendantes (erreurs de mesure, taille des individus...).
\end{itemize}
\subsection{Lois Dérivées de la Loi Normale (les Lois du $\chi^2$, de Student, de Fisher)}
\begin{itemize}
    \item \textbf{Loi du Khi-deux $\chi^2(n)$ :} Loi de la somme des carrés de $n$ v.a. normales centrées réduites indépendantes.
    \item \textbf{Loi de Student $t(n)$ :} Loi du rapport entre une v.a. normale et la racine carrée d'une v.a. de Khi-deux.
    \item \textbf{Loi de Fisher $F(n,m)$ :} Loi du rapport de deux v.a. de Khi-deux.
\end{itemize}
\begin{remark}
    Ces trois lois sont fondamentales en statistiques. Elles sont les lois des estimateurs usuels de la moyenne et de la variance dans les modèles gaussiens.
\end{remark}

\section{Liens entre les Lois et Stabilité}
\subsection{Théorèmes de Convergence}
\begin{itemize}
    \item \textbf{Binomiale $\to$ Poisson.}
    \item \textbf{Binomiale $\to$ Normale (Moivre-Laplace).}
    \item \textbf{Poisson $\to$ Normale.}
\end{itemize}
\subsection{Stabilité par Addition}
\begin{itemize}
    \item Les lois binomiale, de Poisson, du Khi-deux et la loi normale sont stables par addition (pour des v.a. indépendantes).
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{La propriété d'absence de mémoire de la loi exponentielle (ou géométrique) :} Un développement simple mais qui montre une bonne compréhension du concept.
        \item \textbf{Convergence de la loi binomiale vers la loi de Poisson :} Un classique qui doit être parfaitement maîtrisé.
        \item \textbf{Stabilité de la loi normale par addition par la méthode des fonctions caractéristiques :} Un développement qui montre la puissance de cet outil.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Faire un catalogue sans âme :} Il faut raconter l'histoire de chaque loi.
        \item \textbf{Confondre les lois :} En particulier, la loi géométrique et la loi exponentielle.
        \item \textbf{Ne pas connaître les liens entre les lois :} C'est ce qui montre la cohérence de la théorie.
        \item \textbf{Oublier les lois du $\chi^2$, de Student, de Fisher,} qui sont le pont vers les statistiques.
    \end{itemize}
\end{erreurs}

% ... (Les leçons restantes 263-267, 290 seraient ajoutées ici sur le même modèle)
\chapter{Leçon 263 : Variables aléatoires à densité.}

\begin{philosophie}
    Cette leçon est le pendant continu de la leçon sur les variables discrètes. Elle est au cœur de la modélisation des phénomènes continus (temps, mesures physiques...). L'objectif est de montrer la maîtrise des outils de l'analyse (intégration, changement de variables) au service des probabilités. La notion de densité, comme "distribution infinitésimale" de la probabilité, doit être le fil conducteur. La leçon doit s'articuler autour de la caractérisation de ces lois, des opérations pour en construire de nouvelles (transformation, convolution), et de l'étude des vecteurs aléatoires.
\end{philosophie}

\section{Caractérisation et Propriétés Fondamentales}
\subsection{Définition et Lien avec la Fonction de Répartition}
\begin{itemize}
    \item \textbf{Définition :} Une v.a. réelle $X$ est à densité s'il existe une fonction $f_X: \mathbb{R} \to \mathbb{R}_+$ (la densité) telle que sa fonction de répartition $F_X$ s'écrit $F_X(x) = \int_{-\infty}^x f_X(t)dt$.
    \item \textbf{Propriétés de la densité :} $f_X$ est une fonction mesurable, positive, et d'intégrale 1.
    \item \textbf{Lien :} $f_X$ est la dérivée (presque partout) de $F_X$.
\end{itemize}
\subsection{Calcul d'Espérance et de Moments}
\begin{itemize}
    \item \textbf{Théorème de Transfert :} C'est la version "continue" du théorème. Pour toute fonction mesurable $g$, l'espérance de la v.a. $Y=g(X)$ est donnée par :
    $$ E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) dx $$
    \item \textbf{Calcul des moments :} $E[X^k] = \int_{-\infty}^{\infty} x^k f_X(x) dx$.
\end{itemize}
\begin{example}[Calculs pour les lois usuelles]
    Savoir calculer l'espérance et la variance de la loi uniforme, exponentielle et normale. Pour la loi normale, le calcul de la constante de normalisation est un classique (intégrale de Gauss).
\end{example}

\section{Opérations sur les Variables à Densité}
\subsection{Transformation d'une Variable à Densité}
\begin{itemize}
    \item \textbf{Théorème (Formule de Changement de Variable) :} Soit $Y=g(X)$ où $g$ est un difféomorphisme de classe $\mathcal{C}^1$. Alors $Y$ est une v.a. à densité, et sa densité est donnée par :
    $$ f_Y(y) = f_X(g^{-1}(y)) |(g^{-1})'(y)| $$
    \item \textbf{Application 1 : Loi du $\chi^2(1)$.} Si $X \sim \mathcal{N}(0,1)$, alors $Y=X^2$ suit une loi Gamma, qui est la loi du $\chi^2$ à 1 degré de liberté.
    \item \textbf{Application 2 : Méthode de la fonction muette.} Pour les cas où $g$ n'est pas un difféomorphisme, on revient à la définition $E[\phi(Y)] = E[\phi(g(X))]$ pour trouver la loi.
\end{itemize}
\subsection{Somme de Variables Indépendantes : le Produit de Convolution}
\begin{itemize}
    \item \textbf{Théorème :} Si $X$ et $Y$ sont indépendantes, de densités $f_X$ et $f_Y$, alors leur somme $S=X+Y$ est une v.a. à densité, et sa densité est le produit de convolution :
    $$ f_{X+Y}(s) = (f_X * f_Y)(s) = \int_{-\infty}^{\infty} f_X(x) f_Y(s-x) dx $$
    \item \textbf{Exemple : Somme de deux lois uniformes sur $[0,1]$.} Le calcul de la convolution donne une densité "triangulaire".
\end{itemize}

\section{Vecteurs Aléatoires à Densité}
\subsection{Densité Conjointe et Marginales}
\begin{itemize}
    \item \textbf{Définition :} Un vecteur $(X,Y)$ est à densité si sa loi est absolument continue par rapport à la mesure de Lebesgue sur $\mathbb{R}^2$. La densité conjointe est $f_{X,Y}(x,y)$.
    \item \textbf{Densités Marginales :} On retrouve la densité de $X$ en "intégrant" l'autre variable : $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy$.
\end{itemize}
\subsection{Indépendance}
\begin{itemize}
    \item \textbf{Caractérisation :} Deux v.a. à densité $X$ et $Y$ sont indépendantes si et seulement si leur densité conjointe est le produit de leurs densités marginales : $f_{X,Y}(x,y) = f_X(x) f_Y(y)$.
\end{itemize}
\subsection{Exemple Fondamental : le Vecteur Gaussien}
\begin{itemize}
    \item \textbf{Définition :} Un vecteur gaussien est un vecteur dont toute combinaison linéaire de ses composantes suit une loi normale.
    \item \textbf{Propriété Remarquable :} Pour un vecteur gaussien, l'indépendance de ses composantes est \textbf{équivalente} à la nullité de leur covariance. La réciproque fausse en général devient vraie dans le cas gaussien.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Calcul de la loi de la somme de deux lois uniformes par convolution :} Un développement calculatoire mais très formateur.
        \item \textbf{Loi du $\chi^2(n)$ comme somme de carrés de lois normales :} Un résultat fondamental en statistiques.
        \item \textbf{Stabilité de la loi Gamma (ou Normale) par addition :} Un bon exemple d'utilisation de la convolution ou des fonctions caractéristiques.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier le Jacobien} dans la formule de changement de variable.
        \item \textbf{Confondre densité et fonction de répartition.}
        \item \textbf{Ne pas savoir calculer une convolution simple.}
        \item \textbf{Croire que la corrélation nulle implique l'indépendance en général :} Il faut connaître le contre-exemple et savoir que ça devient vrai pour les vecteurs gaussiens.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 264 : Variables aléatoires discrètes.}

\begin{philosophie}
    C'est la leçon sur le monde du comptage et du dénombrement en probabilités. L'outil principal n'est plus l'intégrale, mais la sommation. Cette leçon doit montrer comment les outils combinatoires permettent de définir des lois, et comment les séries (en particulier les séries génératrices) deviennent l'outil d'analyse principal pour calculer des espérances, des variances, et étudier des sommes de variables.
\end{philosophie}

\section{Caractérisation et Indicateurs}
\subsection{Loi d'une Variable Aléatoire Discrète}
\begin{itemize}
    \item \textbf{Définition :} Une v.a. est discrète si elle prend un nombre fini ou dénombrable de valeurs.
    \item \textbf{Caractérisation par la loi de probabilité :} La loi est entièrement déterminée par la donnée des probabilités ponctuelles $p_k = \mathbb{P}(X=x_k)$, avec $\sum p_k = 1$.
    \item \textbf{Fonction de répartition :} C'est une fonction en escalier.
\end{itemize}
\subsection{Espérance, Variance et Moments}
\begin{itemize}
    \item \textbf{Calcul par des séries :} L'espérance et les moments sont définis par des séries (qui doivent converger absolument).
    \item \textbf{Espérance :} $E[X] = \sum_k x_k \mathbb{P}(X=x_k)$.
    \item \textbf{Variance :} $V(X) = \sum_k (x_k - E[X])^2 \mathbb{P}(X=x_k)$.
\end{itemize}

\section{Lois Discrètes Usuelles et Modélisation}
\subsection{Les Briques de Base}
\begin{itemize}
    \item \textbf{Loi de Bernoulli $\mathcal{B}(p)$ :} Modélise une expérience à deux issues.
    \item \textbf{Loi Uniforme sur $\{1,..,n\}$ :} Modélise un tirage équiprobable.
\end{itemize}
\subsection{Schémas de Tirages et Temps d'Attente}
\begin{itemize}
    \item \textbf{Loi Binomiale $\mathcal{B}(n,p)$ :} Somme de $n$ Bernoulli i.i.d. Modélise le nombre de succès en $n$ épreuves avec remise.
    \item \textbf{Loi Hypergéométrique :} Tirages sans remise.
    \item \textbf{Loi Géométrique $\mathcal{G}(p)$ :} Rang du premier succès dans un schéma de Bernoulli. Modélise un temps d'attente discret. Propriété d'absence de mémoire.
    \item \textbf{Loi Binomiale Négative :} Rang du $k$-ième succès.
\end{itemize}
\subsection{La Loi des Événements Rares}
\begin{itemize}
    \item \textbf{Loi de Poisson $\mathcal{P}(\lambda)$ :} Limite de la loi binomiale. Modélise un comptage d'événements.
\end{itemize}

\section{L'Outil des Fonctions Génératrices}
\subsection{Définition et Propriétés}
\begin{itemize}
    \item \textbf{Définition :} Pour une v.a. $X$ à valeurs dans $\mathbb{N}$, sa fonction génératrice est la série entière $G_X(s) = E[s^X] = \sum_{k=0}^\infty \mathbb{P}(X=k) s^k$.
    \item \textbf{Propriétés Fondamentales :}
        \begin{itemize}
            \item $G_X(1)=1$. Le rayon de convergence est au moins 1.
            \item La fonction génératrice caractérise la loi.
            \item On retrouve l'espérance et la variance par dérivation : $E[X]=G'_X(1)$, $V(X)=G''_X(1)+G'_X(1)-(G'_X(1))^2$.
        \end{itemize}
\end{itemize}
\subsection{Somme de Variables Indépendantes}
\begin{itemize}
    \item \textbf{Théorème :} Si $X,Y$ sont indépendantes, la fonction génératrice de la somme est le produit des fonctions génératrices : $G_{X+Y}(s) = G_X(s) G_Y(s)$.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item \textbf{Stabilité de la loi de Poisson :} Si $X \sim \mathcal{P}(\lambda)$ et $Y \sim \mathcal{P}(\mu)$ sont indépendantes, $X+Y \sim \mathcal{P}(\lambda+\mu)$. La preuve est immédiate avec les fonctions génératrices.
    \item \textbf{Processus de Galton-Watson :} L'étude de l'extinction d'une population se fait en étudiant l'itération de la fonction génératrice du nombre d'enfants.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Convergence de la loi binomiale vers la loi de Poisson :} Un classique qui doit être parfaitement maîtrisé.
        \item \textbf{Calcul de l'espérance et de la variance de la loi binomiale négative par les fonctions génératrices :} Montre la puissance de l'outil sur un cas non trivial.
        \item \textbf{La propriété d'absence de mémoire de la loi géométrique :} Un développement simple mais conceptuellement important.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Confondre les différents schémas de tirage :} Tirage avec remise (Binomiale) et sans remise (Hypergéométrique).
        \item \textbf{Faire des erreurs de calcul} sur les espérances et variances des lois usuelles.
        \item \textbf{Ne pas justifier la convergence des séries} pour l'existence de l'espérance.
        \item \textbf{Mal utiliser les fonctions génératrices,} par exemple en oubliant la condition d'indépendance pour une somme.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 265 : Exemples d'études et de modélisations de phénomènes aléatoires.}

\begin{philosophie}
    C'est une leçon très ouverte, une "leçon de culture". Le jury attend de voir une capacité à raconter des histoires avec les probabilités. Le plan ne doit pas être un catalogue de modèles, mais une galerie de "paradigmes" de modélisation. Chaque partie doit illustrer un grand type de problème : l'évolution dans le temps (chaînes de Markov), l'attente et le service (files d'attente), l'échantillonnage géométrique, et la simulation numérique. Pour chaque exemple, il faut être capable de poser le modèle, d'énoncer le résultat principal et d'en donner l'interprétation.
\end{philosophie}

\section{Modélisation de l'Évolution : les Chaînes de Markov}
\subsection{Le Modèle}
\begin{itemize}
    \item \textbf{Principe :} Un système qui évolue dans le temps entre un nombre fini d'états. L'évolution est "sans mémoire" (markovienne) : le futur ne dépend du passé qu'à travers l'état présent.
    \item \textbf{Outils :} Graphe des transitions, matrice de transition $M$.
\end{itemize}
\subsection{Exemple 1 : Le Modèle d'Ehrenfest (Diffusion de particules)}
\begin{itemize}
    \item \textbf{Description :} Deux urnes, $N$ boules. À chaque étape, on choisit une boule au hasard et on la change d'urne. L'état est le nombre de boules dans la première urne.
    \item \textbf{Question :} Comportement à long terme. La chaîne admet une distribution stationnaire (une loi binomiale) vers laquelle elle converge.
\end{itemize}
\subsection{Exemple 2 : L'Algorithme PageRank de Google}
\begin{itemize}
    \item \textbf{Modèle :} Un surfeur aléatoire sur le graphe du web. Les états sont les pages, la matrice de transition est construite à partir des liens hypertextes.
    \item \textbf{Résultat :} La distribution stationnaire de cette chaîne de Markov donne un score de "popularité" à chaque page, le PageRank.
\end{itemize}

\section{Modélisation de l'Attente : les Files d'Attente}
\subsection{Le Modèle M/M/1}
\begin{itemize}
    \item \textbf{Description :} Un seul serveur, arrivées des clients selon un processus de Poisson, temps de service selon une loi exponentielle.
    \item \textbf{Outils :} La longueur de la file d'attente est une chaîne de Markov sur $\mathbb{N}$.
    \item \textbf{Question :} Condition de stabilité. Il existe une distribution stationnaire si le taux d'arrivée est strictement inférieur au taux de service. On peut alors calculer la longueur moyenne de la file, le temps d'attente moyen...
\end{itemize}

\section{Modélisation par la Géométrie : Probabilités Géométriques}
\subsection{Le Problème de l'Aiguille de Buffon}
\begin{itemize}
    \item \textbf{Description :} On lance une aiguille de longueur $L$ sur un parquet dont les lattes sont espacées d'une distance $D \ge L$. Quelle est la probabilité que l'aiguille croise une ligne ?
    \item \textbf{Modélisation :} L'espace des possibles est paramétré par la position du centre de l'aiguille et son angle. On calcule des aires sur cet espace.
    \item \textbf{Résultat :} La probabilité est $\frac{2L}{\pi D}$.
\end{itemize}
\subsection{Application : Méthode de Monte-Carlo pour estimer $\pi$}
\begin{itemize}
    \item En lançant un grand nombre d'aiguilles et en comptant la fréquence d'intersection, on peut obtenir une estimation de $\pi$.
\end{itemize}

\section{Modélisation par Simulation Numérique}
\subsection{Principe de la Méthode de Monte-Carlo}
\begin{itemize}
    \item \textbf{Idée :} Pour calculer une quantité déterministe complexe (une intégrale, une espérance...), on la réinterprète comme l'espérance d'une variable aléatoire que l'on sait simuler. Par la Loi des Grands Nombres, la moyenne empirique des simulations converge vers la quantité recherchée.
\end{itemize}
\subsection{Exemple : Calcul d'intégrales en grande dimension}
\begin{itemize}
    \item \textbf{Problème :} Calculer $I = \int_{[0,1]^d} f(x) dx$ pour $d$ grand. Les méthodes de quadrature classiques sont inefficaces ("fléau de la dimension").
    \item \textbf{Solution Monte-Carlo :} $I = E[f(U)]$ où $U$ est une v.a. uniforme sur $[0,1]^d$. On estime $I$ par $\frac{1}{N} \sum f(U_i)$. Le TCL nous dit que la précision de l'estimation est en $1/\sqrt{N}$, indépendamment de la dimension $d$.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Étude de l'état stationnaire d'une chaîne de Markov simple (modèle d'Ehrenfest ou météo) :} Un développement très complet.
        \item \textbf{Calcul de la probabilité d'intersection pour l'aiguille de Buffon :} Un beau développement qui mêle probabilités et géométrie.
        \item \textbf{Justification de la méthode de Monte-Carlo pour le calcul d'intégrales, avec intervalle de confiance :} Montre une bonne compréhension des théorèmes limites.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Être trop vague :} Il faut choisir quelques exemples et les traiter en profondeur.
        \item \textbf{Ne pas poser le modèle mathématique rigoureusement :} Il faut définir l'espace, la tribu, la probabilité.
        \item \textbf{Ne pas savoir simuler un exemple simple :} Le jury peut demander comment simuler une loi exponentielle à partir d'une loi uniforme (méthode de la transformée inverse).
    \end{itemize}
\end{erreurs}
\chapter{Leçon 266 : Utilisation de la loi des grands nombres.}

\begin{philosophie}
    Le titre est "Utilisation...". Cette leçon n'est pas un cours sur la LGN, mais une démonstration de sa puissance philosophique et pratique. Elle est le pont entre les probabilités théoriques (l'espérance) et le monde réel (la moyenne des observations). C'est le théorème qui justifie l'approche fréquentiste des probabilités et qui fonde la statistique et les méthodes de simulation. Le plan doit illustrer ce triple rôle.
\end{philosophie}

\section{Les Deux Lois des Grands Nombres : le Résultat Théorique}
\subsection{Énoncés}
\begin{itemize}
    \item \textbf{Loi Faible des Grands Nombres (Bienaymé-Tchebychev) :} Soit $(X_n)$ i.i.d. dans $L^2$. Alors la moyenne empirique $\bar{X}_n$ converge \textbf{en probabilité} vers $E[X_1]$.
    \item \textbf{Loi Forte des Grands Nombres (Kolmogorov) :} Soit $(X_n)$ i.i.d. dans $L^1$. Alors $\bar{X}_n$ converge \textbf{presque sûrement} vers $E[X_1]$.
\end{itemize}
\subsection{Comparaison des Convergences}
\begin{itemize}
    \item La convergence presque sûre est beaucoup plus forte que la convergence en probabilité.
    \item La loi forte a des hypothèses plus faibles (juste l'existence de l'espérance). C'est le résultat le plus puissant.
\end{itemize}

\section{Application Fondamentale en Statistique}
\subsection{La LGN comme Justification de l'Estimation}
\begin{itemize}
    \item \textbf{Principe :} On observe un échantillon $(X_1, \dots, X_n)$ d'une loi inconnue. On veut estimer un paramètre de cette loi, par exemple son espérance $\mu$.
    \item \textbf{Estimateur Naturel :} La moyenne empirique $\bar{X}_n$.
    \item \textbf{Justification par la LGN :} La loi forte des grands nombres nous dit que $\bar{X}_n$ est un estimateur \textbf{convergent} (et non biaisé) de $\mu$. C'est la garantie que si l'on prend un échantillon assez grand, notre estimation sera proche de la vraie valeur.
\end{itemize}
\subsection{Exemple : les Sondages}
\begin{itemize}
    \item Pour estimer la proportion $p$ d'individus ayant une certaine opinion dans une population, on effectue un sondage sur $n$ personnes. C'est une suite de Bernoulli. La fréquence observée est la moyenne empirique. La LGN garantit que cette fréquence converge vers $p$.
\end{itemize}

\section{Application en Analyse : le Théorème de Weierstrass}
\subsection{Le Pont entre Analyse et Probabilités}
\begin{itemize}
    \item \textbf{Polynômes de Bernstein :} Pour une fonction continue $f:[0,1] \to \mathbb{R}$, on définit le $n$-ième polynôme de Bernstein par :
    $$ B_n(f)(x) = \sum_{k=0}^n \binom{n}{k} x^k (1-x)^{n-k} f(k/n) $$
\end{itemize}
\subsection{Preuve de Weierstrass par la LGN}
\begin{itemize}
    \item \textbf{Réinterprétation Probabiliste :} On reconnaît que $B_n(f)(x)$ est l'espérance de $f(S_n/n)$, où $S_n \sim \mathcal{B}(n,x)$.
    \item \textbf{La Preuve :} La loi des grands nombres nous dit que $S_n/n$ converge en probabilité vers son espérance, qui est $x$. Comme $f$ est continue (et même uniformément continue), on peut montrer que $E[f(S_n/n)]$ converge vers $f(x)$. Avec un peu plus de travail, on montre que la convergence est uniforme.
\end{itemize}

\section{Application en Analyse Numérique : Méthodes de Monte-Carlo}
\subsection{Calcul d'Intégrales}
\begin{itemize}
    \item \textbf{Principe :} Pour calculer $I = \int_A f(x)dx$, on réécrit $I$ comme une espérance.
    \item \textbf{Exemple :} $I = \int_0^1 g(t)dt = E[g(U)]$ où $U \sim \mathcal{U}([0,1])$.
    \item \textbf{Algorithme :} On simule un grand nombre $N$ de variables $U_i$ et on approche $I$ par la moyenne empirique $\frac{1}{N}\sum g(U_i)$. La LGN garantit la convergence de l'approximation.
\end{itemize}
\subsection{Avantages de la Méthode}
\begin{itemize}
    \item \textbf{Simplicité :} Très facile à implémenter.
    \item \textbf{Efficacité en grande dimension :} La vitesse de convergence (donnée par le TCL) est en $1/\sqrt{N}$ et ne dépend pas de la dimension de l'espace d'intégration, contrairement aux méthodes de quadrature classiques.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Preuve de la loi faible des grands nombres par l'inégalité de Bienaymé-Tchebychev :} Un développement court, simple et fondamental.
        \item \textbf{Démonstration du théorème de Weierstrass par les polynômes de Bernstein :} Un développement magnifique qui connecte deux grands chapitres de l'analyse.
        \item \textbf{Justification de la méthode de Monte-Carlo pour le calcul d'intégrales, avec intervalle de confiance (en utilisant le TCL).}
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Ne pas connaître la différence entre la loi faible et la loi forte.}
        \item \textbf{Penser que la LGN donne la vitesse de convergence.} C'est le rôle du Théorème Central Limite. La LGN ne fait que garantir la convergence.
        \item \textbf{Mal appliquer la LGN :} Oublier l'hypothèse d'indépendance ou d'identique distribution.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 267 : Utilisation du théorème central limite.}

\begin{philosophie}
    Le titre est "Utilisation...". Cette leçon est la suite naturelle de la LGN. La LGN nous dit que les moyennes convergent. Le TCL nous dit \textit{comment} elles convergent : il décrit la "forme" universelle des fluctuations autour de la limite. La leçon doit donc se concentrer sur le TCL comme un outil d'approximation (l'universalité de la loi normale) et comme le fondement de la statistique inférentielle (intervalles de confiance et tests).
\end{philosophie}

\section{Le Théorème et son Interprétation}
\subsection{Énoncé (Lindeberg-Lévy)}
\begin{itemize}
    \item \textbf{Théorème :} Pour une suite de v.a. i.i.d. $(X_n)$ d'espérance $\mu$ et de variance finie $\sigma^2$, on a la convergence en loi de la moyenne normalisée :
    $$ Z_n = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} = \frac{S_n - n\mu}{\sigma\sqrt{n}} \xrightarrow{\mathcal{L}} \mathcal{N}(0,1) $$
\end{itemize}
\subsection{L'Universalité de la Loi Normale}
\begin{itemize}
    \item \textbf{Philosophie :} Le TCL explique pourquoi la loi normale est omniprésente. De nombreux phénomènes sont le résultat de la somme d'un grand nombre de petites causes indépendantes. Quelle que soit la loi de ces causes, leur somme normalisée suivra approximativement une loi normale.
\end{itemize}

\section{Le TCL comme Outil d'Approximation}
\subsection{Approximation de la Loi Binomiale (Théorème de Moivre-Laplace)}
\begin{itemize}
    \item C'est le premier exemple historique du TCL. Si $S_n \sim \mathcal{B}(n,p)$, alors $\frac{S_n - np}{\sqrt{np(1-p)}}$ converge en loi vers $\mathcal{N}(0,1)$.
    \item \textbf{Application :} Calculer des probabilités pour une loi binomiale avec un grand $n$. On utilise souvent une correction de continuité pour améliorer la qualité de l'approximation.
\end{itemize}
\subsection{Approximation d'autres lois}
\begin{itemize}
    \item \textbf{Loi de Poisson :} Si $X \sim \mathcal{P}(\lambda)$ avec $\lambda$ grand, alors $\frac{X-\lambda}{\sqrt{\lambda}}$ peut être approchée par une loi normale.
    \item \textbf{Loi Gamma, Loi du $\chi^2$...}
\end{itemize}

\section{Application Fondamentale à la Statistique Inférentielle}
\subsection{Construction d'Intervalles de Confiance Asymptotiques}
\begin{itemize}
    \item \textbf{Principe :} C'est l'application la plus importante. Pour estimer une moyenne $\mu$ inconnue, le TCL nous dit que la statistique $Z_n = \frac{\sqrt{n}(\bar{X}_n - \mu)}{S_n}$ (où $S_n$ est l'écart-type empirique) suit approximativement une loi $\mathcal{N}(0,1)$.
    \item \textbf{Construction :} On en déduit l'intervalle de confiance asymptotique pour $\mu$ au niveau $1-\alpha$ :
    $$ \left[ \bar{X}_n - z_{\alpha/2} \frac{S_n}{\sqrt{n}}, \bar{X}_n + z_{\alpha/2} \frac{S_n}{\sqrt{n}} \right] $$
    \item \textbf{Application : Sondages d'opinion.} La fameuse "marge d'erreur" d'un sondage est la demi-longueur de cet intervalle de confiance.
\end{itemize}
\subsection{Construction de Tests d'Hypothèses Asymptotiques}
\begin{itemize}
    \item \textbf{Principe :} Pour tester une hypothèse $H_0: \mu = \mu_0$, on utilise la même statistique. Sous $H_0$, $Z = \frac{\sqrt{n}(\bar{X}_n - \mu_0)}{S_n}$ devrait suivre une loi $\mathcal{N}(0,1)$.
    \item \textbf{Règle de décision :} On calcule la valeur observée de $Z$ et on la compare aux quantiles de la loi normale. Si elle est trop "extrême" (dans la zone de rejet), on rejette $H_0$.
    \item \textbf{Application :} Tests de conformité d'une moyenne, comparaison de deux moyennes.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Preuve du Théorème de Moivre-Laplace (via les fonctions caractéristiques) :} Un grand classique qui montre la puissance de cet outil.
        \item \textbf{Construction d'un intervalle de confiance pour une proportion (sondage) :} Un développement très concret et appliqué.
        \item \textbf{Vitesse de convergence : Théorème de Berry-Esseen (énoncé et discussion) :} Un développement de haut niveau qui montre une grande culture.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Oublier le facteur de normalisation $\sqrt{n}$.} C'est le cœur du théorème.
        \item \textbf{Confondre la convergence en loi avec d'autres types de convergence.}
        \item \textbf{Appliquer le TCL quand la variance est infinie.}
        \item \textbf{Mal interpréter un intervalle de confiance.} Ce n'est pas le paramètre qui est aléatoire, mais l'intervalle.
    \end{itemize}
\end{erreurs}
\chapter{Leçon 290 : Probabilités conditionnelles et applications.}

\begin{philosophie}
    C'est la leçon sur la dynamique de l'information en probabilités. La probabilité conditionnelle est l'outil qui nous permet de mettre à jour nos croyances à la lumière de nouvelles informations. Le plan doit progresser de la notion élémentaire (conditionnement par un événement) à la notion bien plus profonde et puissante d'espérance conditionnelle (conditionnement par une information plus générale, une tribu). Les applications doivent montrer comment ce concept est la clé de la modélisation des phénomènes séquentiels.
\end{philosophie}

\section{Probabilité Conditionnelle Élémentaire}
\subsection{Conditionnement par un Événement}
\begin{itemize}
    \item \textbf{Définition :} Si $\mathbb{P}(B)>0$, $\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}$.
    \item \textbf{L'application $A \mapsto \mathbb{P}(A|B)$ est une nouvelle mesure de probabilité sur $(\Omega, \mathcal{F})$.}
\end{itemize}
\subsection{Les Formules Fondamentales de l'Inférence}
\begin{itemize}
    \item \textbf{Formule des Probabilités Composées :} $\mathbb{P}(A_1 \cap \dots \cap A_n) = \mathbb{P}(A_1) \mathbb{P}(A_2|A_1) \cdots \mathbb{P}(A_n | A_1 \cap \dots \cap A_{n-1})$.
    \item \textbf{Formule des Probabilités Totales :} $\mathbb{P}(A) = \sum_i \mathbb{P}(A|B_i)\mathbb{P}(B_i)$ pour une partition $(B_i)$.
    \item \textbf{Formule de Bayes :} Permet d'"inverser" le conditionnement : $\mathbb{P}(B_i|A) \propto \mathbb{P}(A|B_i)\mathbb{P}(B_i)$.
\end{itemize}
\subsection{Indépendance et Indépendance Conditionnelle}
\begin{itemize}
    \item \textbf{Indépendance :} $A,B$ indépendants $\iff \mathbb{P}(A|B) = \mathbb{P}(A)$.
    \item \textbf{Indépendance Conditionnelle :} $A,B$ sont indépendants conditionnellement à $C$ si $\mathbb{P}(A \cap B | C) = \mathbb{P}(A|C)\mathbb{P}(B|C)$.
    \item \textbf{Contre-exemple :} L'indépendance n'implique pas l'indépendance conditionnelle, et vice-versa.
\end{itemize}

\section{Espérance Conditionnelle : Conditionner par une Information}
\subsection{Le Cas Discret}
\begin{itemize}
    \item \textbf{Définition :} L'espérance de $X$ sachant l'événement $A$ est $E[X|A]$.
    \item \textbf{Définition :} L'espérance de $Y$ sachant $X=x$ est $E[Y|X=x]$.
    \item \textbf{L'espérance conditionnelle $E[Y|X]$ est une variable aléatoire,} qui est une fonction de $X$.
\end{itemize}
\subsection{Le Cas Général : la Projection Hilbertienne}
\begin{itemize}
    \item \textbf{Motivation :} Comment conditionner par une information qui n'est pas un simple événement, mais une tribu $\mathcal{G} \subset \mathcal{F}$ ?
    \item \textbf{Théorème-Définition :} Soit $X \in L^2(\Omega)$. L'espérance conditionnelle $E[X|\mathcal{G}]$ est la \textbf{projection orthogonale} de $X$ sur le sous-espace de Hilbert $L^2(\Omega, \mathcal{G}, \mathbb{P})$.
    \item \textbf{Caractérisation :} C'est l'unique v.a. $Z$ qui est $\mathcal{G}$-mesurable et qui vérifie $\forall A \in \mathcal{G}, \int_A Z d\mathbb{P} = \int_A X d\mathbb{P}$.
\end{itemize}
\subsection{Propriétés Fondamentales}
\begin{itemize}
    \item \textbf{Linéarité, Positivité.}
    \item \textbf{Tour de l'espérance (ou espérance itérée) :} $E[E[X|\mathcal{G}]] = E[X]$.
    \item \textbf{On peut "sortir" ce qui est connu :} Si $Z$ est $\mathcal{G}$-mesurable, $E[ZX|\mathcal{G}] = Z E[X|\mathcal{G}]$.
\end{itemize}

\section{Applications aux Processus Stochastiques}
\subsection{Les Chaînes de Markov}
\begin{itemize}
    \item \textbf{Définition formelle :} Une suite de v.a. $(X_n)$ est une chaîne de Markov par rapport à la filtration $(\mathcal{F}_n)$ si $E[f(X_{n+1})|\mathcal{F}_n] = E[f(X_{n+1})|X_n]$. Le futur ne dépend du passé qu'à travers le présent.
\end{itemize}
\subsection{Les Martingales}
\begin{itemize}
    \item \textbf{Définition :} Un processus $(M_n)$ est une martingale si $E[M_{n+1}|\mathcal{F}_n]=M_n$. C'est le modèle d'un jeu équitable : l'espérance de la fortune de demain, sachant tout ce qui s'est passé jusqu'à aujourd'hui, est la fortune d'aujourd'hui.
    \item \textbf{Théorèmes de Convergence :} Les martingales (positives ou bornées dans $L^1$) convergent presque sûrement.
\end{itemize}

\begin{developpements}
    \begin{itemize}
        \item \textbf{Application de la formule de Bayes à un problème de diagnostic médical :} Un grand classique qui montre la bonne compréhension de la formule et des problèmes d'inversion.
        \item \textbf{Calcul d'une espérance conditionnelle dans un cas non-trivial :} Par exemple, si $(X,Y)$ suit une loi uniforme sur le disque unité, calculer $E[X|Y=y]$.
        \item \textbf{Le paradoxe des deux enveloppes ou le problème de Monty Hall :} Des applications ludiques mais profondes du conditionnement.
    \end{itemize}
\end{developpements}

\begin{erreurs}
    \begin{itemize}
        \item \textbf{Mal appliquer la formule de Bayes :} Se tromper dans ce qu'est l'hypothèse et ce qu'est l'observation.
        \item \textbf{Confondre indépendance et indépendance conditionnelle.}
        \item \textbf{Ne pas comprendre que $E[X|\mathcal{G}]$ est une variable aléatoire,} et non un nombre.
        \item \textbf{Ne pas connaître la définition de l'espérance conditionnelle dans le cas général ($L^2$),} qui est un attendu de l'agrégation.
    \end{itemize}
\end{erreurs}

\end{document}
